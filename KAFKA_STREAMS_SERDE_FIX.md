# CRITICAL FIX: Kafka Streams Serde Configuration

## ‚ùå Problem: ClassCastException in UnifiedMarketDataProcessor

### Error Message
```
ClassCastException: class java.lang.String cannot be cast to class com.kotsin.consumer.model.TickData
```

### Root Cause
**UnifiedMarketDataProcessor** was creating streams **WITHOUT specifying Consumed serdes**:

```java
// WRONG (before):
KStream<String, TickData> ticks = builder.stream(ticksTopic);
KStream<String, OpenInterest> oiStream = builder.stream(oiTopic);
KStream<String, OrderBookSnapshot> orderbookStream = builder.stream(orderbookTopic);
```

This caused Kafka Streams to use **default deserialization** (String), not the custom JSON deserializers.

---

## ‚úÖ Solution Applied

### Added Consumed Serdes to All Streams

```java
// CORRECT (after):
KStream<String, TickData> ticks = builder.stream(
    ticksTopic,
    Consumed.with(Serdes.String(), TickData.serde())  // ‚Üê Specify serdes!
);

KStream<String, OpenInterest> oiStream = builder.stream(
    oiTopic,
    Consumed.with(Serdes.String(), OpenInterest.serde())  // ‚Üê Specify serdes!
);

KStream<String, OrderBookSnapshot> orderbookStream = builder.stream(
    orderbookTopic,
    Consumed.with(Serdes.String(), OrderBookSnapshot.serde())  // ‚Üê Specify serdes!
);
```

### Why This Fixes It

1. **optionProducerJava** sends data as JSON bytes (using MarketDataSerializer, OpenInterestSerializer, etc.)
2. **Kafka Streams** needs matching **deserializers** to read JSON back to Java objects
3. **TickData.serde()**, **OpenInterest.serde()**, **OrderBookSnapshot.serde()** provide the correct JSON deserializers
4. **Consumed.with()** tells Kafka Streams: "Use these specific serdes for this stream"

---

## üîç How optionProducerJava Sends Data

### Producer Configuration (KafkaProducerConfig.java)

```java
// MarketData (TickData) producer
configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, MarketDataSerializer.class);

// OpenInterest producer
configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, OpenInterestSerializer.class);

// OrderBook producer
configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, OrderBookDepthSerializer.class);
```

### MarketDataSerializer
```java
public byte[] serialize(String topic, MarketData data) {
    return objectMapper.writeValueAsBytes(data);  // ‚Üê Writes JSON
}
```

**Result**: All data is sent as **JSON bytes** to Kafka

---

## üîÑ How streamingcandle Must Consume Data

### TickData.serde() (Already Defined)
```java
public static Serde<TickData> serde() {
    return Serdes.serdeFrom(
        new TickDataSerializer(),
        new TickDataDeserializer()  // ‚Üê Uses ObjectMapper.readValue()
    );
}
```

### OpenInterest.serde() (Already Defined)
```java
public static Serde<OpenInterest> serde() {
    return new JsonSerde<>(OpenInterest.class);  // ‚Üê Spring's JSON deserializer
}
```

### OrderBookSnapshot.serde() (Already Defined)
```java
public static Serde<OrderBookSnapshot> serde() {
    return new JsonSerde<>(OrderBookSnapshot.class);  // ‚Üê Spring's JSON deserializer
}
```

---

## üìä Data Flow (Before vs After)

### Before Fix (BROKEN)
```
optionProducerJava:
  MarketData ‚Üí MarketDataSerializer ‚Üí JSON bytes ‚Üí Kafka

Kafka Streams (UnifiedMarketDataProcessor):
  Kafka ‚Üí String deserializer (default) ‚Üí String ‚ùå
  
Join Operation:
  String cannot be cast to TickData ‚Üí ClassCastException ‚ùå
```

### After Fix (WORKING)
```
optionProducerJava:
  MarketData ‚Üí MarketDataSerializer ‚Üí JSON bytes ‚Üí Kafka

Kafka Streams (UnifiedMarketDataProcessor):
  Kafka ‚Üí TickData.serde() ‚Üí TickDataDeserializer ‚Üí TickData ‚úÖ
  
Join Operation:
  TickData + OpenInterest + OrderBookSnapshot ‚Üí Works! ‚úÖ
```

---

## üöÄ Deployment Steps

### 1. Rebuild on Local Machine
```bash
cd /Users/shivendrapratap/Documents/kotsin/streamingcandle
mvn clean package -DskipTests
```

**Expected**: BUILD SUCCESS

### 2. Deploy to Production Server

```bash
# Copy fixed file to server
scp src/main/java/com/kotsin/consumer/processor/UnifiedMarketDataProcessor.java \
    ubuntu@13.203.60.173:~/streamingcandle/src/main/java/com/kotsin/consumer/processor/

# OR via git:
git add .
git commit -m "fix: Add Consumed serdes to UnifiedMarketDataProcessor streams"
git push origin main

# Then on server:
ssh ubuntu@13.203.60.173
cd ~/streamingcandle
git pull origin main
```

### 3. Stop Existing Process
```bash
# On server
pkill -f streamingcandle

# Verify it stopped
ps aux | grep streamingcandle | grep -v grep
```

### 4. Clean State Stores (IMPORTANT!)
```bash
# Remove old state stores to prevent serde mismatch errors
rm -rf /home/ubuntu/kstreams/consumer/*

# This forces Kafka Streams to rebuild state with correct serdes
```

### 5. Rebuild and Restart
```bash
# Clean build
mvn clean package -DskipTests

# Start application
nohup mvn spring-boot:run > streamingcandle.log 2>&1 &

# Monitor logs
tail -f streamingcandle.log
```

---

## ‚úÖ Verification Steps

### 1. Check for No Serde Errors
```bash
# Should NOT see ClassCastException
grep -i "ClassCastException" streamingcandle.log

# Should see successful startup
grep "Unified Market Data Processor started successfully" streamingcandle.log
```

### 2. Check Kafka Streams is Processing
```bash
# Should see RUNNING state, no errors
tail -50 streamingcandle.log | grep -E "RUNNING|ERROR"
```

### 3. Verify Output Topic Has Data
```bash
kafka-console-consumer --bootstrap-server 13.203.60.173:9094 \
  --topic enriched-market-data \
  --from-beginning \
  --max-messages 3
```

**Expected**: JSON messages with:
- `scripCode`
- `multiTimeframeCandles` (1m, 2m, 3m, 5m, 15m, 30m)
- `openInterest`
- `imbalanceBars`
- `microstructure`
- `metadata`

---

## üéØ Test Data Producer Status

**Good News**: Your test_data_producer.py is working PERFECTLY! ‚úÖ

From the terminal output you shared:
```
‚úÖ Sent to forwardtesting-data | Partition: 0 | Offset: 20
üìä Tick: NIFTY @ ‚Çπ19,449.10 | Vol: 3,477,868
‚úÖ Sent to Orderbook | Partition: 0 | Offset: 10
üìñ OrderBook: NIFTY | Bid: ‚Çπ19,449.05 | Ask: ‚Çπ19,449.15
‚úÖ Sent to OpenInterest | Partition: 0 | Offset: 4
üìà OI: NIFTY | OI: 40,422,042 | Change: +2,804
```

**Producer is sending data correctly!** The problem was on the consumer side (UnifiedMarketDataProcessor).

---

## üìù Files Modified

1. `/Users/shivendrapratap/Documents/kotsin/streamingcandle/src/main/java/com/kotsin/consumer/processor/UnifiedMarketDataProcessor.java`
   - Added `Consumed.with()` to specify serdes for each stream
   - Ensures proper JSON deserialization

---

## üîó Related Issues Fixed in This Session

| # | Issue | Fix | Status |
|---|-------|-----|--------|
| 1 | 100+ compilation errors | OrderBookSnapshot methods, JsonSerde fixes, etc. | ‚úÖ Fixed |
| 2 | Missing RestTemplate bean | Created WebConfig.java | ‚úÖ Fixed |
| 3 | ClassCastException in streams | Added Consumed serdes | ‚úÖ Fixed |

---

## üìö Key Learnings

### 1. Kafka Streams Serde Configuration

**Rule**: Always explicitly specify serdes when:
- Creating streams: `builder.stream(topic, Consumed.with(keySerde, valueSerde))`
- Joining streams: `StreamJoined.with(keySerde, valueSerde1, valueSerde2)`
- Materializing state: `Materialized.with(keySerde, valueSerde)`

**Why**: Default serdes may not match the actual data format

### 2. Custom Serializers in Producer

**optionProducerJava** uses custom serializers (MarketDataSerializer, etc.) that write JSON.

**streamingcandle** must use matching deserializers (TickData.serde(), etc.) that read JSON.

### 3. State Store Cleanup

When changing serdes, **always clean state stores** to prevent mismatch errors:
```bash
rm -rf /home/ubuntu/kstreams/consumer/*
```

---

## üéØ Next Steps

1. ‚úÖ **DONE**: Fixed Consumed serdes in UnifiedMarketDataProcessor
2. **NOW**: Deploy to production server
3. **THEN**: Clean state stores
4. **FINALLY**: Restart and verify enriched-market-data topic has output

---

## üî• Quick Deploy Commands

```bash
# On production server
ssh ubuntu@13.203.60.173

# Stop old process
pkill -f streamingcandle

# Clean state stores (CRITICAL!)
rm -rf /home/ubuntu/kstreams/consumer/*

# Navigate to app
cd ~/streamingcandle

# Pull latest changes
git pull origin main

# Rebuild
mvn clean package -DskipTests

# Start
nohup mvn spring-boot:run > streamingcandle.log 2>&1 &

# Monitor
tail -f streamingcandle.log
```

---

**Fix Applied**: October 20, 2025  
**Severity**: üî¥ CRITICAL  
**Impact**: UnifiedMarketDataProcessor can now deserialize data correctly  
**Status**: ‚úÖ READY FOR DEPLOYMENT  

---

**This was the root cause of all the Kafka Streams errors. The fix is simple but critical!**

