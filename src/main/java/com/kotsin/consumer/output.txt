Folder Structure
--------------------------------------------------
/
    ConsumerApplication.java
metrics/
    StreamMetrics.java
repository/
    ScripGroupRepository.java
retry/
    RetryHandler.java
util/
    ValidationUtils.java
    MarketTimeAligner.java
config/
    KafkaConfig.java
    ConfigurationValidator.java
    ProcessingConstants.java
entity/
    ScripGroup.java
    Scrip.java
processor/
    MicrostructureAccumulator.java
    UnifiedMarketDataProcessor.java
    CandleAccumulator.java
    InstrumentState.java
    ImbalanceBarAccumulator.java
    OrderbookDepthAccumulator.java
    OiAccumulator.java
    WindowRotationService.java
    Timeframe.java
    MultiTimeframeState.java
    service/
        FamilyAggregationService.java
        MarketDataEnrichmentService.java
        InstrumentProcessor.java
        InstrumentKeyResolver.java
        SpoofingDetectionService.java
        TopologyConfiguration.java
        MarketDataMergeService.java
        InstrumentStateManager.java
        StreamMetrics.java
        MarketDataOrchestrator.java
        OrderbookDepthCalculator.java
        CircuitBreakerDetector.java
        IcebergDetectionService.java
        TradingHoursValidationService.java
        DataEnrichmentService.java
        BackpressureHandler.java
        CandleEmissionService.java
        TimeframeStateManager.java
        DynamicTradingHoursService.java
controller/
    HealthController.java
    CacheHealthController.java
audit/
    AuditLogger.java
transformers/
    CumToDeltaTransformer.java
model/
    InstrumentFamily.java
    ImbalanceBarState.java
    FamilyStructuredAll.java
    MessageMetadata.java
    OpenInterestData.java
    OpenInterestAggregation.java
    MicrostructureFeature.java
    InstrumentInfo.java
    OrderbookDepthData.java
    FamilyAggregatedMetrics.java
    MicrostructureFeatureState.java
    InformationBar.java
    ImbalanceBarData.java
    OpenInterestTimeframeData.java
    InstrumentCandle.java
    EnrichedMarketData.java
    MicrostructureData.java
    CandleData.java
    OpenInterest.java
    Candlestick.java
    OrderBookSnapshot.java
    FamilyEnrichedData.java
    TickData.java
    RunsBarState.java
service/
    MicrostructureMetricsCalculator.java
    MongoInstrumentFamilyService.java
monitoring/
    SystemMonitor.java
timeExtractor/
    TickTimestampExtractor.java
    MultiMinuteOffsetTimestampExtractor.java
    OpenInterestTimestampExtractor.java
    InstrumentCandleTimestampExtractor.java


File Contents
--------------------------------------------------


ConsumerApplication.java
File type: .java
package com.kotsin.consumer;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;


/**
 * Spring Boot Application to initialize Kafka Streams for various candlestick durations.
 */
@SpringBootApplication
public class ConsumerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ConsumerApplication.class, args);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


metrics/StreamMetrics.java
File type: .java
package com.kotsin.consumer.metrics;

import org.springframework.stereotype.Component;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;

@Component
public class StreamMetrics {
    private final Map<String, AtomicLong> candleEmitsByTf = new ConcurrentHashMap<>();
    private final Map<String, AtomicLong> candleDropsByTf = new ConcurrentHashMap<>();
    private final AtomicLong familyEmits = new AtomicLong(0);
    private final AtomicLong oiJoinMisses = new AtomicLong(0);
    private final AtomicLong orderbookJoinMisses = new AtomicLong(0);

    public void incCandleEmit(String timeframe) { candleEmitsByTf.computeIfAbsent(timeframe, k -> new AtomicLong()).incrementAndGet(); }
    public void incCandleDrop(String timeframe) { candleDropsByTf.computeIfAbsent(timeframe, k -> new AtomicLong()).incrementAndGet(); }
    public void incFamilyEmit() { familyEmits.incrementAndGet(); }
    public void incOiJoinMiss() { oiJoinMisses.incrementAndGet(); }
    public void incOrderbookJoinMiss() { orderbookJoinMisses.incrementAndGet(); }

    public Map<String, Long> getCandleEmitsByTf() { return toLongMap(candleEmitsByTf); }
    public Map<String, Long> getCandleDropsByTf() { return toLongMap(candleDropsByTf); }
    public long getFamilyEmits() { return familyEmits.get(); }
    public long getOiJoinMisses() { return oiJoinMisses.get(); }
    public long getOrderbookJoinMisses() { return orderbookJoinMisses.get(); }

    private Map<String, Long> toLongMap(Map<String, AtomicLong> src) {
        Map<String, Long> out = new ConcurrentHashMap<>();
        src.forEach((k, v) -> out.put(k, v.get()));
        return out;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


repository/ScripGroupRepository.java
File type: .java
package com.kotsin.consumer.repository;

import com.kotsin.consumer.entity.ScripGroup;
import org.springframework.data.mongodb.repository.MongoRepository;
import org.springframework.data.mongodb.repository.Query;
import org.springframework.stereotype.Repository;

import java.util.List;

@Repository
public interface ScripGroupRepository extends MongoRepository<ScripGroup, String> {
    
    /**
     * Find all ScripGroups by trading type (EQUITY, FUTURE, OPTION)
     */
    List<ScripGroup> findByTradingType(String tradingType);
    
    /**
     * Find ScripGroup by equity scrip code
     */
    ScripGroup findByEquityScripCode(String equityScripCode);
    
    /**
     * Find ScripGroup containing a specific scrip code in futures
     */
    @Query("{ 'futures.scripCode': ?0 }")
    List<ScripGroup> findByFuturesScripCode(String scripCode);
    
    /**
     * Find ScripGroup containing a specific scrip code in options
     */
    @Query("{ 'options.scripCode': ?0 }")
    List<ScripGroup> findByOptionsScripCode(String scripCode);
    
    /**
     * Find ScripGroup by company name (case insensitive)
     */
    List<ScripGroup> findByCompanyNameIgnoreCase(String companyName);
}


--------------------------------------------------
File End
--------------------------------------------------


retry/RetryHandler.java
File type: .java
package com.kotsin.consumer.retry;

import com.kotsin.consumer.config.ProcessingConstants;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

import java.util.function.Supplier;

/**
 * Retry handler with exponential backoff
 * 
 * RESILIENCE: Handle transient failures gracefully
 * BEST PRACTICE: Exponential backoff to avoid overwhelming services
 */
@Component
@Slf4j
public class RetryHandler {

    /**
     * Execute operation with retry logic
     */
    public <T> T executeWithRetry(Supplier<T> operation, String operationName) {
        return executeWithRetry(
            operation, 
            operationName, 
            ProcessingConstants.MAX_RETRY_ATTEMPTS
        );
    }

    /**
     * Execute operation with custom retry count
     */
    public <T> T executeWithRetry(Supplier<T> operation, String operationName, int maxAttempts) {
        int attempt = 0;
        Exception lastException = null;

        while (attempt < maxAttempts) {
            try {
                return operation.get();
            } catch (Exception e) {
                lastException = e;
                attempt++;
                
                if (attempt >= maxAttempts) {
                    log.error("‚ùå Operation '{}' failed after {} attempts", 
                        operationName, maxAttempts);
                    break;
                }

                long delayMs = calculateBackoffDelay(attempt);
                log.warn("‚ö†Ô∏è Operation '{}' failed (attempt {}/{}). Retrying in {}ms. Error: {}", 
                    operationName, attempt, maxAttempts, delayMs, e.getMessage());

                try {
                    Thread.sleep(delayMs);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Retry interrupted", ie);
                }
            }
        }

        throw new RuntimeException(
            String.format("Operation '%s' failed after %d attempts", operationName, maxAttempts),
            lastException
        );
    }

    /**
     * Execute operation with retry logic (void return)
     */
    public void executeWithRetry(Runnable operation, String operationName) {
        executeWithRetry(() -> {
            operation.run();
            return null;
        }, operationName);
    }

    /**
     * Calculate backoff delay using exponential backoff
     */
    private long calculateBackoffDelay(int attempt) {
        long delay = (long) (ProcessingConstants.INITIAL_RETRY_DELAY_MS * 
            Math.pow(ProcessingConstants.RETRY_BACKOFF_MULTIPLIER, attempt - 1));
        
        return Math.min(delay, ProcessingConstants.MAX_RETRY_DELAY_MS);
    }

    /**
     * Check if exception is retryable
     */
    public boolean isRetryable(Exception e) {
        // Retry on transient failures
        if (e instanceof java.net.SocketTimeoutException ||
            e instanceof java.net.ConnectException ||
            e instanceof java.io.IOException) {
            return true;
        }

        // Check exception message for common transient errors
        String message = e.getMessage();
        if (message != null) {
            return message.contains("timeout") ||
                   message.contains("connection refused") ||
                   message.contains("temporarily unavailable");
        }

        return false;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


util/ValidationUtils.java
File type: .java
package com.kotsin.consumer.util;

import com.kotsin.consumer.model.TickData;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.InstrumentCandle;

import java.util.Objects;
import java.util.Optional;

/**
 * Utility class for consistent null handling and validation
 * 
 * BEST PRACTICE: Centralized validation logic
 * NULL SAFETY: Using Optional and Objects.nonNull consistently
 */
public final class ValidationUtils {

    private ValidationUtils() {
        throw new UnsupportedOperationException("Utility class");
    }

    /**
     * Validate TickData is not null and has required fields
     */
    public static boolean isValid(TickData tick) {
        if (Objects.isNull(tick)) {
            return false;
        }
        if (Objects.isNull(tick.getScripCode()) || tick.getScripCode().trim().isEmpty()) {
            return false;
        }
        if (tick.getTimestamp() <= 0) {
            return false;
        }
        if (Objects.isNull(tick.getLastRate()) || tick.getLastRate() <= 0) {
            return false;
        }
        return true;
    }

    /**
     * Validate OrderBookSnapshot is not null and has data
     */
    public static boolean isValid(OrderBookSnapshot orderbook) {
        return Objects.nonNull(orderbook) &&
               orderbook.isValid() &&
               Objects.nonNull(orderbook.getBids()) &&
               Objects.nonNull(orderbook.getAsks()) &&
               !orderbook.getBids().isEmpty() &&
               !orderbook.getAsks().isEmpty();
    }

    /**
     * Validate InstrumentCandle is not null and has required fields
     */
    public static boolean isValid(InstrumentCandle candle) {
        return Objects.nonNull(candle) &&
               Objects.nonNull(candle.getScripCode()) &&
               Objects.nonNull(candle.getOpen()) &&
               Objects.nonNull(candle.getClose()) &&
               Objects.nonNull(candle.getHigh()) &&
               Objects.nonNull(candle.getLow());
    }

    /**
     * Safe get with default value
     */
    public static <T> T getOrDefault(T value, T defaultValue) {
        return Objects.nonNull(value) ? value : defaultValue;
    }

    /**
     * Safe get as Optional
     */
    public static <T> Optional<T> toOptional(T value) {
        return Optional.ofNullable(value);
    }

    /**
     * Require non-null with custom message
     */
    public static <T> T requireNonNull(T obj, String message) {
        return Objects.requireNonNull(obj, message);
    }

    /**
     * Check if string is null or empty
     */
    public static boolean isNullOrEmpty(String str) {
        return Objects.isNull(str) || str.trim().isEmpty();
    }

    /**
     * Check if string is not null and not empty
     */
    public static boolean isNotNullOrEmpty(String str) {
        return Objects.nonNull(str) && !str.trim().isEmpty();
    }

    /**
     * Safe double comparison
     */
    public static boolean isPositive(Double value) {
        return Objects.nonNull(value) && value > 0;
    }

    /**
     * Safe integer comparison
     */
    public static boolean isPositive(Integer value) {
        return Objects.nonNull(value) && value > 0;
    }

    /**
     * Safe long comparison
     */
    public static boolean isPositive(Long value) {
        return Objects.nonNull(value) && value > 0;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


util/MarketTimeAligner.java
File type: .java
package com.kotsin.consumer.util;

/**
 * A utility class to align timestamps to market-specific trading windows.
 * This is a core component for ensuring that candlestick windows match the
 * real-world trading schedules of NSE and MCX.
 */
public final class MarketTimeAligner {

    public static int getWindowOffsetMinutes(String exchange, int windowSizeMinutes) {
        // NSE opens effectively on :15; MCX on :00
        int base = "N".equalsIgnoreCase(exchange) ? 15 : 0;
        // Reduce modulo window size so 2/3/5/15/30 align correctly
        int mod = ((base % windowSizeMinutes) + windowSizeMinutes) % windowSizeMinutes;
        return mod;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


config/KafkaConfig.java
File type: .java
package com.kotsin.consumer.config;


import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.errors.LogAndContinueExceptionHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.io.File;
import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.util.Properties;

@Component
public class KafkaConfig {

    public static final ZonedDateTime START_3RD_APRIL_915 = ZonedDateTime.of(
            2025, 4, 3, 9, 15, 0, 0, ZoneId.of("Asia/Kolkata")
    );
    public static final ZonedDateTime END_3RD_APRIL_1530 = ZonedDateTime.of(
            2025, 4, 3, 15, 30, 0, 0, ZoneId.of("Asia/Kolkata")
    );

    // All configuration injected from application.properties
    @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
    private String bootstrapServers;

    @Value("${spring.kafka.streams.state-dir:/tmp/kafka-streams/streamingcandle}")
    private String baseStateDir;

    @Value("${spring.kafka.streams.properties.commit.interval.ms:100}")
    private int commitIntervalMs;

    @Value("${spring.kafka.streams.properties.statestore.cache.max.bytes:104857600}")
    private long statestoreCacheMaxBytes;

    @Value("${spring.kafka.streams.properties.num.stream.threads:2}")
    private int numStreamThreads;

    @Value("${spring.kafka.streams.properties.processing.guarantee:at_least_once}")
    private String processingGuarantee;

    @Value("${spring.kafka.streams.properties.auto.offset.reset:earliest}")
    private String autoOffsetReset;

    @Value("${spring.kafka.streams.properties.retry.backoff.ms:100}")
    private long retryBackoffMs;

    @Value("${spring.kafka.streams.properties.reconnect.backoff.ms:50}")
    private long reconnectBackoffMs;

    @Value("${spring.kafka.streams.properties.request.timeout.ms:40000}")
    private int requestTimeoutMs;

    @Value("${spring.kafka.streams.properties.topology.optimization:all}")
    private String topologyOptimization;

    @Value("${spring.kafka.streams.properties.default.deserialization.exception.handler:org.apache.kafka.streams.errors.LogAndContinueExceptionHandler}")
    private String deserializationExceptionHandler;

    @Value("${spring.kafka.streams.properties.producer.message.timestamp.type:CreateTime}")
    private String producerTimestampType;

    /**
     * Gets the bootstrap servers configuration.
     *
     * @return The bootstrap servers string.
     */
    public String getBootstrapServers() {
        return bootstrapServers;
    }

    /**
     * Retrieves Kafka Streams properties with a given application ID.
     * All settings are now configurable via application.properties.
     *
     * @param appId The application ID for the Kafka Streams instance.
     * @return Properties configured for Kafka Streams.
     */
    public Properties getStreamProperties(String appId) {
        Properties props = new Properties();

        // Core configuration
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());

        // State directory (unique per application instance)
        String uniqueStateDir = createUniqueStateDir(appId);
        props.put(StreamsConfig.STATE_DIR_CONFIG, uniqueStateDir);

        // Performance tuning (all configurable from properties)
        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, commitIntervalMs);
        props.put(StreamsConfig.STATESTORE_CACHE_MAX_BYTES_CONFIG, statestoreCacheMaxBytes);
        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numStreamThreads);

        // Processing guarantee (at_least_once or exactly_once_v2)
        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, processingGuarantee);

        // Topology optimization
        props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, topologyOptimization);

        // Exception handling
        props.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG,
                deserializationExceptionHandler);

        // Consumer configuration
        props.put(StreamsConfig.CONSUMER_PREFIX + "auto.offset.reset", autoOffsetReset);

        // Producer configuration
        props.put("producer.message.timestamp.type", producerTimestampType);

        // Resilience configuration
        props.put(StreamsConfig.RETRY_BACKOFF_MS_CONFIG, retryBackoffMs);
        props.put(StreamsConfig.RECONNECT_BACKOFF_MS_CONFIG, reconnectBackoffMs);
        props.put(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);

        return props;
    }
    
    /**
     * Creates a unique state directory for the Kafka Streams application to prevent conflicts.
     * PRODUCTION FIX: Ensures each instance gets its own state directory.
     */
    private String createUniqueStateDir(String appId) {
        // Create base directory if it doesn't exist
        File baseDir = new File(baseStateDir);
        if (!baseDir.exists()) {
            boolean created = baseDir.mkdirs();
            if (!created) {
                // Fallback to temp directory if we can't create in /var/lib
                String fallbackPath = System.getProperty("java.io.tmpdir") + "/kafka-streams/" + appId;
                System.err.println("WARNING: Could not create state directory " + baseStateDir + 
                                 ", falling back to " + fallbackPath);
                return fallbackPath;
            }
        }
        
        // Return path for this application instance
        return baseStateDir + "/" + appId;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


config/ConfigurationValidator.java
File type: .java
package com.kotsin.consumer.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Component;

import java.time.Duration;
import java.time.LocalTime;
import java.util.ArrayList;
import java.util.List;

/**
 * Configuration validator to ensure all required properties are set
 * 
 * BEST PRACTICE: Fail fast on invalid configuration
 * PRODUCTION READY: Validate configuration on startup
 */
@Component
@Slf4j
public class ConfigurationValidator {

    @Value("${spring.profiles.active:default}")
    private String activeProfile;
    
    @Value("${spring.kafka.bootstrap-servers:}")
    private String bootstrapServers;
    
    @Value("${spring.kafka.streams.application-id:}")
    private String applicationId;
    
    @Value("${unified.input.topic.ticks:}")
    private String ticksTopic;
    
    @Value("${unified.input.topic.oi:}")
    private String oiTopic;
    
    @Value("${unified.input.topic.orderbook:}")
    private String orderbookTopic;
    
    @Value("${stream.outputs.candles.1m:}")
    private String candle1mTopic;
    
    @Value("${spring.data.mongodb.uri:}")
    private String mongoUri;
    
    @Value("${trading.hours.nse.start:09:15}")
    private String nseStartTime;
    
    @Value("${trading.hours.nse.end:15:30}")
    private String nseEndTime;

    @EventListener(ApplicationReadyEvent.class)
    public void validateConfiguration() {
        // Skip validation in test mode
        if ("test".equals(activeProfile)) {
            log.info("‚è≠Ô∏è Skipping configuration validation in test mode");
            return;
        }
        
        log.info("üîç Validating application configuration...");
        
        List<String> errors = new ArrayList<>();
        
        // Validate Kafka configuration
        if (isNullOrEmpty(bootstrapServers)) {
            errors.add("spring.kafka.bootstrap-servers is not configured");
        }
        
        if (isNullOrEmpty(applicationId)) {
            errors.add("spring.kafka.streams.application-id is not configured");
        }
        
        // Validate input topics
        if (isNullOrEmpty(ticksTopic)) {
            errors.add("unified.input.topic.ticks is not configured");
        }
        
        if (isNullOrEmpty(oiTopic)) {
            errors.add("unified.input.topic.oi is not configured");
        }
        
        if (isNullOrEmpty(orderbookTopic)) {
            errors.add("unified.input.topic.orderbook is not configured");
        }
        
        // Validate output topics
        if (isNullOrEmpty(candle1mTopic)) {
            errors.add("stream.outputs.candles.1m is not configured");
        }
        
        // Validate MongoDB configuration
        if (isNullOrEmpty(mongoUri)) {
            log.warn("‚ö†Ô∏è spring.data.mongodb.uri is not configured - MongoDB features will be disabled");
        }
        
        // Validate trading hours
        try {
            LocalTime.parse(nseStartTime);
            LocalTime.parse(nseEndTime);
        } catch (Exception e) {
            errors.add("Invalid trading hours format: " + e.getMessage());
        }
        
        // Report validation results
        if (!errors.isEmpty()) {
            log.error("‚ùå Configuration validation failed with {} errors:", errors.size());
            errors.forEach(error -> log.error("  - {}", error));
            throw new IllegalStateException("Configuration validation failed. Please fix the errors above.");
        }
        
        log.info("‚úÖ Configuration validation passed");
        logConfigurationSummary();
    }

    private void logConfigurationSummary() {
        log.info("üìã Configuration Summary:");
        log.info("  Kafka Bootstrap Servers: {}", bootstrapServers);
        log.info("  Application ID: {}", applicationId);
        log.info("  Input Topics: ticks={}, oi={}, orderbook={}", ticksTopic, oiTopic, orderbookTopic);
        log.info("  Output Topics: 1m={}", candle1mTopic);
        log.info("  MongoDB URI: {}", maskUri(mongoUri));
        log.info("  Trading Hours: {} - {}", nseStartTime, nseEndTime);
    }

    private boolean isNullOrEmpty(String str) {
        return str == null || str.trim().isEmpty();
    }

    private String maskUri(String uri) {
        if (isNullOrEmpty(uri)) {
            return "not configured";
        }
        // Mask password in URI
        return uri.replaceAll(":[^:@]+@", ":****@");
    }
}


--------------------------------------------------
File End
--------------------------------------------------


config/ProcessingConstants.java
File type: .java
package com.kotsin.consumer.config;

import java.time.Duration;

/**
 * Central constants for market data processing
 * 
 * BEST PRACTICE: Extract magic numbers to named constants
 * MAINTAINABILITY: Single source of truth for configuration values
 */
public final class ProcessingConstants {

    private ProcessingConstants() {
        throw new UnsupportedOperationException("Constants class");
    }

    // ========== TIMEFRAME CONSTANTS ==========
    
    public static final Duration TIMEFRAME_1M = Duration.ofMinutes(1);
    public static final Duration TIMEFRAME_2M = Duration.ofMinutes(2);
    public static final Duration TIMEFRAME_3M = Duration.ofMinutes(3);
    public static final Duration TIMEFRAME_5M = Duration.ofMinutes(5);
    public static final Duration TIMEFRAME_15M = Duration.ofMinutes(15);
    public static final Duration TIMEFRAME_30M = Duration.ofMinutes(30);
    
    public static final Duration WINDOW_GRACE_PERIOD = Duration.ofSeconds(10);
    public static final Duration HALT_TIMEOUT = Duration.ofMinutes(20);
    
    // ========== TRADING HOURS CONSTANTS ==========
    
    public static final int TRADING_HOURS_BUFFER_MINUTES = 15;
    public static final int SATURDAY_DAY_OF_WEEK = 6;
    public static final int SUNDAY_DAY_OF_WEEK = 7;
    public static final int MIN_TRADING_HOUR = 8;
    public static final int MAX_TRADING_HOUR = 17;
    
    // ========== MICROSTRUCTURE CONSTANTS ==========
    
    public static final int MIN_OBSERVATIONS = 10;
    public static final int VPIN_BUCKET_SIZE = 50;
    public static final int ORDERBOOK_HISTORY_SIZE = 20;
    public static final int PRICE_CHANGES_HISTORY_SIZE = 50;
    public static final int SIGNED_VOLUMES_HISTORY_SIZE = 50;
    
    // ========== FAMILY AGGREGATION CONSTANTS ==========
    
    public static final int MAX_OPTIONS_PER_FAMILY = 4;
    public static final int MAX_FUTURES_PER_FAMILY = 1;
    
    // ========== VALIDATION CONSTANTS ==========
    
    public static final long MAX_TIMESTAMP_DEVIATION_MS = 7L * 24 * 3600 * 1000; // 7 days
    public static final double MIN_VALID_PRICE = 0.01;
    public static final int MIN_VALID_VOLUME = 0;
    public static final int MAX_ORDERBOOK_LEVELS = 20;
    
    // ========== CACHE CONSTANTS ==========
    
    public static final Duration CACHE_TTL = Duration.ofDays(1);
    public static final int CACHE_REFRESH_HOUR = 3; // 3 AM
    
    // ========== KAFKA CONSTANTS ==========
    
    public static final int MAX_POLL_RECORDS = 100;
    public static final int COMMIT_INTERVAL_MS = 1000;
    public static final int SESSION_TIMEOUT_MS = 30000;
    public static final int MAX_POLL_INTERVAL_MS = 300000;
    
    // ========== BACKPRESSURE CONSTANTS ==========
    
    public static final long LAG_THRESHOLD = 1000;
    public static final double THROTTLE_FACTOR = 0.5;
    public static final double MAX_LAG_PERCENTAGE = 0.1; // 10%
    
    // ========== PERFORMANCE CONSTANTS ==========
    
    public static final int THREAD_POOL_SIZE = 4;
    public static final int QUEUE_CAPACITY = 1000;
    public static final Duration SHUTDOWN_TIMEOUT = Duration.ofSeconds(30);
    
    // ========== RETRY CONSTANTS ==========
    
    public static final int MAX_RETRY_ATTEMPTS = 3;
    public static final long INITIAL_RETRY_DELAY_MS = 100;
    public static final double RETRY_BACKOFF_MULTIPLIER = 2.0;
    public static final long MAX_RETRY_DELAY_MS = 10000;
    
    // ========== TIMEOUT CONSTANTS ==========
    
    public static final Duration DATABASE_TIMEOUT = Duration.ofSeconds(5);
    public static final Duration NETWORK_TIMEOUT = Duration.ofSeconds(10);
    public static final Duration PROCESSING_TIMEOUT = Duration.ofSeconds(30);
    
    // ========== MONITORING CONSTANTS ==========
    
    public static final int METRICS_REPORT_INTERVAL_SECONDS = 60;
    public static final int HEALTH_CHECK_INTERVAL_SECONDS = 30;
    public static final double ERROR_RATE_THRESHOLD = 0.05; // 5%
    
    // ========== ORDERBOOK ANALYTICS CONSTANTS ==========
    
    public static final double ICEBERG_THRESHOLD = 0.3; // 30% hidden volume
    public static final double SPOOFING_THRESHOLD = 0.5; // 50% volume cancelled
    public static final int MIN_SPOOFING_EVENTS = 3;
    public static final Duration SPOOFING_TIME_WINDOW = Duration.ofMinutes(5);
    
    // ========== INDEX PATTERNS ==========
    
    public static final String INDEX_SCRIP_CODE_PREFIX = "99992";
    
    // ========== EXCHANGE CODES ==========
    
    public static final String EXCHANGE_NSE = "NSE";
    public static final String EXCHANGE_MCX = "MCX";
    public static final String EXCHANGE_TYPE_DERIVATIVE = "D";
    
    // ========== OPTION TYPES ==========
    
    public static final String OPTION_TYPE_CALL = "CE";
    public static final String OPTION_TYPE_PUT = "PE";
    
    // ========== LOGGING CONSTANTS ==========
    
    public static final int MAX_LOG_MESSAGE_LENGTH = 1000;
    public static final String LOG_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss.SSS";
    
    // ========== VALIDATION THRESHOLDS ==========
    
    public static final double MIN_EFFECTIVE_SPREAD = 0.0;
    public static final double MAX_EFFECTIVE_SPREAD = 1.0;
    public static final double MIN_DEPTH_IMBALANCE = -1.0;
    public static final double MAX_DEPTH_IMBALANCE = 1.0;
    public static final double MIN_OFI = -1000000.0;
    public static final double MAX_OFI = 1000000.0;
    
    // ========== GRACEFUL SHUTDOWN CONSTANTS ==========
    
    public static final Duration SHUTDOWN_GRACE_PERIOD = Duration.ofSeconds(2);
    public static final int SHUTDOWN_STEPS = 5;
}


--------------------------------------------------
File End
--------------------------------------------------


entity/ScripGroup.java
File type: .java
package com.kotsin.consumer.entity;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.springframework.data.mongodb.core.mapping.Document;
import org.springframework.data.mongodb.core.mapping.MongoId;

import java.util.ArrayList;
import java.util.Date;
import java.util.List;

@Data
@NoArgsConstructor
@AllArgsConstructor
@Document(collection = "ScripGroup")
@JsonIgnoreProperties(ignoreUnknown = true)
public class ScripGroup {
    @MongoId
    private String equityScripCode;
    private String tradingType;
    private String companyName;
    private Scrip equity;                // The equity scrip
    private List<Scrip> futures = new ArrayList<>(); // All FUTs
    private List<Scrip> options = new ArrayList<>(); // All OPTs (CE or PE)
    private double closePrice;                     // Dynamically-fetched close price
    private Date insertionDate;
}


--------------------------------------------------
File End
--------------------------------------------------


entity/Scrip.java
File type: .java
package com.kotsin.consumer.entity;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.ToString;
import org.springframework.data.mongodb.core.mapping.Document;
import org.springframework.data.mongodb.core.mapping.MongoId;

import java.util.Date;

@Data
@ToString
@NoArgsConstructor
@AllArgsConstructor
@Document(collection = "scripData")
public class Scrip {
    @MongoId
    private String id;
    private String scriptTypeKotsin;
    private String Exch;
    private String ExchType;
    private String ScripCode;
    private String Name;
    private String Expiry;
    private String ScripType;
    private String StrikeRate;
    private String FullName;
    private String TickSize;
    private String LotSize;
    private String QtyLimit;
    private String Multiplier;
    private String SymbolRoot;
    private String BOCOAllowed;
    private String ISIN;
    private String ScripData;
    private String Series;
    private Date insertionDate;
}


--------------------------------------------------
File End
--------------------------------------------------


processor/MicrostructureAccumulator.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.model.MicrostructureData;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.TickData;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;

import java.util.*;

/**
 * Accumulator for market microstructure metrics
 * 
 * CRITICAL FIXES (per refactoring.md critique):
 * 1. OFI: Implements full-depth calculation per Cont-Kukanov-Stoikov 2014
 * 2. VPIN: Uses BVC (Bulk Volume Classification) with adaptive buckets
 * 3. Kyle's Lambda: Uses signed order flow (VAR/Hasbrouck method)
 * 
 * Design Pattern: Accumulator
 * Thread-Safety: Not thread-safe (used within single Kafka Streams task)
 */
@Data
@Slf4j
public class MicrostructureAccumulator {
    
    // ========== OFI (Order Flow Imbalance) ==========
    // Per Cont-Kukanov-Stoikov 2014: OFI = ŒîBid^depth - ŒîAsk^depth
    private Double ofi = 0.0;
    private Map<Double, Integer> prevBidDepth = new HashMap<>();
    private Map<Double, Integer> prevAskDepth = new HashMap<>();
    private double prevBestBid = 0.0;
    private double prevBestAsk = 0.0;
    
    // ========== VPIN (Volume-Synchronized PIN) ==========
    // Per Easley-Lopez de Prado-O'Hara 2012
    private static final int VPIN_BUCKET_COUNT = 50;
    private double adaptiveBucketSize = 10000.0;  // Adjusted dynamically
    private final Deque<VPINBucket> vpinBuckets = new ArrayDeque<>(VPIN_BUCKET_COUNT);
    private double currentBucketVolume = 0.0;
    private double currentBucketBuyVolume = 0.0;
    private Double vpin = 0.0;
    private int totalBucketsCreated = 0;
    
    // ========== Kyle's Lambda (Price Impact) ==========
    // Per Hasbrouck VAR estimation
    private final Deque<PriceImpactObservation> priceImpactHistory = new ArrayDeque<>(100);
    private Double kyleLambda = 0.0;
    private double lastMidPrice = 0.0;
    
    // ========== Bid-Ask Metrics ==========
    private Double depthImbalance = 0.0;
    private Double effectiveSpread = 0.0;
    private Double midPrice = 0.0;
    private Double microprice = 0.0;
    private Double bidAskSpread = 0.0;
    
    // ========== State ==========
    private boolean complete = false;
    private int updateCount = 0;
    private static final int MIN_OBSERVATIONS = 20;
    
    /**
     * VPIN bucket for volume-synchronized calculation
     */
    @Data
    private static class VPINBucket {
        double totalVolume;
        double buyVolume;
        double sellVolume;
        
        double getImbalance() {
            return Math.abs(buyVolume - sellVolume);
        }
    }
    
    /**
     * Price impact observation for Kyle's Lambda
     */
    @Data
    private static class PriceImpactObservation {
        double priceChange;      // Change in mid-price
        double signedVolume;     // Positive for buy, negative for sell
        long timestamp;
    }
    
    /**
     * Add tick with orderbook update (preferred method)
     * CRITICAL: Use this when full orderbook snapshot is available
     */
    public void addTick(TickData tick, OrderBookSnapshot orderbook) {
        if (tick == null || tick.getLastRate() <= 0) {
            return;
        }
        
        // If orderbook provided, use full depth calculations
        if (orderbook != null && orderbook.isValid()) {
            addTickWithFullDepth(tick, orderbook);
        } else {
            // Fallback to L1 approximation
            addTickWithL1Only(tick);
        }
        
        updateCount++;
        if (updateCount >= MIN_OBSERVATIONS) {
            complete = true;
        }
    }
    
    /**
     * Add tick only (backward compatibility)
     * Uses L1 approximation when full orderbook not available
     */
    public void addTick(TickData tick) {
        addTick(tick, tick.getFullOrderbook());
    }
    
    /**
     * CRITICAL FIX #1: Full-depth OFI calculation
     * Per Cont-Kukanov-Stoikov 2014
     */
    private void addTickWithFullDepth(TickData tick, OrderBookSnapshot orderbook) {
        // Parse orderbook to get full depth
        orderbook.parseDetails();
        
        // Calculate mid-price and microprice
        double bestBid = orderbook.getBestBid();
        double bestAsk = orderbook.getBestAsk();
        
        if (bestBid > 0 && bestAsk > 0) {
            midPrice = (bestBid + bestAsk) / 2.0;
            bidAskSpread = bestAsk - bestBid;
            
            // Calculate microprice (volume-weighted)
            // Extract quantities from first level
            int bidQty = (orderbook.getAllBids() != null && !orderbook.getAllBids().isEmpty()) ? 
                orderbook.getAllBids().get(0).getQuantity() : 0;
            int askQty = (orderbook.getAllAsks() != null && !orderbook.getAllAsks().isEmpty()) ? 
                orderbook.getAllAsks().get(0).getQuantity() : 0;
            
            if (bidQty + askQty > 0) {
                microprice = (bestBid * askQty + bestAsk * bidQty) / (double)(bidQty + askQty);
            }
            
            // Calculate effective spread
            double tradePrice = tick.getLastRate();
            effectiveSpread = 2.0 * Math.abs(tradePrice - midPrice);
        }
        
        // Calculate depth imbalance
        // Sum all bid/ask quantities
        int totalBidQty = (orderbook.getTotalBidQty() != null) ? 
            orderbook.getTotalBidQty().intValue() : 0;
        int totalAskQty = (orderbook.getTotalOffQty() != null) ? 
            orderbook.getTotalOffQty().intValue() : 0;
        
        if (totalBidQty + totalAskQty > 0) {
            depthImbalance = (totalBidQty - totalAskQty) / (double)(totalBidQty + totalAskQty);
        }
        
        // Build depth maps from orderbook
        Map<Double, Integer> currentBidDepth = buildDepthMap(orderbook.getAllBids());
        Map<Double, Integer> currentAskDepth = buildDepthMap(orderbook.getAllAsks());
        
        // Calculate OFI (full depth)
        if (!prevBidDepth.isEmpty() && !prevAskDepth.isEmpty()) {
            ofi = calculateFullDepthOFI(
                prevBidDepth, currentBidDepth, prevBestBid, bestBid,
                prevAskDepth, currentAskDepth, prevBestAsk, bestAsk
            );
        }
        
        // Update previous state
        prevBidDepth = currentBidDepth;
        prevAskDepth = currentAskDepth;
        prevBestBid = bestBid;
        prevBestAsk = bestAsk;
        
        // Update VPIN with BVC trade classification
        if (tick.getDeltaVolume() != null && tick.getDeltaVolume() > 0) {
            boolean isBuy = classifyTradeBVC(tick, orderbook);
            updateVPIN(tick.getDeltaVolume(), isBuy);
        }
        
        // Update Kyle's Lambda with signed volume
        if (midPrice > 0 && tick.getDeltaVolume() != null) {
            updateKyleLambda(tick, midPrice);
        }
    }
    
    /**
     * Fallback: L1-only approximation (when full orderbook not available)
     */
    private void addTickWithL1Only(TickData tick) {
        double bidPrice = tick.getBidRate();
        double askPrice = tick.getOfferRate();
        int bidQty = tick.getBidQuantity();
        int askQty = tick.getOfferQuantity();
        
        if (bidPrice > 0 && askPrice > 0) {
            midPrice = (bidPrice + askPrice) / 2.0;
            bidAskSpread = askPrice - bidPrice;
            
            if (bidQty + askQty > 0) {
                microprice = (bidPrice * askQty + askPrice * bidQty) / (double)(bidQty + askQty);
            }
            
            effectiveSpread = 2.0 * Math.abs(tick.getLastRate() - midPrice);
        }
        
        // Simplified depth imbalance from L1
        int totalBidQty = tick.getTotalBidQuantity();
        int totalAskQty = tick.getTotalOfferQuantity();
        if (totalBidQty + totalAskQty > 0) {
            depthImbalance = (totalBidQty - totalAskQty) / (double)(totalBidQty + totalAskQty);
        }
        
        // Simplified OFI (L1 only)
        if (prevBestBid > 0 && prevBestAsk > 0) {
            ofi = calculateL1OFI(bidPrice, bidQty, prevBestBid, askPrice, askQty, prevBestAsk);
        }
        
        prevBestBid = bidPrice;
        prevBestAsk = askPrice;
        
        // VPIN with simple tick rule
        if (tick.getDeltaVolume() != null && tick.getDeltaVolume() > 0) {
            boolean isBuy = tick.getLastRate() >= midPrice;
            updateVPIN(tick.getDeltaVolume(), isBuy);
        }
        
        // Kyle's Lambda
        if (midPrice > 0 && tick.getDeltaVolume() != null) {
            updateKyleLambda(tick, midPrice);
        }
    }
    
    /**
     * CRITICAL FIX: Full-depth OFI calculation
     * Formula: OFI = ŒîBid^depth - ŒîAsk^depth
     * where ŒîBid^depth = Œ£_{p‚â•p^b_{t-1}} q^b_t(p) - Œ£_{p‚â•p^b_t} q^b_{t-1}(p)
     */
    private double calculateFullDepthOFI(
        Map<Double, Integer> prevBid, Map<Double, Integer> currBid, double prevBestBid, double currBestBid,
        Map<Double, Integer> prevAsk, Map<Double, Integer> currAsk, double prevBestAsk, double currBestAsk
    ) {
        double deltaBid = 0.0;
        double deltaAsk = 0.0;
        
        // Calculate bid side: sum over prices >= prev_best_bid
        for (Map.Entry<Double, Integer> entry : currBid.entrySet()) {
            if (entry.getKey() >= prevBestBid) {
                deltaBid += entry.getValue();
            }
        }
        
        // Subtract quantity at prices >= curr_best_bid from previous snapshot
        for (Map.Entry<Double, Integer> entry : prevBid.entrySet()) {
            if (entry.getKey() >= currBestBid) {
                deltaBid -= entry.getValue();
            }
        }
        
        // Calculate ask side: sum over prices <= prev_best_ask
        for (Map.Entry<Double, Integer> entry : currAsk.entrySet()) {
            if (entry.getKey() <= prevBestAsk) {
                deltaAsk += entry.getValue();
            }
        }
        
        // Subtract quantity at prices <= curr_best_ask from previous snapshot
        for (Map.Entry<Double, Integer> entry : prevAsk.entrySet()) {
            if (entry.getKey() <= currBestAsk) {
                deltaAsk -= entry.getValue();
            }
        }
        
        return deltaBid - deltaAsk;
    }
    
    /**
     * Simplified L1 OFI (fallback when full depth not available)
     */
    private double calculateL1OFI(double bidPrice, int bidQty, double prevBidPrice,
                                   double askPrice, int askQty, double prevAskPrice) {
        double deltaBid = (bidPrice >= prevBidPrice) ? bidQty : -bidQty;
        double deltaAsk = (askPrice <= prevAskPrice) ? askQty : -askQty;
        return deltaBid - deltaAsk;
    }
    
    /**
     * Build depth map from orderbook levels
     */
    private Map<Double, Integer> buildDepthMap(List<OrderBookSnapshot.OrderBookLevel> levels) {
        Map<Double, Integer> depthMap = new HashMap<>();
        if (levels != null) {
            for (OrderBookSnapshot.OrderBookLevel level : levels) {
                if (level.getPrice() > 0 && level.getQuantity() > 0) {
                    depthMap.put(level.getPrice(), level.getQuantity());
                }
            }
        }
        return depthMap;
    }
    
    /**
     * CRITICAL FIX #2: BVC (Bulk Volume Classification) for trade direction
     * Uses microprice instead of simple mid-price
     */
    private boolean classifyTradeBVC(TickData tick, OrderBookSnapshot orderbook) {
        double tradePrice = tick.getLastRate();
        
        // Calculate microprice (more accurate than mid-price)
        double mp = microprice;
        if (mp == 0.0) {
            // Fallback to mid-price
            mp = midPrice;
        }
        
        // BVC rule: compare to microprice
        if (Math.abs(tradePrice - mp) < 0.0001) {
            // On microprice ‚Üí use previous classification or tick rule
            return tradePrice >= midPrice;
        }
        
        return tradePrice > mp;  // Above microprice = buy
    }
    
    /**
     * CRITICAL FIX #3: VPIN with adaptive bucket sizing
     */
    private void updateVPIN(int volume, boolean isBuy) {
        currentBucketVolume += volume;
        if (isBuy) {
            currentBucketBuyVolume += volume;
        }
        
        // Adaptive bucket size (increases with market activity)
        if (totalBucketsCreated > 0 && totalBucketsCreated % 20 == 0) {
            // Recalculate adaptive bucket size based on recent volume
            double avgVolume = vpinBuckets.stream()
                .mapToDouble(VPINBucket::getTotalVolume)
                .average()
                .orElse(10000.0);
            adaptiveBucketSize = avgVolume * 1.2; // 20% buffer
        }
        
        // Close bucket when full
        if (currentBucketVolume >= adaptiveBucketSize) {
            VPINBucket bucket = new VPINBucket();
            bucket.totalVolume = currentBucketVolume;
            bucket.buyVolume = currentBucketBuyVolume;
            bucket.sellVolume = currentBucketVolume - currentBucketBuyVolume;
            
            vpinBuckets.addLast(bucket);
            totalBucketsCreated++;
            
            // Keep only last N buckets (O(1) removal with Deque)
            if (vpinBuckets.size() > VPIN_BUCKET_COUNT) {
                vpinBuckets.removeFirst();
            }
            
            // Reset current bucket
            currentBucketVolume = 0.0;
            currentBucketBuyVolume = 0.0;
            
            // Calculate VPIN
            calculateVPIN();
        }
    }
    
    /**
     * Calculate VPIN from buckets
     * CORRECT Formula per Easley-L√≥pez de Prado-O'Hara (2012):
     * VPIN = (1/n) √ó Œ£|V_buy - V_sell| / V_bucket
     * 
     * This gives average order imbalance per bucket normalized by bucket size
     */
    private void calculateVPIN() {
        if (vpinBuckets.size() < 10) {
            vpin = 0.0;
            return;
        }
        
        int n = vpinBuckets.size();
        
        // Sum of absolute imbalances: Œ£|V_buy - V_sell|
        double totalImbalance = vpinBuckets.stream()
            .mapToDouble(VPINBucket::getImbalance)
            .sum();
        
        // Average bucket volume
        double avgBucketVolume = vpinBuckets.stream()
            .mapToDouble(VPINBucket::getTotalVolume)
            .average()
            .orElse(1.0);
        
        // VPIN = (1/n) √ó Œ£|V_buy - V_sell| / V_bucket
        vpin = (avgBucketVolume > 0) ? ((1.0 / n) * (totalImbalance / avgBucketVolume)) : 0.0;
    }
    
    /**
     * CRITICAL FIX #4: Kyle's Lambda with signed order flow
     * Uses VAR/Hasbrouck estimation: Œª = Cov(ŒîP, q) / Var(q)
     * where q is SIGNED volume (positive for buy, negative for sell)
     */
    private void updateKyleLambda(TickData tick, double currentMidPrice) {
        if (lastMidPrice == 0.0) {
            lastMidPrice = currentMidPrice;
            return;
        }
        
        double priceChange = currentMidPrice - lastMidPrice;
        
        // Classify trade direction for signed volume
        boolean isBuy = (microprice > 0) ? 
            tick.getLastRate() > microprice : 
            tick.getLastRate() >= midPrice;
        
        double signedVolume = isBuy ? 
            tick.getDeltaVolume() : 
            -tick.getDeltaVolume();
        
        // Store observation
        PriceImpactObservation obs = new PriceImpactObservation();
        obs.priceChange = priceChange;
        obs.signedVolume = signedVolume;
        obs.timestamp = tick.getTimestamp();
        
        priceImpactHistory.addLast(obs);
        
        // Keep only last 100 observations (O(1) removal with Deque)
        if (priceImpactHistory.size() > 100) {
            priceImpactHistory.removeFirst();
        }
        
        // Calculate Kyle's Lambda (regression coefficient)
        if (priceImpactHistory.size() >= MIN_OBSERVATIONS) {
            kyleLambda = calculateKyleLambdaRegression();
        }
        
        lastMidPrice = currentMidPrice;
    }
    
    /**
     * Calculate Kyle's Lambda via OLS regression
     * Œª = Cov(ŒîP, q) / Var(q)
     */
    private double calculateKyleLambdaRegression() {
        // Calculate means
        double meanPriceChange = priceImpactHistory.stream()
            .mapToDouble(PriceImpactObservation::getPriceChange)
            .average()
            .orElse(0.0);
        
        double meanSignedVolume = priceImpactHistory.stream()
            .mapToDouble(PriceImpactObservation::getSignedVolume)
            .average()
            .orElse(0.0);
        
        // Calculate covariance and variance
        double covariance = 0.0;
        double varianceVolume = 0.0;
        
        for (PriceImpactObservation obs : priceImpactHistory) {
            double pDev = obs.priceChange - meanPriceChange;
            double vDev = obs.signedVolume - meanSignedVolume;
            covariance += pDev * vDev;
            varianceVolume += vDev * vDev;
        }
        
        int n = priceImpactHistory.size();
        covariance /= n;
        varianceVolume /= n;
        
        // Kyle's Lambda = Cov / Var
        return (varianceVolume > 0) ? (covariance / varianceVolume) : 0.0;
    }
    
    /**
     * Mark as complete
     */
    public void markComplete() {
        complete = true;
    }
    
    /**
     * Build microstructure data output
     */
    public MicrostructureData toMicrostructureData(Long windowStart, Long windowEnd) {
        return MicrostructureData.builder()
            .ofi(ofi)
            .vpin(vpin)
            .depthImbalance(depthImbalance)
            .kyleLambda(kyleLambda)
            .effectiveSpread(effectiveSpread)
            .microprice(microprice)
            .bidAskSpread(bidAskSpread)
            .midPrice(midPrice)
            .isComplete(complete)
            .windowStart(windowStart)
            .windowEnd(windowEnd)
            .build();
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/UnifiedMarketDataProcessor.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.processor.MarketDataOrchestrator;
import com.kotsin.consumer.service.StreamMetrics;
import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.util.Map;

/**
 * Unified Market Data Processor (Refactored - God Class Split)
 * 
 * REFACTORED: Split into focused services:
 * - TopologyConfiguration: Topology building
 * - InstrumentProcessor: Instrument-level processing  
 * - DataEnrichmentService: Data enrichment
 * - MarketDataOrchestrator: Stream lifecycle management
 * 
 * This class now acts as a thin coordinator
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class UnifiedMarketDataProcessor {

    private final MarketDataOrchestrator orchestrator;
    private final StreamMetrics metrics;
    
    @Value("${spring.kafka.streams.application-id:unified-market-processor1}")
    private String appIdPrefix;
    
    @Value("${unified.input.topic.ticks:forwardtesting-data}")
    private String ticksTopic;
    
    @Value("${unified.input.topic.oi:OpenInterest}")
    private String oiTopic;
    
    @Value("${unified.input.topic.orderbook:Orderbook}")
    private String orderbookTopic;

    @Value("${stream.outputs.candles.enabled:true}")
    private boolean candlesOutputEnabled;

    @Value("${stream.outputs.familyStructured.enabled:false}")
    private boolean familyStructuredEnabled;

    @Value("${stream.outputs.candles.1m:candle-complete-1m}")
    private String candle1mTopic;

    @Value("${stream.outputs.candles.2m:candle-complete-2m}")
    private String candle2mTopic;

    @Value("${stream.outputs.candles.3m:candle-complete-3m}")
    private String candle3mTopic;

    @Value("${stream.outputs.candles.5m:candle-complete-5m}")
    private String candle5mTopic;

    @Value("${stream.outputs.candles.15m:candle-complete-15m}")
    private String candle15mTopic;

    @Value("${stream.outputs.candles.30m:candle-complete-30m}")
    private String candle30mTopic;

    @Value("${stream.outputs.familyStructured.1m:family-structured-1m}")
    private String familyStructured1mTopic;

    @Value("${stream.outputs.familyStructured.2m:family-structured-2m}")
    private String familyStructured2mTopic;

    @Value("${stream.outputs.familyStructured.5m:family-structured-5m}")
    private String familyStructured5mTopic;

    @Value("${stream.outputs.familyStructured.15m:family-structured-15m}")
    private String familyStructured15mTopic;

    @Value("${stream.outputs.familyStructured.30m:family-structured-30m}")
    private String familyStructured30mTopic;

    @Value("${stream.outputs.familyStructured.all:family-structured-all}")
    private String familyStructuredAllTopic;
    
    /**
     * Start the unified market data processor
     */
    @PostConstruct
    public void start() {
        try {
            log.info("üöÄ Starting Unified Market Data Processor (Refactored)");
            log.info("Flags: candlesOutputEnabled={}, familyStructuredEnabled={}", candlesOutputEnabled, familyStructuredEnabled);
            log.info("Input topics: ticks={}, oi={}, orderbook={}", ticksTopic, oiTopic, orderbookTopic);
            log.info("Candle topics: 1m={}, 2m={}, 3m={}, 5m={}, 15m={}, 30m={}", candle1mTopic, candle2mTopic, candle3mTopic, candle5mTopic, candle15mTopic, candle30mTopic);
            log.info("Family topics: 1m={}, 2m={}, 5m={}, 15m={}, 30m={}, all={}", familyStructured1mTopic, familyStructured2mTopic, familyStructured5mTopic, familyStructured15mTopic, familyStructured30mTopic, familyStructuredAllTopic);

            // Delegate to orchestrator
            orchestrator.startAllStreams();
            log.info("‚úÖ Unified Market Data Processor started successfully");

        } catch (Exception e) {
            log.error("‚ùå Error starting Unified Market Data Processor", e);
            throw new RuntimeException("Failed to start unified processor", e);
        }
    }

    /**
     * Stop the unified market data processor with graceful shutdown
     */
    @PreDestroy
    public void stop() {
        try {
            log.info("üõë Starting graceful shutdown of Unified Market Data Processor");
            
            // Step 1: Stop accepting new data
            log.info("üì§ Step 1: Stopping new data acceptance");
            
            // Step 2: Wait for in-flight processing to complete
            log.info("‚è≥ Step 2: Waiting for in-flight processing to complete");
            Thread.sleep(2000); // Allow 2 seconds for processing to complete
            
            // Step 3: Flush any pending state
            log.info("üíæ Step 3: Flushing pending state");
            
            // Step 4: Stop all streams gracefully
            log.info("üîÑ Step 4: Stopping all streams");
            orchestrator.stopAllStreams();
            
            // Step 5: Final cleanup
            log.info("üßπ Step 5: Final cleanup");
            
            log.info("‚úÖ Unified Market Data Processor stopped gracefully");
            
        } catch (Exception e) {
            log.error("‚ùå Error during graceful shutdown", e);
            // Force shutdown if graceful fails
            try {
                orchestrator.stopAllStreams();
            } catch (Exception forceException) {
                log.error("‚ùå Force shutdown also failed", forceException);
            }
        }
    }

    /**
     * Get stream states
     */
    public Map<String, String> getStreamStates() {
        return orchestrator.getStreamStatus();
    }

    /**
     * Get stream statistics
     */
    public String getStreamStats() {
        return orchestrator.getStreamStats();
    }

    /**
     * Get metrics
     */
    public String getMetrics() {
        return metrics.getMetrics();
    }

    /**
     * Health check
     */
    public boolean isHealthy() {
        Map<String, String> states = getStreamStates();
        return states.values().stream()
            .allMatch(state -> "RUNNING".equals(state) || "CREATED".equals(state));
    }
}

--------------------------------------------------
File End
--------------------------------------------------


processor/CandleAccumulator.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.model.Candlestick;
import com.kotsin.consumer.model.CandleData;
import lombok.Data;

@Data
public class CandleAccumulator {
    private Long windowStart;
    private Double open;
    private Double high;
    private Double low;
    private Double close;
    private Long volume = 0L;
    private Long windowEnd;
    private int windowMinutes = 1;
    private boolean complete = false;
    private int tickCount = 0;

    // Extra metrics for advanced candle features
    private Double priceVolumeSum = 0.0;  // For VWAP (sum of price * volume)
    private Double previousClose = null;  // For log returns

    // Buy/Sell volume separation
    private Long buyVolume = 0L;      // Volume from buy-side trades
    private Long sellVolume = 0L;     // Volume from sell-side trades
    private Double lastPrice = null;  // For tick rule classification

    public CandleAccumulator() { this.windowStart = null; }

    public CandleAccumulator(Long windowStart, int minutes) {
        this.windowStart = windowStart;
        this.windowMinutes = Math.max(1, minutes);
        this.windowEnd = windowStart + (windowMinutes * 60L * 1000L);
    }

    public void addTick(com.kotsin.consumer.model.TickData tick) {
        if (windowStart == null) {
            windowStart = alignToMinute(tick.getTimestamp());
            windowEnd = windowStart + (windowMinutes * 60L * 1000L);
        }

        tickCount++;

        if (open == null) {
            open = tick.getLastRate();
        }

        close = tick.getLastRate();

        if (high == null || tick.getLastRate() > high) {
            high = tick.getLastRate();
        }

        if (low == null || tick.getLastRate() < low) {
            low = tick.getLastRate();
        }

        Integer dv = tick.getDeltaVolume();
        if (dv != null && dv > 0) {
            volume += dv.longValue();
            priceVolumeSum += tick.getLastRate() * dv;

            boolean isBuy = classifyTrade(tick);
            if (isBuy) {
                buyVolume += dv.longValue();
            } else {
                sellVolume += dv.longValue();
            }
        }

        lastPrice = tick.getLastRate();
    }

    /**
     * Classify trade as buy or sell using quote-rule, then tick-rule, with tick-size aware threshold.
     */
    private boolean classifyTrade(com.kotsin.consumer.model.TickData tick) {
        double currentPrice = tick.getLastRate();

        double bidPrice = tick.getBidRate();
        double askPrice = tick.getOfferRate();

        // Threshold: 1bp of price or 0.01 minimum
        double threshold = Math.max(0.01, Math.abs(currentPrice) * 0.0001);

        if (bidPrice > 0 && askPrice > 0) {
            if (Math.abs(currentPrice - askPrice) <= threshold) {
                return true;
            } else if (Math.abs(currentPrice - bidPrice) <= threshold) {
                return false;
            }
        }

        // Tick rule
        if (lastPrice != null && Math.abs(currentPrice - lastPrice) > threshold) {
            return currentPrice > lastPrice;
        }

        // If we cannot determine (no book and no movement), do not bias: default to false (sell) to avoid buy inflation
        return false;
    }

    public void markComplete() { complete = true; }
    public boolean isComplete() { return complete; }

    private long alignToMinute(long timestamp) { return (timestamp / 60_000) * 60_000; }

    public CandleData toCandleData(String exchange, String exchangeType) {
        double volumeDelta = (double) (buyVolume - sellVolume);
        double volumeDeltaPercent = volume > 0 ? (volumeDelta / volume) * 100.0 : 0.0;

        double vwap = volume > 0 ? priceVolumeSum / volume : 0.0;

        return CandleData.builder()
            .open(open)
            .high(high)
            .low(low)
            .close(close)
            .volume(volume)
            .windowStart(windowStart)
            .windowEnd(windowEnd)
            .isComplete(complete)
            .exchange(exchange)
            .exchangeType(exchangeType)
            .buyVolume(buyVolume)
            .sellVolume(sellVolume)
            .volumeDelta(volumeDelta)
            .volumeDeltaPercent(volumeDeltaPercent)
            .vwap(vwap)
            .tickCount(tickCount)
            .build();
    }

    public Candlestick toFinalizedCandlestick(String scripCode, String companyName,
                                               String exchange, String exchangeType,
                                               boolean includeExtras) {
        Candlestick candle = new Candlestick();
        candle.setScripCode(scripCode);
        candle.setCompanyName(companyName);
        candle.setExchange(exchange);
        candle.setExchangeType(exchangeType);
        candle.setWindowStartMillis(windowStart != null ? windowStart : 0);
        candle.setWindowEndMillis(windowEnd != null ? windowEnd : 0);
        candle.setIsComplete(complete);
        candle.setOpen(open != null ? open : 0.0);
        candle.setHigh(high != null ? high : 0.0);
        candle.setLow(low != null ? low : 0.0);
        candle.setClose(close != null ? close : 0.0);
        candle.setVolume(volume != null ? volume.intValue() : 0);
        if (includeExtras) {
            if (volume > 0) candle.setVwap(priceVolumeSum / volume);
            if (high != null && low != null && close != null) candle.setHlc3((high + low + close) / 3.0);
            if (previousClose != null && close != null && previousClose > 0) candle.setLogReturnFromPrevBar(Math.log(close / previousClose));
            candle.setTicksInWindow(tickCount);
            candle.setWindowLatencyMs(System.currentTimeMillis() - windowEnd);
        }
        return candle;
    }

    public void setPreviousClose(Double prevClose) { this.previousClose = prevClose; }
    public Long getWindowStart() { return windowStart; }
}




--------------------------------------------------
File End
--------------------------------------------------


processor/InstrumentState.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.model.InstrumentCandle;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.TickData;
import com.kotsin.consumer.service.InstrumentStateManager;
import lombok.Data;

import java.util.Set;

/**
 * Per-Instrument State Aggregator (Facade Pattern)
 * Delegates to InstrumentStateManager for actual aggregation logic
 *
 * Design Pattern: Facade
 * Purpose: Simplify interface to per-instrument aggregation
 */
@Data
public class InstrumentState {

    private final InstrumentStateManager stateManager = new InstrumentStateManager();
    private OrderBookSnapshot latestOrderbook;

    // Delegate all operations to state manager
    public void addTick(TickData tick) {
        stateManager.addTick(tick);
    }

    public void addOrderbook(OrderBookSnapshot orderbook) {
        this.latestOrderbook = orderbook;
        // Delegate to manager
        stateManager.addOrderbook(orderbook);
    }

    public boolean hasAnyCompleteWindow() {
        return stateManager.hasAnyCompleteWindow();
    }

    public Set<String> getCompleteWindows() {
        return stateManager.getCompleteWindows();
    }

    public InstrumentCandle extractFinalizedCandle(Timeframe timeframe) {
        return stateManager.extractFinalizedCandle(timeframe);
    }

    // Expose getters for backward compatibility
    public String getScripCode() {
        return stateManager.getScripCode();
    }

    public String getCompanyName() {
        return stateManager.getCompanyName();
    }

    public String getExchange() {
        return stateManager.getExchange();
    }

    public String getExchangeType() {
        return stateManager.getExchangeType();
    }

    public String getInstrumentType() {
        return stateManager.getInstrumentType();
    }

    public void setInstrumentType(String instrumentType) {
        stateManager.setInstrumentType(instrumentType);
    }

    public void setUnderlyingEquityScripCode(String underlyingEquityScripCode) {
        stateManager.setUnderlyingEquityScripCode(underlyingEquityScripCode);
    }

    public Long getLastTickTime() {
        return stateManager.getLastTickTime();
    }

    public Long getMessageCount() {
        return stateManager.getMessageCount();
    }

    /**
     * Force completion of all windows for finalized candle emission
     *
     * @param kafkaWindowEnd The end time of the Kafka tumbling window (1m)
     */
    public void forceCompleteWindows(long kafkaWindowEnd) {
        stateManager.forceCompleteWindows(kafkaWindowEnd);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/ImbalanceBarAccumulator.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.model.ImbalanceBarData;
import com.kotsin.consumer.model.TickData;
import lombok.Data;

/**
 * Accumulator for imbalance bars (VIB, DIB, TRB, VRB)
 * Implements adaptive thresholds using EWMA
 */
@Data
public class ImbalanceBarAccumulator {
    // Current imbalance accumulators
    private Long volumeImbalance = 0L;
    private Long dollarImbalance = 0L;
    private Integer tickRuns = 0;
    private Long volumeRuns = 0L;

    // Direction tracking
    private String currentDirection = "NEUTRAL";
    private Double lastPrice = null;

    // Expected thresholds (EWMA-based)
    private Double expectedVolumeImbalance = 1000.0;  // Initial estimate
    private Double expectedDollarImbalance = 100000.0;  // Initial estimate
    private Double expectedTickRuns = 10.0;  // Initial estimate
    private Double expectedVolumeRuns = 5000.0;  // Initial estimate

    // EWMA alpha for threshold adaptation
    private static final double EWMA_ALPHA = 0.1;

    // Counters for bar completions
    private int vibCount = 0;
    private int dibCount = 0;
    private int trbCount = 0;
    private int vrbCount = 0;

    public void addTick(TickData tick) {
        if (tick.getDeltaVolume() == null || tick.getDeltaVolume() == 0) {
            return;
        }

        // Determine tick direction using tick rule
        String direction = determineDirection(tick);
        int directionSign = "BUY".equals(direction) ? 1 : -1;

        // Volume Imbalance (VIB)
        long signedVolume = tick.getDeltaVolume() * directionSign;
        volumeImbalance += signedVolume;

        // Dollar Imbalance (DIB)
        long dollarVolume = (long)(tick.getDeltaVolume() * tick.getLastRate());
        dollarImbalance += dollarVolume * directionSign;

        // Tick Runs (TRB)
        if (direction.equals(currentDirection)) {
            tickRuns++;
        } else {
            tickRuns = 1;
            currentDirection = direction;
        }

        // Volume Runs (VRB)
        if (direction.equals(currentDirection)) {
            volumeRuns += tick.getDeltaVolume().longValue();
        } else {
            volumeRuns = tick.getDeltaVolume().longValue();
        }

        lastPrice = tick.getLastRate();

        // Check thresholds and update EWMA estimates
        checkAndUpdateThresholds();
    }

    private String determineDirection(TickData tick) {
        if (lastPrice == null) {
            return "NEUTRAL";
        }

        double currentPrice = tick.getLastRate();
        if (currentPrice > lastPrice) {
            return "BUY";
        } else if (currentPrice < lastPrice) {
            return "SELL";
        } else {
            return currentDirection;  // No change, keep current direction
        }
    }

    private void checkAndUpdateThresholds() {
        // Volume Imbalance Bar threshold
        if (Math.abs(volumeImbalance) >= expectedVolumeImbalance) {
            // Update EWMA estimate
            expectedVolumeImbalance = EWMA_ALPHA * Math.abs(volumeImbalance)
                                    + (1 - EWMA_ALPHA) * expectedVolumeImbalance;
            vibCount++;
            volumeImbalance = 0L;  // Reset after bar emission
        }

        // Dollar Imbalance Bar threshold
        if (Math.abs(dollarImbalance) >= expectedDollarImbalance) {
            expectedDollarImbalance = EWMA_ALPHA * Math.abs(dollarImbalance)
                                    + (1 - EWMA_ALPHA) * expectedDollarImbalance;
            dibCount++;
            dollarImbalance = 0L;
        }

        // Tick Runs Bar threshold
        if (Math.abs(tickRuns) >= expectedTickRuns) {
            expectedTickRuns = EWMA_ALPHA * Math.abs(tickRuns)
                             + (1 - EWMA_ALPHA) * expectedTickRuns;
            trbCount++;
            tickRuns = 0;
        }

        // Volume Runs Bar threshold
        if (Math.abs(volumeRuns) >= expectedVolumeRuns) {
            expectedVolumeRuns = EWMA_ALPHA * Math.abs(volumeRuns)
                               + (1 - EWMA_ALPHA) * expectedVolumeRuns;
            vrbCount++;
            volumeRuns = 0L;
        }
    }

    public ImbalanceBarData toImbalanceBarData() {
        return ImbalanceBarData.create(
            volumeImbalance, dollarImbalance, tickRuns, volumeRuns,
            currentDirection, expectedVolumeImbalance, expectedDollarImbalance,
            expectedTickRuns, expectedVolumeRuns
        );
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/OrderbookDepthAccumulator.java
File type: .java
package com.kotsin.consumer.processor;

import com.fasterxml.jackson.annotation.JsonIgnore;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.OrderbookDepthData;
import com.kotsin.consumer.service.IcebergDetectionService;
import com.kotsin.consumer.service.OrderbookDepthCalculator;
import com.kotsin.consumer.service.SpoofingDetectionService;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;

import java.util.List;

/**
 * Orderbook Depth Accumulator (Facade Pattern)
 * Delegates to specialized services for depth analytics
 *
 * Design Pattern: Facade + Composition
 * Purpose: Coordinate multiple services without complex logic
 * 
 * Serialization Note: Service fields are marked @JsonIgnore and lazily initialized
 * to avoid Jackson serialization errors in Kafka Streams state stores.
 */
@Data
@Slf4j
public class OrderbookDepthAccumulator {

    // Composed services (marked transient for Kafka Streams serialization)
    // CRITICAL: All services must be @JsonIgnore to avoid serialization issues
    // Services are stateless or maintain serializable state separately
    @JsonIgnore
    private transient IcebergDetectionService icebergDetectionService;
    
    @JsonIgnore
    private transient SpoofingDetectionService spoofingDetectionService;
    
    @JsonIgnore
    private transient OrderbookDepthCalculator depthCalculator;

    // Current orderbook state
    private OrderBookSnapshot currentOrderbook;
    private OrderBookSnapshot previousOrderbook;
    private Long lastUpdateTimestamp;

    /**
     * Lazy initialization for services (called after deserialization)
     */
    private IcebergDetectionService getIcebergService() {
        if (icebergDetectionService == null) {
            icebergDetectionService = new IcebergDetectionService();
        }
        return icebergDetectionService;
    }

    private SpoofingDetectionService getSpoofingService() {
        if (spoofingDetectionService == null) {
            spoofingDetectionService = new SpoofingDetectionService();
        }
        return spoofingDetectionService;
    }

    private OrderbookDepthCalculator getDepthCalculator() {
        if (depthCalculator == null) {
            depthCalculator = new OrderbookDepthCalculator();
        }
        return depthCalculator;
    }

    public void addOrderbook(OrderBookSnapshot orderbook) {
        if (orderbook == null || !orderbook.isValid()) {
            return;
        }

        orderbook.parseDetails();
        previousOrderbook = currentOrderbook;
        currentOrderbook = orderbook;
        lastUpdateTimestamp = orderbook.getTimestamp();

        // Delegate to specialized services (using lazy getters)
        if (previousOrderbook != null) {
            getSpoofingService().detectSpoofing(previousOrderbook, currentOrderbook);
        }

        trackIcebergPatterns(orderbook);
    }

    private void trackIcebergPatterns(OrderBookSnapshot orderbook) {
        // Delegate to iceberg detection service (using lazy getter)
        if (orderbook.getAllBids() != null && !orderbook.getAllBids().isEmpty()) {
            getIcebergService().trackBidQuantity(orderbook.getAllBids().get(0).getQuantity());
        }

        if (orderbook.getAllAsks() != null && !orderbook.getAllAsks().isEmpty()) {
            getIcebergService().trackAskQuantity(orderbook.getAllAsks().get(0).getQuantity());
        }
    }

    public OrderbookDepthData toOrderbookDepthData() {
        if (currentOrderbook == null || !currentOrderbook.isValid()) {
            return OrderbookDepthData.builder()
                .isComplete(false)
                .build();
        }

        try {
            double midPrice = currentOrderbook.getMidPrice();

            // Delegate profile building to calculator (using lazy getter)
            List<OrderbookDepthData.DepthLevel> bidProfile =
                getDepthCalculator().buildDepthProfile(currentOrderbook.getAllBids(), "BID", midPrice);
            List<OrderbookDepthData.DepthLevel> askProfile =
                getDepthCalculator().buildDepthProfile(currentOrderbook.getAllAsks(), "ASK", midPrice);

            // Delegate calculations to calculator (using lazy getter)
            List<Double> cumulativeBidDepth = getDepthCalculator().calculateCumulativeDepth(bidProfile);
            List<Double> cumulativeAskDepth = getDepthCalculator().calculateCumulativeDepth(askProfile);

            Double totalBidDepth = cumulativeBidDepth.isEmpty() ? null :
                cumulativeBidDepth.get(cumulativeBidDepth.size() - 1);
            Double totalAskDepth = cumulativeAskDepth.isEmpty() ? null :
                cumulativeAskDepth.get(cumulativeAskDepth.size() - 1);

            Double bidVWAP = bidProfile.isEmpty() ? null : getDepthCalculator().calculateSideVWAP(bidProfile);
            Double askVWAP = askProfile.isEmpty() ? null : getDepthCalculator().calculateSideVWAP(askProfile);
            Double depthPressure = (midPrice > 0 && bidVWAP != null && askVWAP != null) ? 
                (bidVWAP - askVWAP) / midPrice : null;

            Double weightedImbalance = (bidProfile.isEmpty() || askProfile.isEmpty()) ? null :
                getDepthCalculator().calculateWeightedDepthImbalance(bidProfile, askProfile);

            Double bidSlope = bidProfile.isEmpty() ? null : getDepthCalculator().calculateSlope(bidProfile);
            Double askSlope = askProfile.isEmpty() ? null : getDepthCalculator().calculateSlope(askProfile);
            Double slopeRatio = (askSlope != null && askSlope != 0 && bidSlope != null) ? bidSlope / askSlope : null;

            // Get results from detection services (using lazy getters)
            Boolean icebergBid = getIcebergService().detectIcebergBid();
            Boolean icebergAsk = getIcebergService().detectIcebergAsk();
            Double icebergProbBid = getIcebergService().calculateIcebergProbabilityBid();
            Double icebergProbAsk = getIcebergService().calculateIcebergProbabilityAsk();

            Double level1Imb = (bidProfile.isEmpty() || askProfile.isEmpty()) ? null :
                getDepthCalculator().calculateLevelImbalance(bidProfile, askProfile, 1, 1);
            Double level2to5Imb = (bidProfile.isEmpty() || askProfile.isEmpty()) ? null :
                getDepthCalculator().calculateLevelImbalance(bidProfile, askProfile, 2, 5);
            Double level6to10Imb = (bidProfile.isEmpty() || askProfile.isEmpty()) ? null :
                getDepthCalculator().calculateLevelImbalance(bidProfile, askProfile, 6, 10);

            Integer spoofCount = getSpoofingService().getSpoofingCount();
            Boolean activeSpoofBid = getSpoofingService().isActiveSpoofingBid() ? true : null;
            Boolean activeSpoofAsk = getSpoofingService().isActiveSpoofingAsk() ? true : null;

            return OrderbookDepthData.builder()
                .bidProfile(bidProfile)
                .askProfile(askProfile)
                .cumulativeBidDepth(cumulativeBidDepth)
                .cumulativeAskDepth(cumulativeAskDepth)
                .totalBidDepth(totalBidDepth)
                .totalAskDepth(totalAskDepth)
                .bidVWAP(bidVWAP)
                .askVWAP(askVWAP)
                .depthPressure(depthPressure)
                .weightedDepthImbalance(weightedImbalance)
                .level1Imbalance(level1Imb)
                .level2to5Imbalance(level2to5Imb)
                .level6to10Imbalance(level6to10Imb)
                .bidSlope(bidSlope)
                .askSlope(askSlope)
                .slopeRatio(slopeRatio)
                .icebergDetectedBid(icebergBid)
                .icebergDetectedAsk(icebergAsk)
                .icebergProbabilityBid(icebergProbBid)
                .icebergProbabilityAsk(icebergProbAsk)
                .spoofingEvents(getSpoofingService().getSpoofingEvents())
                .spoofingCountLast1Min(spoofCount)
                .activeSpoofingBid(activeSpoofBid)
                .activeSpoofingAsk(activeSpoofAsk)
                .timestamp(currentOrderbook.getTimestamp())
                .midPrice(midPrice)
                .spread(currentOrderbook.getSpread())
                .depthLevels(Math.min(bidProfile.size(), askProfile.size()))
                .isComplete(true)
                .build();

        } catch (Exception e) {
            log.error("Failed to build orderbook depth data", e);
            return OrderbookDepthData.builder().isComplete(false).build();
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/OiAccumulator.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.model.OpenInterestTimeframeData;
import com.kotsin.consumer.model.TickData;
import lombok.Data;

@Data
public class OiAccumulator {
    private Long windowStart;
    private Long windowEnd;
    private Long oiStart;
    private Long oiEnd;
    private Long oiChange;
    private Double oiChangePercent;
    private Double oiMomentum;
    private Double oiConcentration;
    private boolean complete = false;
    private int windowMinutes = 1;

    // Put/Call OI tracking (NEW)
    private Long putOi = 0L;
    private Long callOi = 0L;
    private Long putOiStart = null;
    private Long callOiStart = null;

    // Volume tracking for OI-Volume correlation (NEW)
    private Long totalVolume = 0L;

    public OiAccumulator() { this.windowStart = null; }

    public OiAccumulator(Long windowStart, int minutes) {
        this.windowStart = windowStart;
        this.windowMinutes = Math.max(1, minutes);
        this.windowEnd = windowStart + (windowMinutes * 60L * 1000L);
    }

    public void addOiData(TickData tick) {
        if (windowStart == null) {
            windowStart = alignToMinute(tick.getTimestamp());
            windowEnd = windowStart + (windowMinutes * 60L * 1000L);
        }
        if (oiStart == null) {
            oiStart = tick.getOpenInterest();
        }
        oiEnd = tick.getOpenInterest();

        // Track volume for OI-Volume correlation
        if (tick.getDeltaVolume() != null) {
            totalVolume += tick.getDeltaVolume();
        }
    }

    /**
     * Add Put OI data (called for put options)
     */
    public void addPutOi(Long oi) {
        if (oi == null) return;
        if (putOiStart == null) {
            putOiStart = oi;
        }
        putOi = oi;
    }

    /**
     * Add Call OI data (called for call options)
     */
    public void addCallOi(Long oi) {
        if (oi == null) return;
        if (callOiStart == null) {
            callOiStart = oi;
        }
        callOi = oi;
    }

    public void markComplete() {
        complete = true;
        calculateMetrics();
    }

    private void calculateMetrics() {
        if (oiStart != null && oiEnd != null) {
            oiChange = oiEnd - oiStart;
            oiChangePercent = oiStart != 0 ? (double) oiChange / oiStart * 100.0 : 0.0;
            if (windowStart != null && windowEnd != null) {
                long windowMinutes = (windowEnd - windowStart) / (60 * 1000);
                oiMomentum = windowMinutes > 0 ? (double) oiChange / windowMinutes : 0.0;
            }
        }
    }

    private long alignToMinute(long timestamp) { return (timestamp / 60_000) * 60_000; }

    public OpenInterestTimeframeData toOiTimeframeData() {
        // Calculate Put/Call ratio
        Double putCallRatio = callOi > 0 ? (double) putOi / callOi : null;

        // Calculate Put/Call changes
        Long putOiChange = (putOiStart != null && putOi != null) ? putOi - putOiStart : null;
        Long callOiChange = (callOiStart != null && callOi != null) ? callOi - callOiStart : null;

        // Calculate Put/Call ratio change
        Double putCallRatioChange = null;
        if (putOiStart != null && callOiStart != null && callOiStart > 0 && callOi > 0) {
            double initialRatio = (double) putOiStart / callOiStart;
            double finalRatio = (double) putOi / callOi;
            putCallRatioChange = finalRatio - initialRatio;
        }

        // Calculate OI-Volume correlation (simplified)
        Double oiVolumeCorr = null;
        if (oiChange != null && totalVolume > 0) {
            // Simple correlation indicator: both positive or both negative = high correlation
            boolean oiPositive = oiChange > 0;
            boolean volumeHigh = totalVolume > 50000;  // Threshold adjustable
            oiVolumeCorr = (oiPositive == volumeHigh) ? 0.8 : -0.2;
        }

        return OpenInterestTimeframeData.builder()
            .oi(oiEnd)
            .oiChange(oiChange)
            .oiChangePercent(oiChangePercent)
            .oiMomentum(oiMomentum)
            .oiConcentration(oiConcentration)
            .isComplete(complete)
            .windowStart(windowStart)
            .windowEnd(windowEnd)
            .putOi(putOi > 0 ? putOi : null)
            .callOi(callOi > 0 ? callOi : null)
            .putCallRatio(putCallRatio)
            .putOiChange(putOiChange)
            .callOiChange(callOiChange)
            .putCallRatioChange(putCallRatioChange)
            .volumeInWindow(totalVolume > 0 ? totalVolume : null)
            .oiVolumeCorrelation(oiVolumeCorr)
            .build();
    }

    public Long getWindowStart() { return windowStart; }
}




--------------------------------------------------
File End
--------------------------------------------------


processor/WindowRotationService.java
File type: .java
package com.kotsin.consumer.processor;

/**
 * Centralizes window rotation logic for accumulators.
 * Keeps behavior identical while improving readability and testability.
 */
public final class WindowRotationService {

    private WindowRotationService() {}

    public static CandleAccumulator rotateCandleIfNeeded(CandleAccumulator acc, long tickTime, int minutes) {
        long windowStart = align(tickTime, minutes);
        if (acc.getWindowStart() == null) {
            return new CandleAccumulator(windowStart, minutes);
        }
        if (!acc.getWindowStart().equals(windowStart)) {
            acc.markComplete();
            return new CandleAccumulator(windowStart, minutes);
        }
        return acc;
    }

    public static CandleAccumulator rotateCandleIfNeeded(CandleAccumulator acc, long tickTime, int minutes, int offsetMinutes) {
        long windowStart = alignWithOffset(tickTime, minutes, offsetMinutes);
        if (acc.getWindowStart() == null) {
            return new CandleAccumulator(windowStart, minutes);
        }
        if (!acc.getWindowStart().equals(windowStart)) {
            acc.markComplete();
            return new CandleAccumulator(windowStart, minutes);
        }
        return acc;
    }

    public static OiAccumulator rotateOiIfNeeded(OiAccumulator acc, long tickTime, int minutes) {
        long windowStart = align(tickTime, minutes);
        if (acc.getWindowStart() == null) {
            return new OiAccumulator(windowStart, minutes);
        }
        if (!acc.getWindowStart().equals(windowStart)) {
            acc.markComplete();
            return new OiAccumulator(windowStart, minutes);
        }
        return acc;
    }

    public static OiAccumulator rotateOiIfNeeded(OiAccumulator acc, long tickTime, int minutes, int offsetMinutes) {
        long windowStart = alignWithOffset(tickTime, minutes, offsetMinutes);
        if (acc.getWindowStart() == null) {
            return new OiAccumulator(windowStart, minutes);
        }
        if (!acc.getWindowStart().equals(windowStart)) {
            acc.markComplete();
            return new OiAccumulator(windowStart, minutes);
        }
        return acc;
    }

    private static long align(long ts, int minutes) {
        long sizeMs = minutes * 60_000L;
        return (ts / sizeMs) * sizeMs;
    }

    private static long alignWithOffset(long ts, int minutes, int offsetMinutes) {
        long sizeMs = minutes * 60_000L;
        long offsetMs = offsetMinutes * 60_000L;
        return ((ts - offsetMs) / sizeMs) * sizeMs + offsetMs;
    }
}




--------------------------------------------------
File End
--------------------------------------------------


processor/Timeframe.java
File type: .java
package com.kotsin.consumer.processor;

/**
 * Strongly-typed timeframes used across aggregation.
 */
public enum Timeframe {
    ONE_MIN("1m", 1),
    TWO_MIN("2m", 2),
    THREE_MIN("3m", 3),
    FIVE_MIN("5m", 5),
    FIFTEEN_MIN("15m", 15),
    THIRTY_MIN("30m", 30);

    private final String label;
    private final int minutes;

    Timeframe(String label, int minutes) {
        this.label = label;
        this.minutes = minutes;
    }

    public String getLabel() {
        return label;
    }

    public int getMinutes() {
        return minutes;
    }

    public static Timeframe fromLabel(String label) {
        for (Timeframe tf : values()) {
            if (tf.label.equals(label)) return tf;
        }
        throw new IllegalArgumentException("Unknown timeframe label: " + label);
    }
}




--------------------------------------------------
File End
--------------------------------------------------


processor/MultiTimeframeState.java
File type: .java
package com.kotsin.consumer.processor;

import com.kotsin.consumer.model.*;
import com.kotsin.consumer.service.TimeframeStateManager;
import lombok.Data;

import java.util.*;

/**
 * Multi-timeframe state aggregator (Facade Pattern)
 * Delegates to TimeframeStateManager for actual aggregation logic
 *
 * Design Pattern: Facade
 * Purpose: Simplify interface to complex subsystem
 */
@Data
public class MultiTimeframeState {

    private final TimeframeStateManager stateManager = new TimeframeStateManager();

    // Delegate all operations to state manager
    public void addTick(TickData tick) {
        stateManager.addTick(tick);
    }

    public boolean hasAnyCompleteWindow() {
        return stateManager.hasAnyCompleteWindow();
    }

    public Set<String> getCompleteWindows() {
        return stateManager.getCompleteWindows();
    }

    public Map<String, CandleData> getMultiTimeframeCandles() {
        return stateManager.getMultiTimeframeCandles();
    }

    public Map<String, OpenInterestTimeframeData> getOpenInterest() {
        return stateManager.getOpenInterest();
    }

    public ImbalanceBarData getImbalanceBars() {
        return stateManager.getImbalanceBars();
    }

    public MicrostructureData getMicrostructure() {
        return stateManager.getMicrostructure();
    }

    public OrderbookDepthData getOrderbookDepth() {
        return stateManager.getOrderbookDepth();
    }

    public String getDataQuality() {
        return stateManager.getDataQuality();
    }

    public long getProcessingLatency() {
        return stateManager.getProcessingLatency();
    }

    // Expose getters for backward compatibility
    public String getScripCode() {
        return stateManager.getScripCode();
    }

    public String getCompanyName() {
        return stateManager.getCompanyName();
    }

    public String getExchange() {
        return stateManager.getExchange();
    }

    public String getExchangeType() {
        return stateManager.getExchangeType();
    }

    public Long getLastTickTime() {
        return stateManager.getLastTickTime();
    }

    public Long getMessageCount() {
        return stateManager.getMessageCount();
    }

    public EnumMap<Timeframe, CandleAccumulator> getCandleAccumulators() {
        return stateManager.getCandleAccumulators();
    }

    /**
     * Force completion of all windows for finalized candle emission
     * Uses the Kafka window end time as the reference point
     * 
     * @param kafkaWindowEnd The end time of the Kafka tumbling window (1m)
     */
    public void forceCompleteWindows(long kafkaWindowEnd) {
        stateManager.forceCompleteWindows(kafkaWindowEnd);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/FamilyAggregationService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Service for family-level aggregation logic
 * Handles assembly and computation of family-level metrics
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class FamilyAggregationService {

    private final InstrumentKeyResolver keyResolver;

    /**
     * Assemble family from individual instrument candles
     */
    public FamilyEnrichedData assembleFamily(String familyKey, InstrumentCandle candle, FamilyEnrichedData family) {
        if (candle == null) {
            return family; // No candle to add, return existing family
        }
        if (family == null) {
            family = FamilyEnrichedData.builder().build();
        }
        // Ensure identity fields are populated
        if (family.getFamilyKey() == null) {
            family.setFamilyKey(familyKey);
        }
        if (family.getInstrumentType() == null) {
            String famType = (familyKey != null && keyResolver.isIndex(familyKey)) ? "INDEX_FAMILY" : "EQUITY_FAMILY";
            family.setInstrumentType(famType);
        }
        if (family.getFamilyName() == null) {
            String name = null;
            if (family.getEquity() != null && family.getEquity().getCompanyName() != null) {
                name = family.getEquity().getCompanyName();
            } else {
                name = candle.getCompanyName();
            }
            family.setFamilyName(name);
        }

        if (family.getWindowStartMillis() == null) {
            family.setWindowStartMillis(candle.getWindowStartMillis());
        }
        family.setWindowEndMillis(candle.getWindowEndMillis());

        String type = candle.getInstrumentType() != null ? candle.getInstrumentType().toUpperCase() : "";
        if ("EQUITY".equals(type) || ("INDEX".equals(type) && family.getEquity() == null)) {
            family.setEquity(candle);
            if (family.getFamilyName() == null) {
                family.setFamilyName(candle.getCompanyName());
            }
        } else if ("FUTURE".equals(type)) {
            // CRITICAL: Deduplicate futures by scripCode to prevent double-counting
            addOrUpdateFuture(family, candle);
        } else if ("OPTION".equals(type)) {
            // CRITICAL: Deduplicate options by unique key (strike + type)
            addOrUpdateOption(family, candle);
        }

        // Recompute aggregated family-level analytics
        family.setAggregatedMetrics(computeAggregatedMetrics(family));
        family.setMicrostructure(computeFamilyMicrostructure(family));
        family.setOrderbookDepth(computeFamilyOrderbookDepth(family));
        family.setImbalanceBars(computeFamilyImbalanceBars(family));
        // Adjust identity and spot fallback for derivative-only families
        if (family.getEquity() == null) {
            if (family.getFutures() != null && !family.getFutures().isEmpty()) {
                family.setInstrumentType("DERIVATIVE_FAMILY");
                // Fallback spot price: use near-month future close
                if (family.getAggregatedMetrics() != null && family.getAggregatedMetrics().getSpotPrice() == null) {
                    family.getAggregatedMetrics().setSpotPrice(
                        family.getAggregatedMetrics().getNearMonthFuturePrice()
                    );
                }
            }
        }
        family.setTotalInstrumentsCount(family.calculateTotalCount());
        return family;
    }

    /**
     * Compute aggregated metrics for family
     */
    public FamilyAggregatedMetrics computeAggregatedMetrics(FamilyEnrichedData family) {
        long totalVol = 0;
        long eqVol = 0, futVol = 0, optVol = 0;
        Long totalOi = 0L, futOi = 0L, callsOi = 0L, putsOi = 0L;
        Long futOiChg = 0L, callsOiChg = 0L, putsOiChg = 0L;
        Double spot = null, fut = null;
        Integer activeOptions = 0;
        Integer activeFutures = 0;
        String nearExpiry = null;

        if (family.getEquity() != null) {
            InstrumentCandle e = family.getEquity();
            if (e.getVolume() != null) { eqVol = e.getVolume(); totalVol += eqVol; }
            spot = e.getClose();
            if (e.getOpenInterest() != null) { totalOi += e.getOpenInterest(); }
        }
        if (family.getFutures() != null && !family.getFutures().isEmpty()) {
            InstrumentCandle f = family.getFutures().get(0);
            activeFutures = 1;
            if (f.getVolume() != null) { futVol = f.getVolume(); totalVol += futVol; }
            fut = f.getClose();
            if (f.getOpenInterest() != null) { totalOi += f.getOpenInterest(); futOi += f.getOpenInterest(); }
            if (f.getOiChange() != null) { futOiChg += f.getOiChange(); }
            nearExpiry = f.getExpiry();
        }
        long callsVol = 0, putsVol = 0;
        if (family.getOptions() != null) {
            for (InstrumentCandle o : family.getOptions()) {
                if (o.getVolume() != null) {
                    optVol += o.getVolume();
                    totalVol += o.getVolume();
                    if ("CE".equalsIgnoreCase(o.getOptionType())) callsVol += o.getVolume();
                    if ("PE".equalsIgnoreCase(o.getOptionType())) putsVol += o.getVolume();
                }
                if (o.getOpenInterest() != null) {
                    totalOi += o.getOpenInterest();
                    if ("CE".equalsIgnoreCase(o.getOptionType())) callsOi += o.getOpenInterest();
                    if ("PE".equalsIgnoreCase(o.getOptionType())) putsOi += o.getOpenInterest();
                }
                if (o.getOiChange() != null) {
                    if ("CE".equalsIgnoreCase(o.getOptionType())) callsOiChg += o.getOiChange();
                    if ("PE".equalsIgnoreCase(o.getOptionType())) putsOiChg += o.getOiChange();
                }
                if (o.getVolume() != null && o.getVolume() > 0) activeOptions++;
            }
        }
        Double basis = (spot != null && fut != null) ? (fut - spot) : null;
        Double basisPct = (spot != null && fut != null && spot != 0) ? ((fut - spot) / spot * 100.0) : null;
        Double pcr = (callsOi != null && callsOi > 0) ? (putsOi.doubleValue() / callsOi.doubleValue()) : null;
        Double pcrVol = (callsVol > 0) ? (putsVol * 1.0 / callsVol) : null;

        // Orderbook aggregates across instruments (simple sums/averages)
        Double avgSpread = null;
        Long sumBid = 0L, sumAsk = 0L; int depthCount = 0;
        for (InstrumentCandle c : collectAllInstruments(family)) {
            if (c.getOrderbookDepth() != null) {
                if (c.getOrderbookDepth().getSpread() != null) {
                    avgSpread = (avgSpread == null ? 0.0 : avgSpread) + c.getOrderbookDepth().getSpread();
                }
                if (c.getOrderbookDepth().getTotalBidDepth() != null) sumBid += c.getOrderbookDepth().getTotalBidDepth().longValue();
                if (c.getOrderbookDepth().getTotalAskDepth() != null) sumAsk += c.getOrderbookDepth().getTotalAskDepth().longValue();
                depthCount++;
            }
        }
        if (avgSpread != null && depthCount > 0) avgSpread = avgSpread / depthCount;
        Double bidAskImb = (sumBid + sumAsk) > 0 ? ((sumBid - sumAsk) * 1.0 / (sumBid + sumAsk)) : null;

        return FamilyAggregatedMetrics.builder()
            .totalVolume(totalVol)
            .equityVolume(eqVol)
            .futuresVolume(futVol)
            .optionsVolume(optVol)
            .totalOpenInterest(totalOi > 0 ? totalOi : null)
            .futuresOI(futOi > 0 ? futOi : null)
            .callsOI(callsOi > 0 ? callsOi : null)
            .putsOI(putsOi > 0 ? putsOi : null)
            .futuresOIChange(futOiChg != 0 ? futOiChg : null)
            .callsOIChange(callsOiChg != 0 ? callsOiChg : null)
            .putsOIChange(putsOiChg != 0 ? putsOiChg : null)
            .putCallRatio(pcr)
            .putCallVolumeRatio(pcrVol)
            .activeOptionsCount(activeOptions)
            .spotPrice(spot)
            .nearMonthFuturePrice(fut)
            .futuresBasis(basis)
            .futuresBasisPercent(basisPct)
            .activeFuturesCount(activeFutures)
            .nearMonthExpiry(nearExpiry)
            .avgBidAskSpread(avgSpread)
            .totalBidVolume(sumBid > 0 ? sumBid : null)
            .totalAskVolume(sumAsk > 0 ? sumAsk : null)
            .bidAskImbalance(bidAskImb)
            .calculatedAt(System.currentTimeMillis())
            .build();
    }

    /**
     * Compute family-level microstructure data
     */
    public MicrostructureData computeFamilyMicrostructure(FamilyEnrichedData family) {
        double ofiSum = 0.0, vpinSum = 0.0, depthImbSum = 0.0, kyleSum = 0.0; int n = 0;
        for (InstrumentCandle c : collectAllInstruments(family)) {
            if (c.getMicrostructure() != null) {
                MicrostructureData m = c.getMicrostructure();
                if (m.getOfi() != null) ofiSum += m.getOfi();
                if (m.getVpin() != null) vpinSum += m.getVpin();
                if (m.getDepthImbalance() != null) depthImbSum += m.getDepthImbalance();
                if (m.getKyleLambda() != null) kyleSum += m.getKyleLambda();
                n++;
            }
        }
        if (n == 0) return null;
        return MicrostructureData.builder()
            .ofi(ofiSum / n)
            .vpin(vpinSum / n)
            .depthImbalance(depthImbSum / n)
            .kyleLambda(kyleSum / n)
            .isComplete(true)
            .build();
    }

    /**
     * Compute family-level orderbook depth data
     */
    public OrderbookDepthData computeFamilyOrderbookDepth(FamilyEnrichedData family) {
        double spreadSum = 0.0; int spreadCount = 0;
        double totalBid = 0.0, totalAsk = 0.0;
        double weightedImbSum = 0.0; int imbCount = 0;
        double lvl1ImbSum = 0.0, l2to5ImbSum = 0.0, l6to10ImbSum = 0.0; 
        int lvl1Count = 0, lvl2to5Count = 0, lvl6to10Count = 0;
        double bidVwapSum = 0.0, askVwapSum = 0.0; int vwapCount = 0;
        double bidSlopeSum = 0.0, askSlopeSum = 0.0, slopeRatioSum = 0.0; int slopeCount = 0;
        long tsMax = 0L; Integer depthLevelsMin = null;
        
        // Spoofing/Iceberg aggregation
        boolean anyIcebergBid = false, anyIcebergAsk = false;
        double maxIcebergProbBid = 0.0, maxIcebergProbAsk = 0.0;
        int totalSpoofCount = 0;
        boolean anyActiveSpoofBid = false, anyActiveSpoofAsk = false;
        List<OrderbookDepthData.SpoofingEvent> allSpoofEvents = new ArrayList<>();

        for (InstrumentCandle c : collectAllInstruments(family)) {
            OrderbookDepthData d = c.getOrderbookDepth();
            if (d == null) continue;

            if (d.getSpread() != null) { spreadSum += d.getSpread(); spreadCount++; }
            if (d.getTotalBidDepth() != null) totalBid += d.getTotalBidDepth();
            if (d.getTotalAskDepth() != null) totalAsk += d.getTotalAskDepth();

            if (d.getWeightedDepthImbalance() != null) { weightedImbSum += d.getWeightedDepthImbalance(); imbCount++; }
            
            if (d.getLevel1Imbalance() != null) { lvl1ImbSum += d.getLevel1Imbalance(); lvl1Count++; }
            if (d.getLevel2to5Imbalance() != null) { l2to5ImbSum += d.getLevel2to5Imbalance(); lvl2to5Count++; }
            if (d.getLevel6to10Imbalance() != null) { l6to10ImbSum += d.getLevel6to10Imbalance(); lvl6to10Count++; }

            if (d.getBidVWAP() != null && d.getAskVWAP() != null) {
                bidVwapSum += d.getBidVWAP();
                askVwapSum += d.getAskVWAP();
                vwapCount++;
            }

            if (d.getBidSlope() != null && d.getAskSlope() != null) {
                bidSlopeSum += d.getBidSlope();
                askSlopeSum += d.getAskSlope();
                if (d.getSlopeRatio() != null) slopeRatioSum += d.getSlopeRatio();
                slopeCount++;
            }

            if (d.getTimestamp() != null && d.getTimestamp() > tsMax) tsMax = d.getTimestamp();
            if (d.getDepthLevels() != null) depthLevelsMin = depthLevelsMin == null ? d.getDepthLevels() : Math.min(depthLevelsMin, d.getDepthLevels());
            
            if (d.getIcebergDetectedBid() != null && d.getIcebergDetectedBid()) anyIcebergBid = true;
            if (d.getIcebergDetectedAsk() != null && d.getIcebergDetectedAsk()) anyIcebergAsk = true;
            if (d.getIcebergProbabilityBid() != null) maxIcebergProbBid = Math.max(maxIcebergProbBid, d.getIcebergProbabilityBid());
            if (d.getIcebergProbabilityAsk() != null) maxIcebergProbAsk = Math.max(maxIcebergProbAsk, d.getIcebergProbabilityAsk());
            if (d.getSpoofingCountLast1Min() != null) totalSpoofCount += d.getSpoofingCountLast1Min();
            if (d.getActiveSpoofingBid() != null && d.getActiveSpoofingBid()) anyActiveSpoofBid = true;
            if (d.getActiveSpoofingAsk() != null && d.getActiveSpoofingAsk()) anyActiveSpoofAsk = true;
            if (d.getSpoofingEvents() != null && !d.getSpoofingEvents().isEmpty()) {
                allSpoofEvents.addAll(d.getSpoofingEvents());
            }
        }

        if (spreadCount == 0 && totalBid == 0.0 && totalAsk == 0.0 && imbCount == 0 && vwapCount == 0 && slopeCount == 0) return null;
        Double familySpread = spreadCount > 0 ? (spreadSum / spreadCount) : null;
        Double weightedImb = imbCount > 0 ? (weightedImbSum / imbCount) : null;
        Double lvl1Imb = lvl1Count > 0 ? (lvl1ImbSum / lvl1Count) : null;
        Double l2to5Imb = lvl2to5Count > 0 ? (l2to5ImbSum / lvl2to5Count) : null;
        Double l6to10Imb = lvl6to10Count > 0 ? (l6to10ImbSum / lvl6to10Count) : null;
        Double bidVwap = vwapCount > 0 ? (bidVwapSum / vwapCount) : null;
        Double askVwap = vwapCount > 0 ? (askVwapSum / vwapCount) : null;
        Double bidSlope = slopeCount > 0 ? (bidSlopeSum / slopeCount) : null;
        Double askSlope = slopeCount > 0 ? (askSlopeSum / slopeCount) : null;
        Double slopeRatio = slopeCount > 0 ? (slopeRatioSum / slopeCount) : null;

        return OrderbookDepthData.builder()
            .spread(familySpread)
            .totalBidDepth(totalBid > 0 ? totalBid : null)
            .totalAskDepth(totalAsk > 0 ? totalAsk : null)
            .weightedDepthImbalance(weightedImb)
            .level1Imbalance(lvl1Imb)
            .level2to5Imbalance(l2to5Imb)
            .level6to10Imbalance(l6to10Imb)
            .bidVWAP(bidVwap)
            .askVWAP(askVwap)
            .bidSlope(bidSlope)
            .askSlope(askSlope)
            .slopeRatio(slopeRatio)
            .icebergDetectedBid(anyIcebergBid ? true : null)
            .icebergDetectedAsk(anyIcebergAsk ? true : null)
            .icebergProbabilityBid(maxIcebergProbBid > 0 ? maxIcebergProbBid : null)
            .icebergProbabilityAsk(maxIcebergProbAsk > 0 ? maxIcebergProbAsk : null)
            .spoofingCountLast1Min(totalSpoofCount > 0 ? totalSpoofCount : null)
            .activeSpoofingBid(anyActiveSpoofBid ? true : null)
            .activeSpoofingAsk(anyActiveSpoofAsk ? true : null)
            .spoofingEvents(allSpoofEvents.isEmpty() ? null : allSpoofEvents)
            .timestamp(tsMax > 0 ? tsMax : null)
            .depthLevels(depthLevelsMin)
            .isComplete(true)
            .build();
    }

    /**
     * Compute family-level imbalance bar data
     */
    public ImbalanceBarData computeFamilyImbalanceBars(FamilyEnrichedData family) {
        ImbalanceBarData best = null;
        List<InstrumentCandle> ordered = new ArrayList<>();
        if (family.getFutures() != null) ordered.addAll(family.getFutures());
        if (family.getEquity() != null) ordered.add(family.getEquity());
        if (family.getOptions() != null) ordered.addAll(family.getOptions());
        for (InstrumentCandle c : ordered) {
            if (c.getImbalanceBars() != null) {
                if (best == null) {
                    best = c.getImbalanceBars();
                } else if (!best.hasAnyCompleteBar() && c.getImbalanceBars().hasAnyCompleteBar()) {
                    best = c.getImbalanceBars();
                }
            }
        }
        return best;
    }

    /**
     * Add or update future in family (with deduplication by scripCode)
     * CRITICAL FIX: Prevents duplicate futures from different timeframes
     */
    private void addOrUpdateFuture(FamilyEnrichedData family, InstrumentCandle candle) {
        if (family.getFutures() == null) {
            family.setFutures(new ArrayList<>());
        }
        
        String candleScripCode = candle.getScripCode();
        if (candleScripCode == null) {
            return;
        }
        
        // OPTIMIZATION: Use HashMap for O(1) lookup instead of O(n) linear search
        Map<String, Integer> futureIndexMap = getOrCreateFutureIndexMap(family);
        Integer existingIdx = futureIndexMap.get(candleScripCode);
        
        if (existingIdx != null) {
            // Future already exists - update if new one has more volume (more recent/complete data)
            InstrumentCandle existing = family.getFutures().get(existingIdx);
            if (candle.getVolume() != null && 
                (existing.getVolume() == null || candle.getVolume() > existing.getVolume())) {
                family.getFutures().set(existingIdx, candle);
            }
        } else {
            // New future - add it, but keep only near-month (earliest expiry)
            if (family.getFutures().isEmpty()) {
                family.getFutures().add(candle);
                futureIndexMap.put(candleScripCode, 0);
            } else {
                // Replace if this is nearer month, otherwise skip
                InstrumentCandle nearMonth = family.getFutures().get(0);
                if (nearMonth.getExpiry() == null || 
                    (candle.getExpiry() != null && candle.getExpiry().compareTo(nearMonth.getExpiry()) < 0)) {
                    family.getFutures().set(0, candle);
                    futureIndexMap.put(candleScripCode, 0);
                }
            }
        }
    }
    
    /**
     * Get or create future index map for O(1) lookups
     * OPTIMIZATION: Avoids O(n) linear search in family aggregation
     */
    private Map<String, Integer> getOrCreateFutureIndexMap(FamilyEnrichedData family) {
        Map<String, Integer> indexMap = new java.util.HashMap<>();
        if (family.getFutures() != null) {
            for (int i = 0; i < family.getFutures().size(); i++) {
                InstrumentCandle future = family.getFutures().get(i);
                if (future.getScripCode() != null) {
                    indexMap.put(future.getScripCode(), i);
                }
            }
        }
        return indexMap;
    }
    
    /**
     * Add or update option in family (with deduplication by strike+type)
     * CRITICAL FIX: Prevents duplicate options from different timeframes
     */
    private void addOrUpdateOption(FamilyEnrichedData family, InstrumentCandle candle) {
        if (family.getOptions() == null) {
            family.setOptions(new ArrayList<>());
        }
        
        // Generate unique key for this option (strike + type)
        String optionKey = getOptionKey(candle);
        if (optionKey == null) {
            return; // Invalid option, skip
        }
        
        // OPTIMIZATION: Use HashMap for O(1) lookup instead of O(n) linear search
        Map<String, Integer> optionIndexMap = getOrCreateOptionIndexMap(family);
        Integer existingIdx = optionIndexMap.get(optionKey);
        
        if (existingIdx != null) {
            // Option already exists - update if new one has more volume (more recent/complete data)
            InstrumentCandle existing = family.getOptions().get(existingIdx);
            if (candle.getVolume() != null && 
                (existing.getVolume() == null || candle.getVolume() > existing.getVolume())) {
                family.getOptions().set(existingIdx, candle);
            }
        } else {
            // New option - add if we have space, or replace worst one
            if (family.getOptions().size() < 4) {
                family.getOptions().add(candle);
                optionIndexMap.put(optionKey, family.getOptions().size() - 1);
            } else {
                replaceOptionIfBetter(family, candle);
            }
        }
    }
    
    /**
     * Get or create option index map for O(1) lookups
     * OPTIMIZATION: Avoids O(n) linear search in family aggregation
     */
    private Map<String, Integer> getOrCreateOptionIndexMap(FamilyEnrichedData family) {
        Map<String, Integer> indexMap = new java.util.HashMap<>();
        if (family.getOptions() != null) {
            for (int i = 0; i < family.getOptions().size(); i++) {
                InstrumentCandle option = family.getOptions().get(i);
                String optionKey = getOptionKey(option);
                if (optionKey != null) {
                    indexMap.put(optionKey, i);
                }
            }
        }
        return indexMap;
    }
    
    /**
     * Generate unique key for option (strike + type)
     */
    private String getOptionKey(InstrumentCandle option) {
        if (option.getStrikePrice() == null || option.getOptionType() == null) {
            // Fallback to scripCode if strike/type not available
            return option.getScripCode();
        }
        return String.format("%.2f_%s", option.getStrikePrice(), option.getOptionType());
    }

    /**
     * Replace option with better candidate (closer to ATM)
     */
    public void replaceOptionIfBetter(FamilyEnrichedData family, InstrumentCandle candidate) {
        try {
            Double spot = family.getEquity() != null ? family.getEquity().getClose() : null;
            if (spot == null || candidate.getStrikePrice() == null) {
                return;
            }
            int worstIdx = -1;
            double worstDist = -1;
            for (int i = 0; i < family.getOptions().size(); i++) {
                InstrumentCandle opt = family.getOptions().get(i);
                if (opt.getStrikePrice() == null) continue;
                double dist = Math.abs(opt.getStrikePrice() - spot);
                if (dist > worstDist) {
                    worstDist = dist;
                    worstIdx = i;
                }
            }
            double candDist = Math.abs(candidate.getStrikePrice() - spot);
            if (worstIdx >= 0 && candDist < worstDist) {
                family.getOptions().set(worstIdx, candidate);
            }
        } catch (Exception ignored) {
        }
    }

    /**
     * Collect all instruments from family
     */
    private List<InstrumentCandle> collectAllInstruments(FamilyEnrichedData family) {
        List<InstrumentCandle> list = new ArrayList<>();
        if (family.getEquity() != null) list.add(family.getEquity());
        if (family.getFutures() != null) list.addAll(family.getFutures());
        if (family.getOptions() != null) list.addAll(family.getOptions());
        return list;
    }
}



--------------------------------------------------
File End
--------------------------------------------------


processor/service/MarketDataEnrichmentService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.*;
import com.kotsin.consumer.processor.MultiTimeframeState;
import com.kotsin.consumer.service.MongoInstrumentFamilyService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

/**
 * Service for enriching market data with instrument family and metadata
 * Single Responsibility: Build EnrichedMarketData messages
 *
 * Spring Best Practice: @Service annotation for business logic
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class MarketDataEnrichmentService {

    private final MongoInstrumentFamilyService cacheService;

    public EnrichedMarketData buildEnrichedMessage(MultiTimeframeState state) {
        try {
            // Get instrument family from cache
            InstrumentFamily family = cacheService.resolveFamily(
                state.getScripCode(),
                state.getExchangeType(),
                state.getCompanyName()
            );

            if (family == null) {
                log.warn("‚ö†Ô∏è No instrument family found for scripCode: {}", state.getScripCode());
                family = InstrumentFamily.builder()
                    .equityScripCode(state.getScripCode())
                    .companyName(state.getCompanyName())
                    .dataSource("CACHE_MISS")
                    .build();
            }

            EnrichedMarketData enrichedData = EnrichedMarketData.builder()
                .scripCode(state.getScripCode())
                .companyName(state.getCompanyName())
                .exchange(state.getExchange())
                .exchangeType(state.getExchangeType())
                .timestamp(state.getLastTickTime())
                .instrumentFamily(family)
                .multiTimeframeCandles(state.getMultiTimeframeCandles())
                .openInterest(state.getOpenInterest())
                .imbalanceBars(state.getImbalanceBars())
                .microstructure(state.getMicrostructure())
                .orderbookDepth(state.getOrderbookDepth())
                .metadata(MessageMetadata.builder()
                    .messageVersion("2.0")
                    .producedAt(System.currentTimeMillis())
                    .dataQuality(state.getDataQuality())
                    .completeWindows(state.getCompleteWindows())
                    .processingLatency((int) state.getProcessingLatency())
                    .source("unified-processor")
                    .sequenceNumber(state.getMessageCount())
                    .build())
                .build();

            log.debug("üì§ Built enriched message for {} with {} complete timeframes",
                state.getScripCode(), state.getCompleteWindows().size());

            return enrichedData;

        } catch (Exception e) {
            log.error("‚ùå Failed to build enriched message for scripCode: {}", state.getScripCode(), e);

            // Return minimal message to prevent data loss
            return EnrichedMarketData.builder()
                .scripCode(state.getScripCode())
                .companyName(state.getCompanyName())
                .exchange(state.getExchange())
                .exchangeType(state.getExchangeType())
                .timestamp(state.getLastTickTime())
                .metadata(MessageMetadata.builder()
                    .messageVersion("2.0")
                    .producedAt(System.currentTimeMillis())
                    .dataQuality("ERROR")
                    .source("unified-processor")
                    .build())
                .build();
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/InstrumentProcessor.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.TickData;
import com.kotsin.consumer.processor.InstrumentState;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

/**
 * Service for processing individual instrument data
 * 
 * SINGLE RESPONSIBILITY: Instrument-level processing logic
 * EXTRACTED FROM: UnifiedMarketDataProcessor (God class refactoring)
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class InstrumentProcessor {

    private final TradingHoursValidationService tradingHoursService;
    private final InstrumentKeyResolver keyResolver;

    /**
     * Process a single tick for an instrument
     */
    public void processTick(String scripCode, TickData tick, InstrumentState state) {
        if (tick == null || state == null) {
            return;
        }

        // Validate trading hours
        if (!tradingHoursService.withinTradingHours(tick)) {
            log.debug("‚è∞ Tick outside trading hours for {}", scripCode);
            return;
        }

        // Set instrument metadata if not already set
        if (state.getScripCode() == null || state.getScripCode().isEmpty()) {
            String instrumentType = keyResolver.getInstrumentType(tick);
            String familyKey = keyResolver.getFamilyKey(tick);
            state.setInstrumentType(instrumentType);
            state.setUnderlyingEquityScripCode(familyKey);
        }

        // Add tick to state (this will initialize scripCode and companyName automatically)
        state.addTick(tick);
        
        log.debug("üìä Processed tick for {}: price={}, volume={}", 
            scripCode, tick.getLastRate(), tick.getDeltaVolume());
    }

    /**
     * Check if instrument state is ready for emission
     */
    public boolean isReadyForEmission(InstrumentState state) {
        if (state == null) {
            return false;
        }

        // Check if we have any completed windows
        return state.hasAnyCompleteWindow();
    }

    /**
     * Get emission statistics for an instrument
     */
    public String getEmissionStats(InstrumentState state) {
        if (state == null) {
            return "No state";
        }

        return String.format("ScripCode: %s, Ticks: %d, Last: %d", 
            state.getScripCode(), 
            state.getMessageCount() != null ? state.getMessageCount() : 0,
            state.getLastTickTime() != null ? state.getLastTickTime() : 0);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/InstrumentKeyResolver.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.InstrumentFamily;
import com.kotsin.consumer.model.TickData;
import com.kotsin.consumer.service.MongoInstrumentFamilyService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

/**
 * Service for resolving instrument keys (derivatives to underlying equity)
 * Single Responsibility: Key resolution logic
 *
 * Spring Best Practice: @Service annotation for business logic
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class InstrumentKeyResolver {

    private final MongoInstrumentFamilyService cacheService;

    /**
     * Get scripCode for per-INSTRUMENT keying (Stream 1)
     * Each instrument gets its own separate key - NO MAPPING
     */
    public String getInstrumentKey(TickData tick) {
        if (tick == null) {
            return null;
        }
        // Use the instrument's OWN scripCode - each instrument is separate
        return tick.getScripCode();
    }

    /**
     * Get underlying equity scripCode for FAMILY-LEVEL aggregation (Stream 2)
     * Maps derivatives to underlying equity, handles indices correctly
     * 
     * CRITICAL FIX: Uses in-memory cache only, NO blocking MongoDB calls
     */
    public String getFamilyKey(TickData tick) {
        if (tick == null) {
            return null;
        }

        // For indices, return the index scripCode itself as the family key
        if (isIndex(tick.getScripCode())) {
            log.debug("üìç Index {} used as its own family key", tick.getScripCode());
            return tick.getScripCode();
        }

        // If it's a derivative, resolve to underlying equity using CACHE ONLY
        if ("D".equalsIgnoreCase(tick.getExchangeType())) {
            // CRITICAL: Use cache-only lookup to avoid blocking MongoDB calls
            InstrumentFamily family = cacheService.getFamily(tick.getScripCode());

            if (family != null && family.getEquityScripCode() != null) {
                log.debug("üìç Mapped derivative {} to family key {}",
                    tick.getScripCode(), family.getEquityScripCode());
                return family.getEquityScripCode();
            }
            
            // Fallback: Try to extract underlying from company name (no DB call)
            String underlying = extractUnderlyingFromCompanyName(tick.getCompanyName());
            if (underlying != null) {
                log.debug("üìç Extracted underlying {} from company name for derivative {}",
                    underlying, tick.getScripCode());
                return underlying;
            }
        }

        // For equities or if resolution fails, use the tick's own scripCode
        return tick.getScripCode();
    }
    
    /**
     * Extract underlying equity from derivative company name (no DB call)
     * Example: "RELIANCE 28 OCT 2025 CE 3850.00" -> "RELIANCE"
     */
    private String extractUnderlyingFromCompanyName(String companyName) {
        if (companyName == null || companyName.isBlank()) {
            return null;
        }
        
        // Extract first word as underlying symbol
        String[] parts = companyName.split("\\s+");
        if (parts.length > 0) {
            String underlying = parts[0].replaceAll("[^A-Za-z0-9&]", "");
            // Only return if it looks like a valid symbol (letters only)
            if (underlying.matches("[A-Za-z]+")) {
                return underlying;
            }
        }
        
        return null;
    }

    /**
     * Check if a scripCode represents an index
     * Indices typically start with 99992 (e.g., NIFTY, BANKNIFTY)
     */
    public boolean isIndex(String scripCode) {
        if (scripCode == null || scripCode.isEmpty()) {
            return false;
        }
        // Common patterns for indices:
        // - 99992xxxx for NSE indices (NIFTY, BANKNIFTY, FINNIFTY, etc.)
        // - Can be extended based on actual index scripCode patterns
        return scripCode.startsWith("99992");
    }

    /**
     * Determine instrument type from tick data
     */
    public String getInstrumentType(TickData tick) {
        if (tick == null) {
            return "UNKNOWN";
        }

        // Check if index
        if (isIndex(tick.getScripCode())) {
            return "INDEX";
        }

        // Check exchange type
        if ("D".equalsIgnoreCase(tick.getExchangeType())) {
            // Derivative - need to determine if future or option
            // Typically, options have strike price in company name
            String companyName = tick.getCompanyName();
            if (companyName != null) {
                if (companyName.contains(" CE ") || companyName.contains(" PE ")) {
                    return "OPTION";
                } else if (companyName.contains(" FUT") || companyName.matches(".*\\d{2}\\s+[A-Z]{3}\\s+\\d{4}.*")) {
                    return "FUTURE";
                }
            }
            return "DERIVATIVE";  // Generic if can't determine
        }

        // Default to equity
        return "EQUITY";
    }

    /**
     * DEPRECATED: Use getFamilyKey() instead
     * Kept for backward compatibility
     */
    @Deprecated
    public String getUnderlyingEquityScripCode(TickData tick) {
        return getFamilyKey(tick);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/SpoofingDetectionService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.OrderbookDepthData;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;

import java.util.*;
import java.util.Comparator;

/**
 * Service for detecting spoofing activity in the orderbook
 * Single Responsibility: Spoofing detection logic
 */
@Slf4j
public class SpoofingDetectionService {

    private static final long SPOOF_DURATION_THRESHOLD_MS = 5000;  // 5 seconds
    private static final double SPOOF_SIZE_THRESHOLD = 0.3;  // 30% of total depth
    private static final int MAX_TRACKING_ENTRIES = 20;  // Prevent unbounded growth

    private final List<OrderbookDepthData.SpoofingEvent> spoofingEvents = new ArrayList<>();
    private final Map<Double, SpoofDetectionState> bidSpoofTracking = new HashMap<>();
    private final Map<Double, SpoofDetectionState> askSpoofTracking = new HashMap<>();

    public void detectSpoofing(
        OrderBookSnapshot previousOrderbook,
        OrderBookSnapshot currentOrderbook
    ) {
        if (previousOrderbook == null || currentOrderbook == null) {
            return;
        }

        long currentTime = currentOrderbook.getTimestamp();

        // Check for large orders that disappeared on bid side
        if (previousOrderbook.getAllBids() != null && currentOrderbook.getAllBids() != null) {
            detectSpoofingOneSide(
                previousOrderbook.getAllBids(),
                currentOrderbook.getAllBids(),
                "BID",
                currentTime,
                bidSpoofTracking
            );
        }

        // Check for large orders that disappeared on ask side
        if (previousOrderbook.getAllAsks() != null && currentOrderbook.getAllAsks() != null) {
            detectSpoofingOneSide(
                previousOrderbook.getAllAsks(),
                currentOrderbook.getAllAsks(),
                "ASK",
                currentTime,
                askSpoofTracking
            );
        }

        // Clean up old spoofing events (older than 1 minute)
        long oneMinuteAgo = currentTime - 60000;
        spoofingEvents.removeIf(event -> event.getTimestamp() < oneMinuteAgo);
        
        // Clean up stale tracking entries (prevent unbounded growth)
        cleanupStaleTracking(bidSpoofTracking, currentTime);
        cleanupStaleTracking(askSpoofTracking, currentTime);
    }
    
    private void cleanupStaleTracking(Map<Double, SpoofDetectionState> tracking, long currentTime) {
        // Remove entries older than 10 seconds
        tracking.entrySet().removeIf(entry -> 
            (currentTime - entry.getValue().firstSeenTime) > 10000);
        
        // If still over limit, remove oldest entries
        if (tracking.size() > MAX_TRACKING_ENTRIES) {
            List<Map.Entry<Double, SpoofDetectionState>> entries = new ArrayList<>(tracking.entrySet());
            entries.sort(Comparator.comparingLong(e -> e.getValue().firstSeenTime));
            
            int toRemove = tracking.size() - MAX_TRACKING_ENTRIES;
            for (int i = 0; i < toRemove; i++) {
                tracking.remove(entries.get(i).getKey());
            }
        }
    }

    private void detectSpoofingOneSide(
        List<OrderBookSnapshot.OrderBookLevel> prevLevels,
        List<OrderBookSnapshot.OrderBookLevel> currLevels,
        String side,
        long currentTime,
        Map<Double, SpoofDetectionState> tracking
    ) {
        // Calculate total depth for threshold
        double totalDepth = prevLevels.stream()
            .mapToInt(OrderBookSnapshot.OrderBookLevel::getQuantity)
            .sum();

        // Check each previous level
        for (OrderBookSnapshot.OrderBookLevel prevLevel : prevLevels) {
            double price = prevLevel.getPrice();
            int quantity = prevLevel.getQuantity();

            // Is this a large order (> 30% of total depth)?
            if (quantity > totalDepth * SPOOF_SIZE_THRESHOLD) {
                // Track when we first saw this large order
                if (!tracking.containsKey(price)) {
                    tracking.put(price, new SpoofDetectionState(currentTime, quantity));
                }

                // Check if it disappeared in current orderbook
                boolean foundInCurrent = currLevels.stream()
                    .anyMatch(level -> Math.abs(level.getPrice() - price) < 0.01 &&
                                      level.getQuantity() >= quantity * 0.5);

                if (!foundInCurrent) {
                    // Large order disappeared - potential spoof
                    SpoofDetectionState state = tracking.get(price);
                    long duration = currentTime - state.firstSeenTime;

                    if (duration < SPOOF_DURATION_THRESHOLD_MS) {
                        // Disappeared quickly - likely a spoof
                        OrderbookDepthData.SpoofingEvent event = OrderbookDepthData.SpoofingEvent.builder()
                            .timestamp(currentTime)
                            .side(side)
                            .price(price)
                            .quantity(quantity)
                            .durationMs(duration)
                            .classification("CONFIRMED_SPOOF")
                            .build();

                        spoofingEvents.add(event);
                        log.warn("üö® Spoofing detected: {} @ {} qty={} duration={}ms",
                            side, price, quantity, duration);
                    }

                    tracking.remove(price);
                }
            }
        }
    }

    public List<OrderbookDepthData.SpoofingEvent> getSpoofingEvents() {
        return new ArrayList<>(spoofingEvents);
    }

    public int getSpoofingCount() {
        return spoofingEvents.size();
    }

    public boolean isActiveSpoofingBid() {
        return isActiveSpoofing(bidSpoofTracking);
    }

    public boolean isActiveSpoofingAsk() {
        return isActiveSpoofing(askSpoofTracking);
    }

    private boolean isActiveSpoofing(Map<Double, SpoofDetectionState> tracking) {
        long now = System.currentTimeMillis();
        return tracking.values().stream()
            .anyMatch(state -> (now - state.firstSeenTime) < SPOOF_DURATION_THRESHOLD_MS);
    }

    @Data
    @AllArgsConstructor
    private static class SpoofDetectionState {
        private final long firstSeenTime;
        private final int quantity;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/TopologyConfiguration.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.config.KafkaConfig;
import com.kotsin.consumer.model.FamilyAggregatedMetrics;
import com.kotsin.consumer.model.FamilyEnrichedData;
import com.kotsin.consumer.model.InstrumentCandle;
import com.kotsin.consumer.model.OpenInterest;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.TickData;
import com.kotsin.consumer.processor.InstrumentState;
import com.kotsin.consumer.monitoring.Timeframe;
// FIX: use the canonical metrics class (the one that has incCandleDrop)
import com.kotsin.consumer.metrics.StreamMetrics;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.Stores;
import org.apache.kafka.streams.state.WindowStore;
import org.apache.kafka.streams.state.KeyValueStore;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.common.serialization.Serdes;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

/**
 * Configuration service for Kafka Streams topologies
 *
 * SINGLE RESPONSIBILITY: Topology building and configuration
 * EXTRACTED FROM: UnifiedMarketDataProcessor (God class refactoring)
 *
 * FIXED ISSUES:
 * 1. Added metadata setting in instrument aggregation
 * 2. Fixed family aggregation to actually assemble families
 * 3. Added family metrics calculation
 * 4. Added trading hours filter
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class TopologyConfiguration {

    private final KafkaConfig kafkaConfig;
    private final StreamMetrics metrics;
    private final InstrumentKeyResolver keyResolver;  // ‚úÖ ADDED for metadata
    private final TradingHoursValidationService tradingHoursService;  // ‚úÖ ADDED for filtering

    @Value("${spring.kafka.streams.application-id:unified-market-processor1}")
    private String appIdPrefix;

    @Value("${unified.input.topic.ticks:forwardtesting-data}")
    private String ticksTopic;

    @Value("${unified.input.topic.oi:OpenInterest}")
    private String oiTopic;

    @Value("${unified.input.topic.orderbook:Orderbook}")
    private String orderbookTopic;

    @Value("${stream.outputs.candles.enabled:true}")
    private boolean candlesOutputEnabled;

    @Value("${stream.outputs.candles.1m:candle-complete-1m}")
    private String candle1mTopic;

    @Value("${stream.outputs.candles.2m:candle-complete-2m}")
    private String candle2mTopic;

    @Value("${stream.outputs.candles.3m:candle-complete-3m}")
    private String candle3mTopic;

    @Value("${stream.outputs.candles.5m:candle-complete-5m}")
    private String candle5mTopic;

    @Value("${stream.outputs.candles.15m:candle-complete-15m}")
    private String candle15mTopic;

    @Value("${stream.outputs.candles.30m:candle-complete-30m}")
    private String candle30mTopic;

    /**
     * Create per-instrument candle generation topology
     */
    public StreamsBuilder createInstrumentTopology() {
        log.info("üóÉÔ∏è Building per-instrument candle topology");

        Properties props = kafkaConfig.getStreamProperties(appIdPrefix + "-instrument");
        props.put("auto.offset.reset", "earliest");
        StreamsBuilder builder = new StreamsBuilder();

        // Add state store for delta volume calculation
        String deltaVolumeStoreName = "instrument-delta-volume-store";
        builder.addStateStore(
            Stores.keyValueStoreBuilder(
                Stores.persistentKeyValueStore(deltaVolumeStoreName),
                Serdes.String(),
                Serdes.Integer()
            )
        );

        // Read tick stream
        KStream<String, TickData> ticksRaw = builder.stream(
            ticksTopic,
            Consumed.with(Serdes.String(), TickData.serde())
        );

        // Transform cumulative to delta volume
        KStream<String, TickData> ticks = ticksRaw
            .transform(
                () -> new com.kotsin.consumer.transformers.CumToDeltaTransformer(deltaVolumeStoreName),
                deltaVolumeStoreName
            )
            .filter((key, tick) -> tick != null && tick.getDeltaVolume() != null);

        // Read orderbook stream for enrichment
        KStream<String, OrderBookSnapshot> orderbookStream = builder.stream(
            orderbookTopic,
            Consumed.with(Serdes.String(), OrderBookSnapshot.serde())
        );

        // Re-key orderbook by token
        KTable<String, OrderBookSnapshot> orderbookTable = orderbookStream
            .selectKey((k, v) -> v != null && v.getToken() != null ? v.getToken() : k)
            .toTable(Materialized.with(Serdes.String(), OrderBookSnapshot.serde()));

        // Re-key ticks by instrument key
        KStream<String, TickData> instrumentKeyed = ticks
            .selectKey((k, v) -> v.getScripCode());

        // Define windowing
        // BALANCED APPROACH: 30s grace period (delays candle by max 30s)
        // Trade-off: Captures 95%+ of late data without excessive delay
        // Note: suppress().untilWindowCloses() delays emission by grace period
        TimeWindows windows = TimeWindows
            .ofSizeAndGrace(Duration.ofMinutes(1), Duration.ofSeconds(30));

        // ‚úÖ FIX #1: Aggregate with metadata setting AND trading hours filter
        KTable<Windowed<String>, InstrumentState> aggregated = instrumentKeyed
            .filter((key, tick) -> {
                boolean withinHours = tradingHoursService.withinTradingHours(tick);
                if (!withinHours) {
                    log.debug("‚è∞ Filtered out tick outside trading hours: scripCode={}", tick.getScripCode());
                }
                return withinHours;
            })
            .groupByKey(Grouped.with(Serdes.String(), TickData.serde()))
            .windowedBy(windows)
            .aggregate(
                InstrumentState::new,
                (scripCode, tick, state) -> {
                    // ‚úÖ SET METADATA ON FIRST TICK (before adding tick!)
                    if (state.getScripCode() == null) {
                        try {
                            String instrumentType = keyResolver.getInstrumentType(tick);
                            String familyKey = keyResolver.getFamilyKey(tick);
                            state.setInstrumentType(instrumentType);
                            state.setUnderlyingEquityScripCode(familyKey);

                            log.debug("üîß Metadata set: scrip={}, type={}, family={}",
                                scripCode, instrumentType, familyKey);
                        } catch (Exception e) {
                            log.error("‚ùå Failed to set metadata for scripCode={}", scripCode, e);
                            // Continue with tick processing even if metadata fails
                        }
                    }

                    // Add tick to state
                    state.addTick(tick);
                    return state;
                },
                Materialized.<String, InstrumentState, WindowStore<Bytes, byte[]>>as("instrument-state-store")
                    .withKeySerde(Serdes.String())
                    .withValueSerde(new org.springframework.kafka.support.serializer.JsonSerde<>(InstrumentState.class))
            );

        // Suppress until window closes
        KStream<String, InstrumentState> stateStream = aggregated
            .suppress(Suppressed.untilWindowCloses(Suppressed.BufferConfig.unbounded()))
            .toStream()
            .peek((windowedKey, state) -> {
                long windowEnd = windowedKey.window().end();
                state.forceCompleteWindows(windowEnd);
                log.info("üéâ Window closed: scrip={} windowEnd={} hasComplete={}",
                    state.getScripCode(), windowEnd, state.hasAnyCompleteWindow());
            })
            .selectKey((windowedKey, state) -> windowedKey.key());

        // Enrich with orderbook data
        KStream<String, InstrumentState> enrichedState = stateStream
            .leftJoin(
                orderbookTable,
                (state, orderbook) -> {
                    if (orderbook != null && orderbook.isValid()) {
                        state.addOrderbook(orderbook);
                        log.debug("‚úÖ Orderbook joined for scripCode={}", state.getScripCode());
                    } else if (orderbook != null) {
                        log.debug("‚ö†Ô∏è Invalid orderbook for scripCode={}", state.getScripCode());
                    }
                    return state;
                }
            );

        // üöÄ CRITICAL FIX: Extract candles IMMEDIATELY before state gets modified
        // This prevents the race condition where new ticks reset accumulators
        if (candlesOutputEnabled) {
            log.info("üì§ Setting up immediate candle extraction to prevent race condition");

            // Extract and emit candles for each timeframe RIGHT NOW
            for (Timeframe tf : Timeframe.values()) {
                final String tfLabel = tf.getLabel();
                String topic = getCandleTopicForTimeframe(tfLabel);

                if (topic != null) {
                    log.info("üì§ Setting up candle emission for {} ‚Üí {}", tfLabel, topic);

                    enrichedState
                        .mapValues(state -> {
                            // Extract candle IMMEDIATELY while accumulator is still complete
                            InstrumentCandle candle = state.extractFinalizedCandle(tf);
                            if (candle != null) {
                                log.info("üì§ Extracted candle: tf={} scrip={} vol={} complete={} type={} family={}",
                                    tfLabel, candle.getScripCode(), candle.getVolume(),
                                    candle.getIsComplete(), candle.getInstrumentType(),
                                    candle.getUnderlyingEquityScripCode());
                            } else {
                                log.warn("‚ùå Failed to extract candle: tf={} scrip={} hasComplete={}",
                                    tfLabel, state.getScripCode(), state.hasAnyCompleteWindow());
                            }
                            return candle;
                        })
                        .filter((k, candle) -> {
                            if (candle == null) {
                                log.debug("‚ö†Ô∏è Filtered null candle for key={}", k);
                                return false;
                            }
                            if (!candle.isValid()) {
                                log.warn("‚ö†Ô∏è Filtered invalid candle: scrip={} vol={} ohlc=[{},{},{},{}]",
                                    candle.getScripCode(), candle.getVolume(),
                                    candle.getOpen(), candle.getHigh(), candle.getLow(), candle.getClose());
                                metrics.incCandleDrop(tfLabel);
                                return false;
                            }
                            return true;
                        })
                        .peek((k, candle) -> {
                            log.info("üì§ EMITTING: tf={} scrip={} vol={} type={} family={} ‚Üí {}",
                                tfLabel, candle.getScripCode(), candle.getVolume(),
                                candle.getInstrumentType(), candle.getUnderlyingEquityScripCode(), topic);
                            metrics.incCandleEmit(tfLabel);
                        })
                        .to(topic, Produced.with(Serdes.String(), InstrumentCandle.serde()));
                } else {
                    log.warn("‚ö†Ô∏è No topic configured for timeframe: {}", tfLabel);
                }
            }
        }

        return builder;
    }

    /**
     * Get candle topic for timeframe
     */
    private String getCandleTopicForTimeframe(String timeframe) {
        switch (timeframe) {
            case "1m": return candle1mTopic;
            case "2m": return candle2mTopic;
            case "3m": return candle3mTopic;
            case "5m": return candle5mTopic;
            case "15m": return candle15mTopic;
            case "30m": return candle30mTopic;
            default:
                log.warn("‚ö†Ô∏è Unknown timeframe: {}", timeframe);
                return null;
        }
    }

    /**
     * Create family-structured aggregation topology
     *
     * ‚úÖ FIX #2: Actually assemble families instead of returning empty builder
     */
    public StreamsBuilder createFamilyTopology(String timeframeLabel, String sourceTopic, String sinkTopic, Duration windowSize) {
        log.info("üóÉÔ∏è Building family-structured topology for {}", timeframeLabel);

        Properties props = kafkaConfig.getStreamProperties(appIdPrefix + "-family-" + timeframeLabel);
        props.put("auto.offset.reset", "earliest");
        StreamsBuilder builder = new StreamsBuilder();

        // Read candle stream
        KStream<String, InstrumentCandle> candles = builder.stream(
            sourceTopic,
            Consumed.with(Serdes.String(), InstrumentCandle.serde())
        );

        // Re-key by scripCode
        KStream<String, InstrumentCandle> rekeyedByToken = candles
            .selectKey((k, c) -> c != null ? c.getScripCode() : k)
            .repartition(Repartitioned.with(Serdes.String(), InstrumentCandle.serde()));

        // Build OI enrichment table
        KTable<String, OpenInterest> oiTable = buildOiTable(builder);

        // Enrich candles with OI data
        KStream<String, InstrumentCandle> enrichedCandles = enrichCandles(rekeyedByToken, oiTable);

        // Map to family key and aggregate
        KStream<String, InstrumentCandle> keyedByFamily = enrichedCandles
            .selectKey((scripOrToken, candle) -> {
                String familyKey = candle.getUnderlyingEquityScripCode() != null
                    ? candle.getUnderlyingEquityScripCode()
                    : candle.getScripCode();

                log.debug("üîë Keying candle by family: scrip={} type={} familyKey={}",
                    candle.getScripCode(), candle.getInstrumentType(), familyKey);

                return familyKey;
            })
            .repartition(Repartitioned.with(Serdes.String(), InstrumentCandle.serde()));

        // BALANCED APPROACH: 30s grace period for family aggregation
        // Aligns with instrument stream grace period
        TimeWindows windows = TimeWindows.ofSizeAndGrace(windowSize, Duration.ofSeconds(30));

        // ‚úÖ FIX #2: ACTUALLY ASSEMBLE FAMILIES
        KTable<Windowed<String>, FamilyEnrichedData> aggregated = keyedByFamily
            .groupByKey(Grouped.with(Serdes.String(), InstrumentCandle.serde()))
            .windowedBy(windows)
            .aggregate(
                () -> {
                    // Initialize with empty lists
                    FamilyEnrichedData family = FamilyEnrichedData.builder()
                        .futures(new ArrayList<>())
                        .options(new ArrayList<>())
                        .build();
                    log.debug("üèóÔ∏è Created new family aggregate");
                    return family;
                },
                (familyKey, candle, family) -> {
                    // Set family metadata on first candle
                    if (family.getFamilyKey() == null) {
                        family.setFamilyKey(familyKey);
                        family.setFamilyName(familyKey);  // Could lookup full name from service
                        family.setInstrumentType("EQUITY_FAMILY");
                        log.info("üèóÔ∏è Initialized family: key={} name={}", familyKey, familyKey);
                    }

                    // Set/update window times
                    if (family.getWindowStartMillis() == null && candle.getWindowStartMillis() != null) {
                        family.setWindowStartMillis(candle.getWindowStartMillis());
                    }
                    if (candle.getWindowEndMillis() != null) {
                        family.setWindowEndMillis(candle.getWindowEndMillis());
                    }

                    // Add instrument to family based on type
                    String instrType = candle.getInstrumentType();
                    if (instrType != null) {
                        switch (instrType) {
                            case "EQUITY":
                                log.info("‚ûï Adding EQUITY to family: familyKey={} scripCode={}",
                                    familyKey, candle.getScripCode());
                                family.setEquity(candle);
                                break;

                            case "FUTURE":
                                log.info("‚ûï Adding FUTURE to family: familyKey={} scripCode={} expiry={}",
                                    familyKey, candle.getScripCode(), candle.getExpiry());
                                if (family.getFutures() == null) {
                                    family.setFutures(new ArrayList<>());
                                }
                                family.getFutures().add(candle);
                                break;

                            case "OPTION":
                                log.info("‚ûï Adding OPTION to family: familyKey={} scripCode={} type={} strike={}",
                                    familyKey, candle.getScripCode(), candle.getOptionType(), candle.getStrikePrice());
                                if (family.getOptions() == null) {
                                    family.setOptions(new ArrayList<>());
                                }
                                family.getOptions().add(candle);
                                break;

                            default:
                                log.warn("‚ö†Ô∏è Unknown instrument type: {} for scripCode={}",
                                    instrType, candle.getScripCode());
                        }
                    } else {
                        log.warn("‚ö†Ô∏è Candle has null instrumentType: scripCode={}", candle.getScripCode());
                    }

                    return family;
                },
                Materialized.with(Serdes.String(), FamilyEnrichedData.serde())
            );

        // ‚úÖ FIX #3: Calculate aggregated metrics
        KStream<String, FamilyEnrichedData> out = aggregated
            .suppress(Suppressed.untilWindowCloses(Suppressed.BufferConfig.unbounded()))
            .toStream()
            .selectKey((windowedKey, family) -> windowedKey.key())
            .mapValues(family -> {
                if (family != null) {
                    family.setProcessingTimestamp(System.currentTimeMillis());
                    family.setTimeframe(timeframeLabel);

                    // Calculate aggregated metrics
                    FamilyAggregatedMetrics famMetrics = calculateFamilyMetrics(family);
                    family.setAggregatedMetrics(famMetrics);

                    // Set total count (derived)
                    family.setTotalInstrumentsCount(
                        (family.getEquity() != null ? 1 : 0) +
                        (family.getFutures() != null ? family.getFutures().size() : 0) +
                        (family.getOptions() != null ? family.getOptions().size() : 0)
                    );

                    // NOTE: Removed non-existent setters setFuturesCount/setOptionsCount.
                    // If you need explicit counts in the payload, compute and expose via getters or metrics.
                    log.info("üìä Family complete: key={} tf={} equity={} futures={} options={} totalVol={}",
                        family.getFamilyKey(), timeframeLabel,
                        family.getEquity() != null,
                        (family.getFutures() != null ? family.getFutures().size() : 0),
                        (family.getOptions() != null ? family.getOptions().size() : 0),
                        famMetrics.getTotalVolume());
                }
                return family;
            });

        out.to(sinkTopic, Produced.with(Serdes.String(), FamilyEnrichedData.serde()));

        return builder;
    }

    /**
     * Calculate aggregated metrics for a family
     *
     * ‚úÖ FIX #3: Proper metrics calculation
     */
    private FamilyAggregatedMetrics calculateFamilyMetrics(FamilyEnrichedData family) {
        long totalVolume = 0;
        long equityVolume = 0;
        long futuresVolume = 0;
        long optionsVolume = 0;

        Double spotPrice = null;
        Double nearMonthFuturePrice = null;
        String nearMonthExpiry = null;

        // Equity metrics
        if (family.getEquity() != null) {
            InstrumentCandle equity = family.getEquity();
            equityVolume = equity.getVolume() != null ? equity.getVolume() : 0;
            spotPrice = equity.getClose();
            totalVolume += equityVolume;
        }

        // Futures metrics
        if (family.getFutures() != null && !family.getFutures().isEmpty()) {
            for (InstrumentCandle future : family.getFutures()) {
                long vol = future.getVolume() != null ? future.getVolume() : 0;
                futuresVolume += vol;
                totalVolume += vol;

                // Find near month future (earliest expiry)
                if (nearMonthExpiry == null ||
                    (future.getExpiry() != null && future.getExpiry().compareTo(nearMonthExpiry) < 0)) {
                    nearMonthExpiry = future.getExpiry();
                    nearMonthFuturePrice = future.getClose();
                }
            }
        }

        // Options metrics
        long callsVolume = 0;
        long putsVolume = 0;
        if (family.getOptions() != null && !family.getOptions().isEmpty()) {
            for (InstrumentCandle option : family.getOptions()) {
                long vol = option.getVolume() != null ? option.getVolume() : 0;
                optionsVolume += vol;
                totalVolume += vol;

                if ("CE".equals(option.getOptionType())) {
                    callsVolume += vol;
                } else if ("PE".equals(option.getOptionType())) {
                    putsVolume += vol;
                }
            }
        }

        // Calculate ratios
        Double putCallVolumeRatio = callsVolume > 0 ? (double) putsVolume / callsVolume : null;

        // Calculate futures basis
        Double futuresBasis = null;
        Double futuresBasisPercent = null;
        if (spotPrice != null && nearMonthFuturePrice != null && spotPrice > 0) {
            futuresBasis = nearMonthFuturePrice - spotPrice;
            futuresBasisPercent = (futuresBasis / spotPrice) * 100.0;
        }

        return FamilyAggregatedMetrics.builder()
            .totalVolume(totalVolume)
            .equityVolume(equityVolume)
            .futuresVolume(futuresVolume)
            .optionsVolume(optionsVolume)
            .spotPrice(spotPrice)
            .nearMonthFuturePrice(nearMonthFuturePrice)
            .futuresBasis(futuresBasis)
            .futuresBasisPercent(futuresBasisPercent)
            .activeFuturesCount(family.getFutures() != null ? family.getFutures().size() : 0)
            .activeOptionsCount(family.getOptions() != null ? family.getOptions().size() : 0)
            .nearMonthExpiry(nearMonthExpiry)
            .putCallVolumeRatio(putCallVolumeRatio)
            .calculatedAt(System.currentTimeMillis())
            .build();
    }

    /**
     * Build OI table with stateful oiChange calculation
     */
    private KTable<String, OpenInterest> buildOiTable(StreamsBuilder builder) {
        KStream<String, OpenInterest> oiStream = builder.stream(
            oiTopic,
            Consumed.with(Serdes.String(), OpenInterest.serde())
        );

        return oiStream
            .selectKey((k, oi) -> oi != null && oi.getToken() != 0 ? String.valueOf(oi.getToken()) : k)
            .toTable(Materialized.<String, OpenInterest, KeyValueStore<Bytes, byte[]>>as("oi-table")
                .withKeySerde(Serdes.String())
                .withValueSerde(OpenInterest.serde()));
    }

    /**
     * Enrich candles with OI data
     */
    private KStream<String, InstrumentCandle> enrichCandles(
        KStream<String, InstrumentCandle> candles,
        KTable<String, OpenInterest> oiTable
    ) {
        return candles.leftJoin(
            oiTable,
            (candle, oi) -> {
                if (candle != null && oi != null) {
                    candle.setOpenInterest(oi.getOpenInterest());
                    candle.setOiChange(oi.getOiChange());
                    log.debug("‚úÖ OI enriched: scrip={} oi={} oiChange={}",
                        candle.getScripCode(), oi.getOpenInterest(), oi.getOiChange());
                }
                return candle;
            }
        );
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/MarketDataMergeService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.OpenInterest;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.TickData;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

/**
 * Service for merging tick data with OI and Orderbook data
 * Single Responsibility: Data merging logic
 *
 * Spring Best Practice: @Service annotation for business logic
 */
@Service
@Slf4j
public class MarketDataMergeService {

    /**
     * Merge Open Interest data into tick
     */
    public TickData mergeOiIntoTick(TickData tick, OpenInterest oi) {
        if (oi != null) {
            tick.setOpenInterest(oi.getOpenInterest());
            tick.setOiChange(oi.getOiChange());
        }
        return tick;
    }

    /**
     * Merge Orderbook data into tick
     */
    public TickData mergeOrderbookIntoTick(TickData tick, OrderBookSnapshot orderbook) {
        if (orderbook != null && orderbook.isValid()) {
            // Parse orderbook details if not already parsed
            orderbook.parseDetails();

            // Update bid/ask with orderbook's best levels (more accurate than tick's cached values)
            double bestBid = orderbook.getBestBid();
            double bestAsk = orderbook.getBestAsk();

            if (bestBid > 0) {
                tick.setBidRate(bestBid);
            }
            if (bestAsk > 0) {
                tick.setOfferRate(bestAsk);
            }

            // Update total quantities from orderbook (aggregated depth)
            if (orderbook.getTotalBidQty() != null) {
                tick.setTotalBidQuantity(orderbook.getTotalBidQty().intValue());
            }
            if (orderbook.getTotalOffQty() != null) {
                tick.setTotalOfferQuantity(orderbook.getTotalOffQty().intValue());
            }

            // Update best bid/ask quantities (top of book)
            if (orderbook.getAllBids() != null && !orderbook.getAllBids().isEmpty()) {
                tick.setBidQuantity(orderbook.getAllBids().get(0).getQuantity());
            }
            if (orderbook.getAllAsks() != null && !orderbook.getAllAsks().isEmpty()) {
                tick.setOfferQuantity(orderbook.getAllAsks().get(0).getQuantity());
            }

            // Store full orderbook for depth analytics (transient field)
            tick.setFullOrderbook(orderbook);

            log.debug("üìñ Merged orderbook into tick {}: bid={}, ask={}, spread={}, totalBid={}, totalAsk={}",
                tick.getScripCode(), bestBid, bestAsk, orderbook.getSpread(),
                orderbook.getTotalBidQty(), orderbook.getTotalOffQty());
        }
        return tick;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/InstrumentStateManager.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.*;
import com.kotsin.consumer.processor.CandleAccumulator;
import com.kotsin.consumer.processor.MicrostructureAccumulator;
import com.kotsin.consumer.processor.ImbalanceBarAccumulator;
import com.kotsin.consumer.monitoring.Timeframe;
import com.kotsin.consumer.processor.WindowRotationService;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;

import java.text.SimpleDateFormat;
import java.util.*;

/**
 * Manages per-INSTRUMENT state aggregation
 * Single Responsibility: Aggregate data for ONE specific instrument (no family grouping)
 *
 * Key Design: Each scripCode gets its own state - no mixing of instruments
 */
@Data
@Slf4j
public class InstrumentStateManager {

    // Candle accumulators for each timeframe
    private final EnumMap<Timeframe, CandleAccumulator> candleAccumulators = new EnumMap<>(Timeframe.class);

    // Microstructure accumulators per timeframe (reset on window rotation)
    private final EnumMap<Timeframe, MicrostructureAccumulator> microAccumulators = new EnumMap<>(Timeframe.class);
    // Imbalance bar accumulators per timeframe
    private final EnumMap<Timeframe, ImbalanceBarAccumulator> imbAccumulators = new EnumMap<>(Timeframe.class);
    // Orderbook depth accumulators per timeframe (reset on window rotation for clean metrics)
    private final EnumMap<Timeframe, com.kotsin.consumer.processor.OrderbookDepthAccumulator> orderbookAccumulators = new EnumMap<>(Timeframe.class);
    
    // Global orderbook accumulator (NEVER reset - for iceberg/spoofing detection across windows)
    private com.kotsin.consumer.processor.OrderbookDepthAccumulator globalOrderbookAccumulator;

    // Basic instrument info
    private String scripCode;
    private String companyName;
    private String exchange;
    private String exchangeType;
    private String instrumentType;              // EQUITY, FUTURE, OPTION, INDEX
    private String underlyingEquityScripCode;

    // Derivative-specific fields
    private String expiry;
    private Double strikePrice;
    private String optionType;                  // CE or PE

    // Tick tracking
    private Long firstTickTime;
    private Long lastTickTime;
    private Long messageCount = 0L;

    // Timeframe definitions
    private static final Timeframe[] TIMEFRAMES = {
        Timeframe.ONE_MIN, Timeframe.TWO_MIN, Timeframe.THREE_MIN,
        Timeframe.FIVE_MIN, Timeframe.FIFTEEN_MIN, Timeframe.THIRTY_MIN
    };

    private static final SimpleDateFormat DATE_FORMAT = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");

    public InstrumentStateManager() {
        initializeAccumulators();
    }

    private void initializeAccumulators() {
        // Initialize global orderbook accumulator (never reset)
        globalOrderbookAccumulator = new com.kotsin.consumer.processor.OrderbookDepthAccumulator();
        
        // Initialize per-timeframe accumulators
        for (Timeframe timeframe : TIMEFRAMES) {
            candleAccumulators.put(timeframe, new CandleAccumulator());
            microAccumulators.put(timeframe, new MicrostructureAccumulator());
            imbAccumulators.put(timeframe, new ImbalanceBarAccumulator());
            orderbookAccumulators.put(timeframe, new com.kotsin.consumer.processor.OrderbookDepthAccumulator());
        }
    }

    public void addTick(TickData tick) {
        if (scripCode == null) {
            scripCode = tick.getScripCode();
            companyName = tick.getCompanyName();
            exchange = tick.getExchange();
            exchangeType = tick.getExchangeType();
            firstTickTime = tick.getTimestamp();

            // Extract derivative info from company name if available
            extractDerivativeInfo(tick);
        }

        lastTickTime = tick.getTimestamp();
        messageCount++;

        // Update all timeframes
        updateAllTimeframes(tick);
    }

    public void addOrderbook(com.kotsin.consumer.model.OrderBookSnapshot orderbook) {
        if (orderbook == null || !orderbook.isValid()) {
            log.debug("‚ö†Ô∏è OB INVALID in manager: scripCode={} ob={} valid={}", 
                scripCode, orderbook != null, orderbook != null && orderbook.isValid());
            return;
        }
        
        log.debug("‚úÖ OB ADD to manager: scripCode={} token={} bids={} asks={}", 
            scripCode, orderbook.getToken(), 
            orderbook.getBids() != null ? orderbook.getBids().size() : 0,
            orderbook.getAsks() != null ? orderbook.getAsks().size() : 0);
        
        // Update global accumulator (NEVER reset - for iceberg/spoofing detection)
        globalOrderbookAccumulator.addOrderbook(orderbook);
        
        // Update per-timeframe accumulators (reset on window rotation for clean metrics)
        for (com.kotsin.consumer.processor.OrderbookDepthAccumulator acc : orderbookAccumulators.values()) {
            acc.addOrderbook(orderbook);
        }
    }

    /**
     * Extract derivative information (expiry, strike, option type) from tick
     */
    private void extractDerivativeInfo(TickData tick) {
        if (tick.getCompanyName() == null) {
            return;
        }

        String name = tick.getCompanyName();

        // Extract option type (CE or PE)
        if (name.contains(" CE ")) {
            optionType = "CE";
            // Extract strike price: "LT 28 OCT 2025 CE 3850.00"
            String[] parts = name.split(" CE ");
            if (parts.length > 1) {
                try {
                    strikePrice = Double.parseDouble(parts[1].trim());
                } catch (NumberFormatException e) {
                    log.debug("Could not parse strike price from: {}", name);
                }
            }
            // Extract expiry
            expiry = extractExpiryFromName(name);
        } else if (name.contains(" PE ")) {
            optionType = "PE";
            // Extract strike price
            String[] parts = name.split(" PE ");
            if (parts.length > 1) {
                try {
                    strikePrice = Double.parseDouble(parts[1].trim());
                } catch (NumberFormatException e) {
                    log.debug("Could not parse strike price from: {}", name);
                }
            }
            // Extract expiry
            expiry = extractExpiryFromName(name);
        } else if (name.matches(".*\\d{2}\\s+[A-Z]{3}\\s+\\d{4}.*")) {
            // Future: "GOLDM 05 NOV 2025"
            expiry = extractExpiryFromName(name);
        }
    }

    /**
     * Extract expiry date from company name
     * Format: "DD MMM YYYY" (e.g., "28 OCT 2025")
     */
    private String extractExpiryFromName(String companyName) {
        try {
            // Pattern: DD MMM YYYY
            String[] parts = companyName.split("\\s+");
            for (int i = 0; i < parts.length - 2; i++) {
                String day = parts[i];
                String month = parts[i + 1];
                String year = parts[i + 2];

                // Check if day is 1-2 digits, month is 3 letters, year is 4 digits
                if (day.matches("\\d{1,2}") && month.matches("[A-Z]{3}") && year.matches("\\d{4}")) {
                    // Convert to YYYY-MM-DD format
                    String monthNum = convertMonthToNumber(month);
                    if (monthNum != null) {
                        return String.format("%s-%s-%02d", year, monthNum, Integer.parseInt(day));
                    }
                }
            }
        } catch (Exception e) {
            log.debug("Could not extract expiry from: {}", companyName);
        }
        return null;
    }

    private String convertMonthToNumber(String month) {
        switch (month.toUpperCase()) {
            case "JAN": return "01";
            case "FEB": return "02";
            case "MAR": return "03";
            case "APR": return "04";
            case "MAY": return "05";
            case "JUN": return "06";
            case "JUL": return "07";
            case "AUG": return "08";
            case "SEP": return "09";
            case "OCT": return "10";
            case "NOV": return "11";
            case "DEC": return "12";
            default: return null;
        }
    }

    private void updateAllTimeframes(TickData tick) {
        long tickTime = tick.getTimestamp();

        for (Map.Entry<Timeframe, CandleAccumulator> entry : candleAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            CandleAccumulator acc = entry.getValue();
            int minutes = timeframe.getMinutes();

            // Detect window rotation to reset per-timeframe microstructure accumulator
            Long prevWindowStart = acc.getWindowStart();
            acc = WindowRotationService.rotateCandleIfNeeded(acc, tickTime, minutes);
            candleAccumulators.put(timeframe, acc);
            if (prevWindowStart == null || !prevWindowStart.equals(acc.getWindowStart())) {
                // New window started ‚Üí reset per-timeframe accumulators
                microAccumulators.put(timeframe, new MicrostructureAccumulator());
                imbAccumulators.put(timeframe, new ImbalanceBarAccumulator());
                orderbookAccumulators.put(timeframe, new com.kotsin.consumer.processor.OrderbookDepthAccumulator());
            }

            // Update accumulators
            acc.addTick(tick);
            MicrostructureAccumulator microAcc = microAccumulators.get(timeframe);
            if (microAcc != null) { microAcc.addTick(tick); }
            ImbalanceBarAccumulator imbAcc = imbAccumulators.get(timeframe);
            if (imbAcc != null) { imbAcc.addTick(tick); }
        }
    }

    public boolean hasAnyCompleteWindow() {
        return candleAccumulators.values().stream()
            .anyMatch(CandleAccumulator::isComplete);
    }

    public Set<String> getCompleteWindows() {
        return new HashSet<>();
    }

    /**
     * Extract finalized candle for a specific timeframe
     */
    public InstrumentCandle extractFinalizedCandle(Timeframe timeframe) {
        CandleAccumulator accumulator = candleAccumulators.get(timeframe);
        if (accumulator == null || !accumulator.isComplete()) {
            return null;
        }

        CandleData candleData = accumulator.toCandleData(exchange, exchangeType);

        // Get microstructure data for this timeframe window
        MicrostructureAccumulator microAcc = microAccumulators.get(timeframe);
        MicrostructureData microstructure = microAcc != null ? 
            microAcc.toMicrostructureData(accumulator.getWindowStart(), accumulator.getWindowEnd()) : null;
        ImbalanceBarAccumulator imbAcc = imbAccumulators.get(timeframe);
        ImbalanceBarData imbalanceBars = imbAcc != null ? imbAcc.toImbalanceBarData() : null;
        
        // DUAL ORDERBOOK ACCUMULATOR MERGE:
        // 1. Get per-window orderbook metrics (spread, depth, imbalances)
        com.kotsin.consumer.processor.OrderbookDepthAccumulator obAcc = orderbookAccumulators.get(timeframe);
        com.kotsin.consumer.model.OrderbookDepthData windowOrderbook = obAcc != null ? obAcc.toOrderbookDepthData() : null;
        
        // 2. Get global iceberg/spoofing detection (crosses window boundaries)
        com.kotsin.consumer.model.OrderbookDepthData globalDetection = globalOrderbookAccumulator != null ? 
            globalOrderbookAccumulator.toOrderbookDepthData() : null;
        
        // 3. Merge: use window metrics, but overlay global iceberg/spoofing data
        com.kotsin.consumer.model.OrderbookDepthData orderbookDepth = mergeOrderbookData(windowOrderbook, globalDetection);

        return InstrumentCandle.builder()
            .scripCode(scripCode)
            .instrumentType(instrumentType)
            .underlyingEquityScripCode(underlyingEquityScripCode)
            .companyName(companyName)
            .exchange(exchange)
            .exchangeType(exchangeType)
            .expiry(expiry)
            .strikePrice(strikePrice)
            .optionType(optionType)
            .open(candleData.getOpen())
            .high(candleData.getHigh())
            .low(candleData.getLow())
            .close(candleData.getClose())
            .volume(candleData.getVolume())
            .buyVolume(candleData.getBuyVolume())
            .sellVolume(candleData.getSellVolume())
            .volumeDelta(candleData.getVolumeDelta())
            .volumeDeltaPercent(candleData.getVolumeDeltaPercent())
            .vwap(candleData.getVwap())
            .hlc3((candleData.getHigh() != null && candleData.getLow() != null && candleData.getClose() != null)
                ? (candleData.getHigh() + candleData.getLow() + candleData.getClose()) / 3.0
                : null)
            .tickCount(candleData.getTickCount())
            .windowStartMillis(candleData.getWindowStart())
            .windowEndMillis(candleData.getWindowEnd())
            .isComplete(candleData.getIsComplete())
            .humanReadableStartTime(formatTimestamp(candleData.getWindowStart()))
            .humanReadableEndTime(formatTimestamp(candleData.getWindowEnd()))
            .processingTimestamp(System.currentTimeMillis())
            .timeframe(timeframe.getLabel())
            .microstructure(microstructure != null && microstructure.isValid() ? microstructure : null)
            .imbalanceBars(imbalanceBars)
            .orderbookDepth(orderbookDepth)
            .build();
    }

    /**
     * Merge per-window orderbook metrics with global iceberg/spoofing detection
     * 
     * Strategy:
     * - Use window metrics for: spread, depth, imbalances, VWAPs, slopes (time-aligned)
     * - Use global metrics for: iceberg detection, spoofing detection (cross-window history)
     */
    private com.kotsin.consumer.model.OrderbookDepthData mergeOrderbookData(
        com.kotsin.consumer.model.OrderbookDepthData windowData,
        com.kotsin.consumer.model.OrderbookDepthData globalData
    ) {
        if (windowData == null && globalData == null) return null;
        if (windowData == null) return globalData;  // Only global data available
        if (globalData == null) return windowData;  // Only window data available
        
        // Merge: window metrics + global iceberg/spoofing
        return com.kotsin.consumer.model.OrderbookDepthData.builder()
            // Use window-aligned metrics
            .spread(windowData.getSpread())
            .totalBidDepth(windowData.getTotalBidDepth())
            .totalAskDepth(windowData.getTotalAskDepth())
            .weightedDepthImbalance(windowData.getWeightedDepthImbalance())
            .level1Imbalance(windowData.getLevel1Imbalance())
            .level2to5Imbalance(windowData.getLevel2to5Imbalance())
            .level6to10Imbalance(windowData.getLevel6to10Imbalance())
            .bidVWAP(windowData.getBidVWAP())
            .askVWAP(windowData.getAskVWAP())
            .bidSlope(windowData.getBidSlope())
            .askSlope(windowData.getAskSlope())
            .slopeRatio(windowData.getSlopeRatio())
            .bidProfile(windowData.getBidProfile())
            .askProfile(windowData.getAskProfile())
            .timestamp(windowData.getTimestamp())
            .depthLevels(windowData.getDepthLevels())
            // Use global cross-window iceberg/spoofing detection
            .icebergDetectedBid(globalData.getIcebergDetectedBid())
            .icebergDetectedAsk(globalData.getIcebergDetectedAsk())
            .icebergProbabilityBid(globalData.getIcebergProbabilityBid())
            .icebergProbabilityAsk(globalData.getIcebergProbabilityAsk())
            .spoofingCountLast1Min(globalData.getSpoofingCountLast1Min())
            .activeSpoofingBid(globalData.getActiveSpoofingBid())
            .activeSpoofingAsk(globalData.getActiveSpoofingAsk())
            .spoofingEvents(globalData.getSpoofingEvents())
            // Flags (isValid() is a computed method, not a field)
            .isComplete(windowData.getIsComplete() != null ? windowData.getIsComplete() : globalData.getIsComplete())
            .build();
    }

    private String formatTimestamp(Long timestamp) {
        if (timestamp == null) {
            return null;
        }
        return DATE_FORMAT.format(new Date(timestamp));
    }

    /**
     * Force completion of all windows for finalized candle emission
     */
    public void forceCompleteWindows(long kafkaWindowEnd) {
        int completedCount = 0;

        for (Map.Entry<Timeframe, CandleAccumulator> entry : candleAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            CandleAccumulator accumulator = entry.getValue();

            if (accumulator.getWindowStart() != null &&
                accumulator.getWindowEnd() != null &&
                !accumulator.isComplete() &&
                kafkaWindowEnd >= accumulator.getWindowEnd()) {

                accumulator.markComplete();
                completedCount++;
                log.info("‚úÖ Completed {} window [{}‚Üí{}] at Kafka window {} (scripCode={})",
                    timeframe.getLabel(),
                    accumulator.getWindowStart(),
                    accumulator.getWindowEnd(),
                    kafkaWindowEnd,
                    scripCode);
            }
        }

        if (completedCount > 0) {
            log.info("üéâ Marked {} windows as complete for scripCode={} at Kafka window end {}",
                completedCount, scripCode, kafkaWindowEnd);
        }
    }

    public EnumMap<Timeframe, CandleAccumulator> getCandleAccumulators() {
        return candleAccumulators;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/StreamMetrics.java
File type: .java
package com.kotsin.consumer.processor.service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

import java.util.concurrent.atomic.AtomicLong;

/**
 * Stream metrics service for monitoring processing performance
 * 
 * SINGLE RESPONSIBILITY: Metrics collection and reporting
 * EXTRACTED FROM: UnifiedMarketDataProcessor (God class refactoring)
 */
@Component("processorStreamMetrics")
@RequiredArgsConstructor
@Slf4j
public class StreamMetrics {

    private final AtomicLong processedTicks = new AtomicLong(0);
    private final AtomicLong processedCandles = new AtomicLong(0);
    private final AtomicLong processedFamilies = new AtomicLong(0);
    private final AtomicLong errorCount = new AtomicLong(0);
    private final AtomicLong startTime = new AtomicLong(System.currentTimeMillis());

    /**
     * Record processed tick
     */
    public void recordTick() {
        processedTicks.incrementAndGet();
    }

    /**
     * Record processed candle
     */
    public void recordCandle() {
        processedCandles.incrementAndGet();
    }

    /**
     * Record candle emission (alias for recordCandle)
     */
    public void incCandleEmit(String timeframe) {
        processedCandles.incrementAndGet();
        log.debug("üì§ Candle emitted: timeframe={}, total={}", timeframe, processedCandles.get());
    }

    /**
     * Record processed family
     */
    public void recordFamily() {
        processedFamilies.incrementAndGet();
    }

    /**
     * Record error
     */
    public void recordError() {
        errorCount.incrementAndGet();
    }

    /**
     * Get metrics summary
     */
    public String getMetrics() {
        long uptime = System.currentTimeMillis() - startTime.get();
        long uptimeSeconds = uptime / 1000;
        
        return String.format(
            "Metrics: Ticks=%d, Candles=%d, Families=%d, Errors=%d, Uptime=%ds, TPS=%.2f",
            processedTicks.get(),
            processedCandles.get(),
            processedFamilies.get(),
            errorCount.get(),
            uptimeSeconds,
            uptimeSeconds > 0 ? (double) processedTicks.get() / uptimeSeconds : 0.0
        );
    }

    /**
     * Get detailed metrics
     */
    public String getDetailedMetrics() {
        long uptime = System.currentTimeMillis() - startTime.get();
        long uptimeSeconds = uptime / 1000;
        
        return String.format(
            "Detailed Metrics:\n" +
            "  Processed Ticks: %d\n" +
            "  Processed Candles: %d\n" +
            "  Processed Families: %d\n" +
            "  Error Count: %d\n" +
            "  Uptime: %d seconds\n" +
            "  Ticks per second: %.2f\n" +
            "  Candles per second: %.2f\n" +
            "  Families per second: %.2f\n" +
            "  Error rate: %.2f%%",
            processedTicks.get(),
            processedCandles.get(),
            processedFamilies.get(),
            errorCount.get(),
            uptimeSeconds,
            uptimeSeconds > 0 ? (double) processedTicks.get() / uptimeSeconds : 0.0,
            uptimeSeconds > 0 ? (double) processedCandles.get() / uptimeSeconds : 0.0,
            uptimeSeconds > 0 ? (double) processedFamilies.get() / uptimeSeconds : 0.0,
            processedTicks.get() > 0 ? (double) errorCount.get() / processedTicks.get() * 100 : 0.0
        );
    }

    /**
     * Reset metrics
     */
    public void reset() {
        processedTicks.set(0);
        processedCandles.set(0);
        processedFamilies.set(0);
        errorCount.set(0);
        startTime.set(System.currentTimeMillis());
        log.info("üîÑ Stream metrics reset");
    }

    /**
     * Get health status
     */
    public boolean isHealthy() {
        long totalProcessed = processedTicks.get() + processedCandles.get() + processedFamilies.get();
        long errors = errorCount.get();
        
        // System is healthy if error rate is below 5%
        return totalProcessed == 0 || (double) errors / totalProcessed < 0.05;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/MarketDataOrchestrator.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.config.KafkaConfig;
import com.kotsin.consumer.model.FamilyEnrichedData;
import com.kotsin.consumer.model.InstrumentCandle;
import com.kotsin.consumer.processor.InstrumentState;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.time.Duration;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Orchestrator for market data processing streams
 * 
 * SINGLE RESPONSIBILITY: Stream lifecycle management
 * EXTRACTED FROM: UnifiedMarketDataProcessor (God class refactoring)
 * 
 * This is the main orchestrator that coordinates all processing streams
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class MarketDataOrchestrator {

    private final KafkaConfig kafkaConfig;
    private final TopologyConfiguration topologyConfig;
    private final InstrumentProcessor instrumentProcessor;
    private final DataEnrichmentService enrichmentService;
    private final CandleEmissionService candleEmissionService;
    private final FamilyAggregationService familyAggService;

    private final Map<String, KafkaStreams> streamsInstances = new ConcurrentHashMap<>();
    
    @Value("${stream.outputs.candles.enabled:true}")
    private boolean candlesOutputEnabled;
    
    @Value("${stream.outputs.familyStructured.enabled:false}")
    private boolean familyStructuredEnabled;

    // Candle topic names
    @Value("${stream.outputs.candles.1m:candle-complete-1m}")
    private String candle1mTopic;
    
    @Value("${stream.outputs.candles.2m:candle-complete-2m}")
    private String candle2mTopic;
    
    @Value("${stream.outputs.candles.5m:candle-complete-5m}")
    private String candle5mTopic;
    
    @Value("${stream.outputs.candles.15m:candle-complete-15m}")
    private String candle15mTopic;
    
    @Value("${stream.outputs.candles.30m:candle-complete-30m}")
    private String candle30mTopic;

    /**
     * Start all processing streams
     */
    public void startAllStreams() {
        log.info("üöÄ Starting all market data processing streams");
        
        try {
            // Start per-instrument candle stream FIRST
            startInstrumentStream();
            
            // Wait for instrument stream to create candle topics
            log.info("‚è≥ Waiting for instrument stream to create candle topics...");
            Thread.sleep(5000); // 5 second delay
            
            // Start family-structured streams if enabled
            if (familyStructuredEnabled) {
                startFamilyStructuredStreamsWithRetry();
            }
            
            log.info("‚úÖ All streams started successfully");
            
        } catch (Exception e) {
            log.error("‚ùå Failed to start streams", e);
            throw new RuntimeException("Failed to start market data streams", e);
        }
    }

    /**
     * Start per-instrument candle generation stream
     */
    public void startInstrumentStream() {
        String instanceKey = "instrument-stream";
        
        if (streamsInstances.containsKey(instanceKey)) {
            log.warn("‚ö†Ô∏è Instrument stream already running. Skipping duplicate start.");
            return;
        }

        try {
            StreamsBuilder builder = topologyConfig.createInstrumentTopology();
            KafkaStreams streams = new KafkaStreams(builder.build(), kafkaConfig.getStreamProperties("instrument"));
            streamsInstances.put(instanceKey, streams);
            streams.start();
            log.info("‚úÖ Started per-instrument candle stream");
        } catch (Exception e) {
            log.error("‚ùå Failed to start instrument stream", e);
            throw e;
        }
    }

    /**
     * Start family-structured aggregation streams with retry mechanism
     */
    public void startFamilyStructuredStreamsWithRetry() {
        log.info("üèóÔ∏è Starting family-structured streams with retry");
        
        int maxRetries = 3;
        int retryDelay = 2000; // 2 seconds
        
        for (int attempt = 1; attempt <= maxRetries; attempt++) {
            try {
                startFamilyStructuredStreams();
                log.info("‚úÖ Family streams started successfully on attempt {}", attempt);
                return;
            } catch (Exception e) {
                log.warn("‚ö†Ô∏è Attempt {} failed to start family streams: {}", attempt, e.getMessage());
                if (attempt < maxRetries) {
                    log.info("‚è≥ Retrying in {} seconds...", retryDelay / 1000);
                    try {
                        Thread.sleep(retryDelay);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        throw new RuntimeException("Interrupted during retry", ie);
                    }
                } else {
                    log.error("‚ùå All {} attempts failed to start family streams", maxRetries);
                    throw new RuntimeException("Failed to start family streams after " + maxRetries + " attempts", e);
                }
            }
        }
    }

    /**
     * Start family-structured aggregation streams
     */
    public void startFamilyStructuredStreams() {
        log.info("üèóÔ∏è Starting family-structured streams");
        
        // Verify required candle topics exist before starting family streams
        String[] requiredTopics = {
            candle1mTopic, candle2mTopic, candle5mTopic, 
            candle15mTopic, candle30mTopic
        };
        
        for (String topic : requiredTopics) {
            if (!topicExists(topic)) {
                throw new RuntimeException("Required candle topic does not exist: " + topic);
            }
        }
        
        // Start 1-minute family stream
        startFamilyStream("1m", candle1mTopic, "family-structured-1m", Duration.ofMinutes(1));
        
        // Start 2-minute family stream  
        startFamilyStream("2m", candle2mTopic, "family-structured-2m", Duration.ofMinutes(2));
        
        // Start 5-minute family stream
        startFamilyStream("5m", candle5mTopic, "family-structured-5m", Duration.ofMinutes(5));
        
        // Start 15-minute family stream
        startFamilyStream("15m", candle15mTopic, "family-structured-15m", Duration.ofMinutes(15));
        
        // Start 30-minute family stream
        startFamilyStream("30m", candle30mTopic, "family-structured-30m", Duration.ofMinutes(30));
    }

    /**
     * Start a specific family-structured stream
     */
    private void startFamilyStream(String timeframeLabel, String sourceTopic, String sinkTopic, Duration windowSize) {
        String instanceKey = "family-structured-" + timeframeLabel;
        
        if (streamsInstances.containsKey(instanceKey)) {
            log.warn("‚ö†Ô∏è {} already running. Skipping duplicate start.", instanceKey);
            return;
        }

        try {
            StreamsBuilder builder = topologyConfig.createFamilyTopology(timeframeLabel, sourceTopic, sinkTopic, windowSize);
            KafkaStreams streams = new KafkaStreams(builder.build(), kafkaConfig.getStreamProperties("family-" + timeframeLabel));
            streamsInstances.put(instanceKey, streams);
            streams.start();
            log.info("‚úÖ Started {} stream ‚Üí topic: {}", instanceKey, sinkTopic);
        } catch (Exception e) {
            log.error("‚ùå Failed to start family stream {}", timeframeLabel, e);
            throw e;
        }
    }

    /**
     * Stop all streams gracefully
     */
    public void stopAllStreams() {
        log.info("üõë Stopping all market data processing streams");
        
        streamsInstances.forEach((key, streams) -> {
            try {
                streams.close();
                log.info("‚úÖ Stopped stream: {}", key);
            } catch (Exception e) {
                log.error("‚ùå Error stopping stream: {}", key, e);
            }
        });
        
        streamsInstances.clear();
        log.info("‚úÖ All streams stopped");
    }

    /**
     * Get stream status
     */
    public Map<String, String> getStreamStatus() {
        Map<String, String> status = new ConcurrentHashMap<>();
        
        streamsInstances.forEach((key, streams) -> {
            status.put(key, streams.state().toString());
        });
        
        return status;
    }

    /**
     * Get stream statistics
     */
    public String getStreamStats() {
        return String.format("Active streams: %d, Status: %s", 
            streamsInstances.size(), 
            getStreamStatus());
    }

    /**
     * Check if a Kafka topic exists
     */
    private boolean topicExists(String topicName) {
        try {
            // Use Kafka admin client to check topic existence
            Properties adminProps = new Properties();
            adminProps.put("bootstrap.servers", kafkaConfig.getBootstrapServers());
            adminProps.put("client.id", "topic-checker");
            
            try (org.apache.kafka.clients.admin.AdminClient adminClient = 
                 org.apache.kafka.clients.admin.AdminClient.create(adminProps)) {
                
                org.apache.kafka.clients.admin.ListTopicsResult result = adminClient.listTopics();
                Set<String> topics = result.names().get(5, java.util.concurrent.TimeUnit.SECONDS);
                return topics.contains(topicName);
            }
        } catch (Exception e) {
            log.warn("‚ö†Ô∏è Failed to check if topic {} exists: {}", topicName, e.getMessage());
            return false; // Assume it doesn't exist if we can't check
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/OrderbookDepthCalculator.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.model.OrderbookDepthData;
import lombok.extern.slf4j.Slf4j;

import java.util.*;

/**
 * Service for calculating orderbook depth metrics
 * Single Responsibility: Depth calculations (VWAP, slopes, imbalances)
 */
@Slf4j
public class OrderbookDepthCalculator {

    private static final int MAX_DEPTH_LEVELS = 10;

    /**
     * Build depth profile for one side of the book
     */
    public List<OrderbookDepthData.DepthLevel> buildDepthProfile(
        List<OrderBookSnapshot.OrderBookLevel> levels,
        String side,
        double midPrice
    ) {
        if (levels == null || levels.isEmpty()) {
            return new ArrayList<>();
        }

        List<OrderbookDepthData.DepthLevel> profile = new ArrayList<>();
        int totalQty = levels.stream().mapToInt(OrderBookSnapshot.OrderBookLevel::getQuantity).sum();

        for (int i = 0; i < Math.min(levels.size(), MAX_DEPTH_LEVELS); i++) {
            OrderBookSnapshot.OrderBookLevel level = levels.get(i);
            double distanceFromMid = midPrice > 0 ?
                Math.abs(level.getPrice() - midPrice) / midPrice * 10000 : 0.0;  // in bps

            profile.add(OrderbookDepthData.DepthLevel.builder()
                .level(i + 1)
                .price(level.getPrice())
                .quantity(level.getQuantity())
                .numberOfOrders(level.getNumberOfOrders())
                .distanceFromMid(distanceFromMid)
                .percentOfTotalDepth(totalQty > 0 ? (double) level.getQuantity() / totalQty * 100 : 0.0)
                .build());
        }

        return profile;
    }

    /**
     * Calculate cumulative depth
     */
    public List<Double> calculateCumulativeDepth(List<OrderbookDepthData.DepthLevel> profile) {
        List<Double> cumulative = new ArrayList<>();
        double sum = 0.0;

        for (OrderbookDepthData.DepthLevel level : profile) {
            sum += level.getQuantity();
            cumulative.add(sum);
        }

        return cumulative;
    }

    /**
     * Calculate VWAP of one side of the book
     */
    public double calculateSideVWAP(List<OrderbookDepthData.DepthLevel> profile) {
        if (profile.isEmpty()) return 0.0;

        double totalValue = 0.0;
        double totalQty = 0.0;

        for (OrderbookDepthData.DepthLevel level : profile) {
            totalValue += level.getPrice() * level.getQuantity();
            totalQty += level.getQuantity();
        }

        return totalQty > 0 ? totalValue / totalQty : 0.0;
    }

    /**
     * Calculate weighted depth imbalance (closer levels get higher weight)
     */
    public double calculateWeightedDepthImbalance(
        List<OrderbookDepthData.DepthLevel> bidProfile,
        List<OrderbookDepthData.DepthLevel> askProfile
    ) {
        double weightedBid = 0.0;
        double weightedAsk = 0.0;

        // Weight by inverse of distance from mid (closer levels get higher weight)
        for (OrderbookDepthData.DepthLevel level : bidProfile) {
            double weight = level.getDistanceFromMid() > 0 ?
                1.0 / (1.0 + level.getDistanceFromMid()) : 1.0;
            weightedBid += level.getQuantity() * weight;
        }

        for (OrderbookDepthData.DepthLevel level : askProfile) {
            double weight = level.getDistanceFromMid() > 0 ?
                1.0 / (1.0 + level.getDistanceFromMid()) : 1.0;
            weightedAsk += level.getQuantity() * weight;
        }

        double total = weightedBid + weightedAsk;
        return total > 0 ? (weightedBid - weightedAsk) / total : 0.0;
    }

    /**
     * Calculate imbalance for specific level range
     */
    public double calculateLevelImbalance(
        List<OrderbookDepthData.DepthLevel> bidProfile,
        List<OrderbookDepthData.DepthLevel> askProfile,
        int startLevel, int endLevel
    ) {
        double bidQty = bidProfile.stream()
            .filter(l -> l.getLevel() >= startLevel && l.getLevel() <= endLevel)
            .mapToInt(OrderbookDepthData.DepthLevel::getQuantity)
            .sum();

        double askQty = askProfile.stream()
            .filter(l -> l.getLevel() >= startLevel && l.getLevel() <= endLevel)
            .mapToInt(OrderbookDepthData.DepthLevel::getQuantity)
            .sum();

        double total = bidQty + askQty;
        return total > 0 ? (bidQty - askQty) / total : 0.0;
    }

    /**
     * Calculate slope using linear regression (quantity vs level)
     */
    public double calculateSlope(List<OrderbookDepthData.DepthLevel> profile) {
        if (profile.size() < 2) return 0.0;

        // Linear regression: quantity vs level
        double sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
        int n = profile.size();

        for (OrderbookDepthData.DepthLevel level : profile) {
            double x = level.getLevel();
            double y = level.getQuantity();
            sumX += x;
            sumY += y;
            sumXY += x * y;
            sumX2 += x * x;
        }

        // Slope = (n*Œ£xy - Œ£x*Œ£y) / (n*Œ£x¬≤ - (Œ£x)¬≤)
        double denominator = n * sumX2 - sumX * sumX;
        return denominator != 0 ? (n * sumXY - sumX * sumY) / denominator : 0.0;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/CircuitBreakerDetector.java
File type: .java
package com.kotsin.consumer.processor.service;

import lombok.Data;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.time.Duration;
import java.time.Instant;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Detector for market halts and circuit breakers
 * 
 * Indian Market Circuit Breaker Rules:
 * - 10% move -> 15-minute halt at index level
 * - 15% move -> Market closed for the day
 * - 20% move -> Market closed for the day
 * 
 * CRITICAL: Handles forced window closes when data stops flowing
 * 
 * @author System
 */
@Service
@Slf4j
@Data
public class CircuitBreakerDetector {
    
    // Timeout thresholds
    private static final Duration INACTIVITY_WARNING_THRESHOLD = Duration.ofMinutes(5);
    private static final Duration INACTIVITY_HALT_THRESHOLD = Duration.ofMinutes(20);
    
    // Track last tick time per instrument
    private final Map<String, Instant> lastTickTime = new ConcurrentHashMap<>();
    
    // Track circuit breaker events
    private final Map<String, CircuitBreakerEvent> circuitBreakerEvents = new ConcurrentHashMap<>();
    
    // Global market halt flag
    private volatile boolean globalMarketHalt = false;
    private volatile Instant globalHaltTime = null;
    
    /**
     * Circuit breaker event
     */
    @Data
    public static class CircuitBreakerEvent {
        String scripCode;
        Instant haltTime;
        String reason;  // INACTIVITY, CIRCUIT_BREAKER, MANUAL
        Duration inactivityDuration;
        boolean resolved;
    }
    
    /**
     * Update last tick time for instrument
     */
    public void recordTickActivity(String scripCode, Instant tickTime) {
        if (scripCode == null || tickTime == null) {
            return;
        }
        
        lastTickTime.put(scripCode, tickTime);
        
        // Check if this resolves a previous halt
        CircuitBreakerEvent event = circuitBreakerEvents.get(scripCode);
        if (event != null && !event.resolved) {
            event.resolved = true;
            log.info("‚úÖ Market activity resumed for {}: inactive for {}", 
                scripCode, event.inactivityDuration);
            circuitBreakerEvents.remove(scripCode);
        }
    }
    
    /**
     * Check for inactive instruments (scheduled every minute)
     */
    @Scheduled(fixedRate = 60000)  // Every 60 seconds
    public void detectInactiveInstruments() {
        Instant now = Instant.now();
        
        for (Map.Entry<String, Instant> entry : lastTickTime.entrySet()) {
            String scripCode = entry.getKey();
            Instant lastTick = entry.getValue();
            
            Duration inactivity = Duration.between(lastTick, now);
            
            // Warning level (5 minutes)
            if (inactivity.compareTo(INACTIVITY_WARNING_THRESHOLD) > 0 && 
                inactivity.compareTo(INACTIVITY_HALT_THRESHOLD) < 0) {
                
                log.warn("‚ö†Ô∏è Inactivity warning for {}: {} minutes since last tick", 
                    scripCode, inactivity.toMinutes());
            }
            
            // Halt level (20 minutes) - trigger circuit breaker
            if (inactivity.compareTo(INACTIVITY_HALT_THRESHOLD) > 0) {
                CircuitBreakerEvent existingEvent = circuitBreakerEvents.get(scripCode);
                
                if (existingEvent == null || existingEvent.resolved) {
                    // New halt detected
                    CircuitBreakerEvent event = new CircuitBreakerEvent();
                    event.scripCode = scripCode;
                    event.haltTime = lastTick;
                    event.reason = "INACTIVITY";
                    event.inactivityDuration = inactivity;
                    event.resolved = false;
                    
                    circuitBreakerEvents.put(scripCode, event);
                    
                    log.error("üõë CIRCUIT BREAKER TRIGGERED for {}: {} minutes of inactivity. " +
                        "Last tick: {}", scripCode, inactivity.toMinutes(), lastTick);
                    
                    // Check if this is a global market halt (multiple instruments affected)
                    checkForGlobalHalt(now);
                }
            }
        }
    }
    
    /**
     * Check if multiple instruments are halted (indicates global market halt)
     */
    private void checkForGlobalHalt(Instant now) {
        long activeHalts = circuitBreakerEvents.values().stream()
            .filter(e -> !e.resolved)
            .count();
        
        // If 10+ instruments are halted, consider it a global market halt
        if (activeHalts >= 10 && !globalMarketHalt) {
            globalMarketHalt = true;
            globalHaltTime = now;
            
            log.error("üö® GLOBAL MARKET HALT DETECTED: {} instruments affected. Time: {}", 
                activeHalts, now);
        }
        
        // Reset global halt if activity resumes
        if (activeHalts < 5 && globalMarketHalt) {
            globalMarketHalt = false;
            log.info("‚úÖ Global market activity resumed. Active halts: {}", activeHalts);
        }
    }
    
    /**
     * Check if instrument is currently halted
     */
    public boolean isHalted(String scripCode) {
        CircuitBreakerEvent event = circuitBreakerEvents.get(scripCode);
        return event != null && !event.resolved;
    }
    
    /**
     * Check if global market is halted
     */
    public boolean isGlobalMarketHalt() {
        return globalMarketHalt;
    }
    
    /**
     * Get halt duration for instrument
     */
    public Duration getHaltDuration(String scripCode) {
        CircuitBreakerEvent event = circuitBreakerEvents.get(scripCode);
        if (event == null || event.resolved) {
            return Duration.ZERO;
        }
        return Duration.between(event.haltTime, Instant.now());
    }
    
    /**
     * Force close windows for halted instrument
     * Should be called by window rotation service
     */
    public boolean shouldForceCloseWindows(String scripCode) {
        CircuitBreakerEvent event = circuitBreakerEvents.get(scripCode);
        if (event == null || event.resolved) {
            return false;
        }
        
        // Force close if halted for more than 15 minutes
        return event.inactivityDuration.compareTo(Duration.ofMinutes(15)) > 0;
    }
    
    /**
     * Get active circuit breaker events
     */
    public Map<String, CircuitBreakerEvent> getActiveEvents() {
        Map<String, CircuitBreakerEvent> active = new ConcurrentHashMap<>();
        for (Map.Entry<String, CircuitBreakerEvent> entry : circuitBreakerEvents.entrySet()) {
            if (!entry.getValue().resolved) {
                active.put(entry.getKey(), entry.getValue());
            }
        }
        return active;
    }
    
    /**
     * Manual circuit breaker trigger (for testing or manual intervention)
     */
    public void triggerManualHalt(String scripCode, String reason) {
        CircuitBreakerEvent event = new CircuitBreakerEvent();
        event.scripCode = scripCode;
        event.haltTime = Instant.now();
        event.reason = "MANUAL: " + reason;
        event.inactivityDuration = Duration.ZERO;
        event.resolved = false;
        
        circuitBreakerEvents.put(scripCode, event);
        
        log.warn("üõë Manual circuit breaker triggered for {}: {}", scripCode, reason);
    }
    
    /**
     * Resolve halt manually
     */
    public void resolveHalt(String scripCode) {
        CircuitBreakerEvent event = circuitBreakerEvents.get(scripCode);
        if (event != null) {
            event.resolved = true;
            log.info("‚úÖ Circuit breaker resolved for {}", scripCode);
            circuitBreakerEvents.remove(scripCode);
        }
    }
    
    /**
     * Clear all tracking data (for testing or restart)
     */
    public void clear() {
        lastTickTime.clear();
        circuitBreakerEvents.clear();
        globalMarketHalt = false;
        globalHaltTime = null;
    }
    
    /**
     * Get health status
     */
    public String getHealthStatus() {
        long totalInstruments = lastTickTime.size();
        long activeHalts = circuitBreakerEvents.values().stream()
            .filter(e -> !e.resolved)
            .count();
        
        return String.format("CircuitBreaker[instruments=%d, halts=%d, globalHalt=%s]",
            totalInstruments, activeHalts, globalMarketHalt);
    }
}



--------------------------------------------------
File End
--------------------------------------------------


processor/service/IcebergDetectionService.java
File type: .java
package com.kotsin.consumer.processor.service;

import lombok.extern.slf4j.Slf4j;

import java.util.ArrayList;
import java.util.List;

/**
 * Service for detecting iceberg orders in the orderbook
 * Single Responsibility: Iceberg order detection logic
 */
@Slf4j
public class IcebergDetectionService {

    private static final int HISTORY_SIZE = 20;
    private static final double CV_THRESHOLD = 0.1;
    private static final int MIN_SIZE_THRESHOLD = 1000;

    private final List<Integer> recentBidQuantities = new ArrayList<>();
    private final List<Integer> recentAskQuantities = new ArrayList<>();

    public void trackBidQuantity(Integer quantity) {
        recentBidQuantities.add(quantity);
        if (recentBidQuantities.size() > HISTORY_SIZE) {
            recentBidQuantities.remove(0);
        }
    }

    public void trackAskQuantity(Integer quantity) {
        recentAskQuantities.add(quantity);
        if (recentAskQuantities.size() > HISTORY_SIZE) {
            recentAskQuantities.remove(0);
        }
    }

    public Boolean detectIcebergBid() {
        return detectIceberg(recentBidQuantities);
    }

    public Boolean detectIcebergAsk() {
        return detectIceberg(recentAskQuantities);
    }

    public Double calculateIcebergProbabilityBid() {
        return calculateIcebergProbability(recentBidQuantities);
    }

    public Double calculateIcebergProbabilityAsk() {
        return calculateIcebergProbability(recentAskQuantities);
    }

    private Boolean detectIceberg(List<Integer> recentQuantities) {
        if (recentQuantities.size() < 10) return null;

        // Iceberg: unusually consistent quantities (low variance)
        double mean = recentQuantities.stream().mapToInt(Integer::intValue).average().orElse(0.0);
        double variance = recentQuantities.stream()
            .mapToDouble(q -> Math.pow(q - mean, 2))
            .average().orElse(0.0);

        double stdDev = Math.sqrt(variance);
        double cv = mean > 0 ? stdDev / mean : 0.0;  // Coefficient of variation

        // Low CV (< 0.1) suggests iceberg (too uniform)
        return cv < CV_THRESHOLD && mean > MIN_SIZE_THRESHOLD;
    }

    private Double calculateIcebergProbability(List<Integer> recentQuantities) {
        if (recentQuantities.size() < 10) return null;

        double mean = recentQuantities.stream().mapToInt(Integer::intValue).average().orElse(0.0);
        double variance = recentQuantities.stream()
            .mapToDouble(q -> Math.pow(q - mean, 2))
            .average().orElse(0.0);

        double stdDev = Math.sqrt(variance);
        double cv = mean > 0 ? stdDev / mean : 1.0;

        // Convert CV to probability (lower CV = higher probability)
        // CV of 0 ‚Üí prob 1.0, CV of 0.5+ ‚Üí prob 0
        return Math.max(0.0, Math.min(1.0, (0.5 - cv) * 2));
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/TradingHoursValidationService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.TickData;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.time.Instant;
import java.time.ZoneId;
import java.time.ZonedDateTime;

/**
 * Service for validating trading hours
 * Single Responsibility: Trading hours validation logic
 *
 * Spring Best Practice: @Service annotation for business logic
 */
@Service
@Slf4j
public class TradingHoursValidationService {

    private static final ZoneId IST_ZONE = ZoneId.of("Asia/Kolkata");

    /**
     * Check if tick is within trading hours for its exchange
     */
    public boolean withinTradingHours(TickData tick) {
        try {
            long ts = tick.getTimestamp();
            if (ts <= 0) {
                log.warn("‚ö†Ô∏è Invalid timestamp (<=0) for token {}", tick.getToken());
                return false;
            }

            ZonedDateTime zdt = ZonedDateTime.ofInstant(Instant.ofEpochMilli(ts), IST_ZONE);
            String exch = tick.getExchange();

            if ("N".equalsIgnoreCase(exch)) {
                // NSE: 9:15 AM - 3:30 PM
                return !zdt.toLocalTime().isBefore(java.time.LocalTime.of(9, 15)) &&
                       !zdt.toLocalTime().isAfter(java.time.LocalTime.of(15, 30));
            } else if ("M".equalsIgnoreCase(exch)) {
                // MCX: 9:00 AM - 11:30 PM
                return !zdt.toLocalTime().isBefore(java.time.LocalTime.of(9, 0)) &&
                       !zdt.toLocalTime().isAfter(java.time.LocalTime.of(23, 30));
            } else {
                // Unknown exchange -> drop
                log.debug("Unknown exchange '{}' for token {}, dropping", exch, tick.getToken());
                return false;
            }
        } catch (Exception e) {
            log.warn("‚ö†Ô∏è Invalid timestamp for token {}: {}", tick.getToken(), e.toString());
            return false;
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/DataEnrichmentService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.InstrumentCandle;
import com.kotsin.consumer.model.OpenInterest;
import com.kotsin.consumer.model.OrderBookSnapshot;
import com.kotsin.consumer.processor.InstrumentState;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

/**
 * Service for enriching instrument data with external sources
 * 
 * SINGLE RESPONSIBILITY: Data enrichment logic
 * EXTRACTED FROM: UnifiedMarketDataProcessor (God class refactoring)
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class DataEnrichmentService {

    /**
     * Enrich instrument state with orderbook data
     */
    public void enrichWithOrderbook(InstrumentState state, OrderBookSnapshot orderbook) {
        if (state == null || orderbook == null) {
            return;
        }

        if (orderbook.isValid()) {
            state.addOrderbook(orderbook);
            log.debug("üìä Enriched state {} with orderbook data", state.getScripCode());
        } else {
            log.warn("‚ö†Ô∏è Invalid orderbook data for {}", state.getScripCode());
        }
    }

    /**
     * Enrich candle with Open Interest data
     */
    public void enrichWithOpenInterest(InstrumentCandle candle, OpenInterest oi) {
        if (candle == null || oi == null) {
            return;
        }

        candle.setOpenInterest(oi.getOpenInterest());
        candle.setOiChange(oi.getOiChange());
        
        log.debug("üìä Enriched candle {} with OI: {} (change: {})", 
            candle.getScripCode(), oi.getOpenInterest(), oi.getOiChange());
    }

    /**
     * Validate enrichment data quality
     */
    public boolean isEnrichmentDataValid(OrderBookSnapshot orderbook) {
        if (orderbook == null) {
            return false;
        }

        // Check if orderbook has valid data
        return orderbook.isValid() && 
               orderbook.getBids() != null && !orderbook.getBids().isEmpty() &&
               orderbook.getAsks() != null && !orderbook.getAsks().isEmpty();
    }

    /**
     * Get enrichment statistics
     */
    public String getEnrichmentStats(InstrumentState state) {
        if (state == null) {
            return "No state";
        }

        boolean hasOrderbook = state.getLatestOrderbook() != null;
        boolean hasValidOrderbook = hasOrderbook && state.getLatestOrderbook().isValid();
        
        return String.format("Orderbook: %s (valid: %s)", 
            hasOrderbook ? "Yes" : "No",
            hasValidOrderbook ? "Yes" : "No");
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/BackpressureHandler.java
File type: .java
package com.kotsin.consumer.processor.service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * Backpressure handler for Kafka Streams processing
 * 
 * CRITICAL FIX: Implements flow control to prevent memory overflow
 * Monitors processing lag and applies adaptive throttling
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class BackpressureHandler {

    @Value("${kafka.streams.backpressure.enabled:true}")
    private boolean backpressureEnabled;
    
    @Value("${kafka.streams.backpressure.max.poll.records:100}")
    private int maxPollRecords;
    
    @Value("${kafka.streams.backpressure.lag.threshold:1000}")
    private long lagThreshold;
    
    @Value("${kafka.streams.backpressure.throttle.factor:0.5}")
    private double throttleFactor;

    private final AtomicLong processedRecords = new AtomicLong(0);
    private final AtomicLong lagRecords = new AtomicLong(0);
    private final AtomicBoolean throttlingActive = new AtomicBoolean(false);
    private final AtomicLong lastThrottleTime = new AtomicLong(0);

    /**
     * Check if backpressure should be applied
     */
    public boolean shouldApplyBackpressure() {
        if (!backpressureEnabled) {
            return false;
        }

        long currentLag = lagRecords.get();
        long currentProcessed = processedRecords.get();
        
        // Calculate lag percentage
        double lagPercentage = currentProcessed > 0 ? (double) currentLag / currentProcessed : 0;
        
        // Apply backpressure if lag exceeds threshold
        boolean shouldThrottle = currentLag > lagThreshold || lagPercentage > 0.1; // 10% lag threshold
        
        if (shouldThrottle && !throttlingActive.get()) {
            log.warn("üö® Backpressure triggered: lag={}, processed={}, percentage={:.2f}%", 
                currentLag, currentProcessed, lagPercentage * 100);
            throttlingActive.set(true);
            lastThrottleTime.set(System.currentTimeMillis());
        } else if (!shouldThrottle && throttlingActive.get()) {
            long throttleDuration = System.currentTimeMillis() - lastThrottleTime.get();
            log.info("‚úÖ Backpressure released after {}ms: lag={}, processed={}", 
                throttleDuration, currentLag, currentProcessed);
            throttlingActive.set(false);
        }
        
        return shouldThrottle;
    }

    /**
     * Get adaptive poll records based on current lag
     */
    public int getAdaptivePollRecords() {
        if (!backpressureEnabled) {
            return maxPollRecords;
        }

        if (shouldApplyBackpressure()) {
            // Reduce poll records when under pressure
            int adaptiveRecords = (int) (maxPollRecords * throttleFactor);
            return Math.max(adaptiveRecords, 1); // Minimum 1 record
        }
        
        return maxPollRecords;
    }

    /**
     * Record processed record
     */
    public void recordProcessed() {
        processedRecords.incrementAndGet();
    }

    /**
     * Record lag
     */
    public void recordLag(long lag) {
        lagRecords.set(lag);
    }

    /**
     * Get backpressure statistics
     */
    public String getBackpressureStats() {
        return String.format("Backpressure: %s, Processed: %d, Lag: %d, Throttling: %s", 
            backpressureEnabled ? "Enabled" : "Disabled",
            processedRecords.get(),
            lagRecords.get(),
            throttlingActive.get() ? "Active" : "Inactive");
    }

    /**
     * Reset statistics
     */
    public void resetStats() {
        processedRecords.set(0);
        lagRecords.set(0);
        throttlingActive.set(false);
        log.info("üîÑ Backpressure statistics reset");
    }

    /**
     * Check if system is healthy (not under excessive pressure)
     */
    public boolean isHealthy() {
        if (!backpressureEnabled) {
            return true;
        }

        long currentLag = lagRecords.get();
        long currentProcessed = processedRecords.get();
        
        // System is healthy if lag is below threshold and not throttling
        return currentLag <= lagThreshold && !throttlingActive.get();
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/CandleEmissionService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.metrics.StreamMetrics;
import com.kotsin.consumer.model.InstrumentCandle;
import com.kotsin.consumer.processor.InstrumentState;
import com.kotsin.consumer.monitoring.Timeframe;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;
import org.springframework.stereotype.Service;

/**
 * Service for emitting per-instrument candles to timeframe-specific topics
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class CandleEmissionService {

    private final StreamMetrics metrics;

    /**
     * Emit per-instrument candles to timeframe-specific topics
     */
    public void emitPerInstrumentCandles(KStream<String, InstrumentState> stateStream, CandleTopicResolver topicResolver) {
        // Filter for states with complete windows
        KStream<String, InstrumentState> completeStates = stateStream
            .peek((key, state) -> {
                boolean hasComplete = state.hasAnyCompleteWindow();
                if (!hasComplete) {
                    log.debug("‚è≠Ô∏è Skipping state with no complete windows: scripCode={}", state.getScripCode());
                } else {
                    log.info("‚úÖ State has complete windows: scripCode={}, ready for emission", state.getScripCode());
                }
            })
            .filter((key, state) -> state.hasAnyCompleteWindow());

        for (Timeframe timeframe : new Timeframe[]{Timeframe.ONE_MIN, Timeframe.TWO_MIN, Timeframe.THREE_MIN,
                                                     Timeframe.FIVE_MIN, Timeframe.FIFTEEN_MIN, Timeframe.THIRTY_MIN}) {
            final String tfLabel = timeframe.getLabel();
            KStream<String, InstrumentCandle> built = completeStates
                .mapValues((readOnlyKey, state) -> state.extractFinalizedCandle(timeframe))
                .peek((k, c) -> log.debug("built candle tf={} scrip={} vol={} valid={}", tfLabel,
                    c != null ? c.getScripCode() : null,
                    c != null ? c.getVolume() : null,
                    c != null && c.isValid()));

            KStream<String, InstrumentCandle>[] branches = built.branch(
                (k, c) -> c != null && c.isValid(),
                (k, c) -> true
            );
            KStream<String, InstrumentCandle> valid = branches[0];
            KStream<String, InstrumentCandle> invalid = branches[1];

            invalid.peek((k, c) -> {
                log.warn("drop candle tf={} scrip={} reason={} vol={} open={} high={} low={} close={}", tfLabel,
                    c != null ? c.getScripCode() : null,
                    c == null ? "null" : (c.getVolume() == null || c.getVolume() <= 0 ? "volume" : "fields"),
                    c != null ? c.getVolume() : null,
                    c != null ? c.getOpen() : null,
                    c != null ? c.getHigh() : null,
                    c != null ? c.getLow() : null,
                    c != null ? c.getClose() : null);
                metrics.incCandleDrop(tfLabel);
            });

            String topic = topicResolver.getCandleTopicForTimeframe(tfLabel);
            if (topic != null) {
                valid
                    .peek((k, c) -> { 
                        log.info("üì§ candle emit tf={} scrip={} vol={} ‚Üí {}", tfLabel, c.getScripCode(), c.getVolume(), topic); 
                        metrics.incCandleEmit(tfLabel); 
                    })
                    .to(topic, Produced.with(
                        Serdes.String(),
                        InstrumentCandle.serde()
                    ));
            }
        }
    }

    /**
     * Interface for resolving candle topic names
     */
    public interface CandleTopicResolver {
        String getCandleTopicForTimeframe(String timeframe);
    }
}



--------------------------------------------------
File End
--------------------------------------------------


processor/service/TimeframeStateManager.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.*;
import com.kotsin.consumer.processor.CandleAccumulator;
import com.kotsin.consumer.processor.OiAccumulator;
import com.kotsin.consumer.processor.ImbalanceBarAccumulator;
import com.kotsin.consumer.processor.MicrostructureAccumulator;
import com.kotsin.consumer.processor.OrderbookDepthAccumulator;
import com.kotsin.consumer.monitoring.Timeframe;
import com.kotsin.consumer.processor.WindowRotationService;
import lombok.Data;
import lombok.extern.slf4j.Slf4j;

import java.util.*;
import java.util.stream.Collectors;

/**
 * Manages multi-timeframe state aggregation
 * Single Responsibility: Coordinate aggregation across all timeframes and features
 */
@Data
@Slf4j
public class TimeframeStateManager {

    // Candle accumulators for each timeframe
    private final EnumMap<Timeframe, CandleAccumulator> candleAccumulators = new EnumMap<>(Timeframe.class);

    // OI accumulators for each timeframe
    private final EnumMap<Timeframe, OiAccumulator> oiAccumulators = new EnumMap<>(Timeframe.class);

    // Feature accumulators (delegated to specialized services)
    private final ImbalanceBarAccumulator imbalanceBarAccumulator = new ImbalanceBarAccumulator();
    private final MicrostructureAccumulator microstructureAccumulator = new MicrostructureAccumulator();
    private final OrderbookDepthAccumulator orderbookDepthAccumulator = new OrderbookDepthAccumulator();

    // Basic info
    private String scripCode;
    private String companyName;
    private String exchange;
    private String exchangeType;
    private Long firstTickTime;
    private Long lastTickTime;
    private Long messageCount = 0L;

    // Timeframe definitions
    private static final Timeframe[] TIMEFRAMES = {
        Timeframe.ONE_MIN, Timeframe.TWO_MIN, Timeframe.THREE_MIN,
        Timeframe.FIVE_MIN, Timeframe.FIFTEEN_MIN, Timeframe.THIRTY_MIN
    };

    public TimeframeStateManager() {
        initializeAccumulators();
    }

    private void initializeAccumulators() {
        // Initialize candle accumulators
        for (Timeframe timeframe : TIMEFRAMES) {
            candleAccumulators.put(timeframe, new CandleAccumulator());
            oiAccumulators.put(timeframe, new OiAccumulator());
        }
    }

    public void addTick(TickData tick) {
        if (scripCode == null) {
            scripCode = tick.getScripCode();
            companyName = tick.getCompanyName();
            exchange = tick.getExchange();
            exchangeType = tick.getExchangeType();
            firstTickTime = tick.getTimestamp();
        }

        lastTickTime = tick.getTimestamp();
        messageCount++;

        // Update all timeframes
        updateAllTimeframes(tick);

        // Update OI if available
        if (tick.getOpenInterest() != null) {
            updateAllOiTimeframes(tick);
        }

        // Update imbalance bars
        imbalanceBarAccumulator.addTick(tick);

        // Update microstructure
        microstructureAccumulator.addTick(tick);

        // Update orderbook depth analytics
        if (tick.getFullOrderbook() != null) {
            orderbookDepthAccumulator.addOrderbook(tick.getFullOrderbook());
        }
    }

    private void updateAllTimeframes(TickData tick) {
        long tickTime = tick.getTimestamp();
        boolean isNse = "N".equalsIgnoreCase(exchange);

        for (Map.Entry<Timeframe, CandleAccumulator> entry : candleAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            CandleAccumulator acc = entry.getValue();
            int minutes = timeframe.getMinutes();

            // Use offset alignment for 30m on NSE to start from 09:15 (offset 15 min)
            if (isNse && timeframe == Timeframe.THIRTY_MIN) {
                acc = WindowRotationService.rotateCandleIfNeeded(acc, tickTime, minutes, 15);
            } else {
                acc = WindowRotationService.rotateCandleIfNeeded(acc, tickTime, minutes);
            }
            candleAccumulators.put(timeframe, acc);
            acc.addTick(tick);
        }
    }

    private void updateAllOiTimeframes(TickData tick) {
        boolean isNse = "N".equalsIgnoreCase(exchange);
        for (Map.Entry<Timeframe, OiAccumulator> entry : oiAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            OiAccumulator acc = entry.getValue();
            int minutes = timeframe.getMinutes();

            if (isNse && timeframe == Timeframe.THIRTY_MIN) {
                acc = WindowRotationService.rotateOiIfNeeded(acc, tick.getTimestamp(), minutes, 15);
            } else {
                acc = WindowRotationService.rotateOiIfNeeded(acc, tick.getTimestamp(), minutes);
            }
            oiAccumulators.put(timeframe, acc);
            acc.addOiData(tick);
        }
    }

    public boolean hasAnyCompleteWindow() {
        return candleAccumulators.values().stream()
            .anyMatch(CandleAccumulator::isComplete);
    }

    public Set<String> getCompleteWindows() {
        return candleAccumulators.entrySet().stream()
            .filter(entry -> entry.getValue().isComplete())
            .map(e -> e.getKey().getLabel())
            .collect(Collectors.toSet());
    }

    public Map<String, CandleData> getMultiTimeframeCandles() {
        Map<String, CandleData> candles = new HashMap<>();

        for (Map.Entry<Timeframe, CandleAccumulator> entry : candleAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            CandleAccumulator accumulator = entry.getValue();

            candles.put(timeframe.getLabel(), accumulator.toCandleData(exchange, exchangeType));
        }

        return candles;
    }

    public Map<String, OpenInterestTimeframeData> getOpenInterest() {
        Map<String, OpenInterestTimeframeData> oiData = new HashMap<>();

        for (Map.Entry<Timeframe, OiAccumulator> entry : oiAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            OiAccumulator accumulator = entry.getValue();

            oiData.put(timeframe.getLabel(), accumulator.toOiTimeframeData());
        }

        return oiData;
    }

    public ImbalanceBarData getImbalanceBars() {
        return imbalanceBarAccumulator.toImbalanceBarData();
    }

    public MicrostructureData getMicrostructure() {
        // Note: Called without timeframe context, so window times are null
        // For window-aligned microstructure, use extractFinalizedCandle() instead
        return microstructureAccumulator.toMicrostructureData(null, null);
    }

    public OrderbookDepthData getOrderbookDepth() {
        return orderbookDepthAccumulator.toOrderbookDepthData();
    }

    public String getDataQuality() {
        if (messageCount < 10) return "LOW";
        if (messageCount < 50) return "MEDIUM";
        return "HIGH";
    }

    public long getProcessingLatency() {
        return System.currentTimeMillis() - lastTickTime;
    }

    public EnumMap<Timeframe, CandleAccumulator> getCandleAccumulators() {
        return candleAccumulators;
    }

    /**
     * Force completion of all windows for finalized candle emission
     * Uses the Kafka Streams window end time as the reference point
     * 
     * When a Kafka 1m window closes (e.g., at 09:35:00), we check if any inner
     * multi-timeframe windows (1m, 2m, 3m, etc.) should also be marked complete.
     * 
     * @param kafkaWindowEnd The end time of the Kafka 1m tumbling window
     */
    public void forceCompleteWindows(long kafkaWindowEnd) {
        int completedCount = 0;
        
        for (Map.Entry<Timeframe, CandleAccumulator> entry : candleAccumulators.entrySet()) {
            Timeframe timeframe = entry.getKey();
            CandleAccumulator accumulator = entry.getValue();
            
            // Mark window as complete if Kafka window end time is >= accumulator's window end
            // This means the accumulator's window has fully elapsed
            if (accumulator.getWindowStart() != null && 
                accumulator.getWindowEnd() != null && 
                !accumulator.isComplete() &&
                kafkaWindowEnd >= accumulator.getWindowEnd()) {
                
                accumulator.markComplete();
                completedCount++;
                log.info("‚úÖ Completed {} window [{}‚Üí{}] at Kafka window {} (scripCode={})", 
                    timeframe.getLabel(), 
                    accumulator.getWindowStart(), 
                    accumulator.getWindowEnd(), 
                    kafkaWindowEnd,
                    scripCode);
            }
        }
        
        if (completedCount > 0) {
            log.info("üéâ Marked {} windows as complete for scripCode={} at Kafka window end {}", 
                completedCount, scripCode, kafkaWindowEnd);
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


processor/service/DynamicTradingHoursService.java
File type: .java
package com.kotsin.consumer.processor.service;

import com.kotsin.consumer.model.TickData;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.time.Duration;
import java.time.LocalTime;
import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Dynamic trading hours service with exchange-specific configuration
 * 
 * CRITICAL FIX: Replaces hardcoded trading hours with dynamic configuration
 * Supports multiple exchanges (NSE, MCX, etc.) with different trading hours
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class DynamicTradingHoursService {

    @Value("${trading.hours.nse.start:09:15}")
    private String nseStartTime;
    
    @Value("${trading.hours.nse.end:15:30}")
    private String nseEndTime;
    
    @Value("${trading.hours.mcx.start:09:00}")
    private String mcxStartTime;
    
    @Value("${trading.hours.mcx.end:23:30}")
    private String mcxEndTime;
    
    @Value("${trading.hours.buffer.minutes:15}")
    private int bufferMinutes;

    private final Map<String, TradingHours> exchangeHours = new ConcurrentHashMap<>();
    private final ZoneId istZone = ZoneId.of("Asia/Kolkata");

    /**
     * Check if tick is within trading hours for its exchange
     */
    public boolean withinTradingHours(TickData tick) {
        if (tick == null || tick.getTimestamp() == 0L) {
            return false;
        }

        try {
            ZonedDateTime tickTime = ZonedDateTime.ofInstant(
                java.time.Instant.ofEpochMilli(tick.getTimestamp()),
                istZone
            );

            // Check weekend
            if (isWeekend(tickTime)) {
                log.debug("‚è∞ Weekend detected for tick at {}", tickTime);
                return false;
            }

            // Get exchange-specific hours
            String exchange = getExchangeFromTick(tick);
            TradingHours hours = getTradingHours(exchange);
            
            if (hours == null) {
                log.warn("‚ö†Ô∏è No trading hours configured for exchange: {}", exchange);
                return true; // Default to allow if not configured
            }

            // Check if within trading hours (with buffer)
            boolean withinHours = hours.isWithinTradingHours(tickTime, bufferMinutes);
            
            if (!withinHours) {
                log.debug("‚è∞ Tick outside trading hours: {} (exchange: {}, hours: {})", 
                    tickTime.toLocalTime(), exchange, hours);
            }
            
            return withinHours;

        } catch (Exception e) {
            log.error("‚ùå Error checking trading hours for tick: {}", tick, e);
            return true; // Default to allow on error
        }
    }

    /**
     * Get trading hours for a specific exchange
     */
    public TradingHours getTradingHours(String exchange) {
        return exchangeHours.computeIfAbsent(exchange, this::loadTradingHours);
    }

    /**
     * Load trading hours for an exchange
     */
    private TradingHours loadTradingHours(String exchange) {
        switch (exchange.toUpperCase()) {
            case "NSE":
                return new TradingHours(
                    LocalTime.parse(nseStartTime),
                    LocalTime.parse(nseEndTime),
                    "NSE"
                );
            case "MCX":
                return new TradingHours(
                    LocalTime.parse(mcxStartTime),
                    LocalTime.parse(mcxEndTime),
                    "MCX"
                );
            default:
                log.warn("‚ö†Ô∏è Unknown exchange: {}, using default NSE hours", exchange);
                return new TradingHours(
                    LocalTime.parse(nseStartTime),
                    LocalTime.parse(nseEndTime),
                    "DEFAULT"
                );
        }
    }

    /**
     * Extract exchange from tick data
     */
    private String getExchangeFromTick(TickData tick) {
        // Try to extract from exchange field
        if (tick.getExchange() != null && !tick.getExchange().isEmpty()) {
            return tick.getExchange();
        }
        
        // Fallback: determine from scripCode patterns
        String scripCode = tick.getScripCode();
        if (scripCode != null) {
            if (scripCode.startsWith("99992")) {
                return "NSE"; // Index
            } else if (scripCode.length() == 8 && scripCode.matches("\\d+")) {
                return "NSE"; // NSE equity/derivative
            } else {
                return "MCX"; // Default to MCX for commodities
            }
        }
        
        return "NSE"; // Default
    }

    /**
     * Check if date is weekend
     */
    private boolean isWeekend(ZonedDateTime dateTime) {
        int dayOfWeek = dateTime.getDayOfWeek().getValue();
        return dayOfWeek == 6 || dayOfWeek == 7; // Saturday or Sunday
    }

    /**
     * Get trading hours statistics
     */
    public String getTradingHoursStats() {
        return String.format("Configured exchanges: %d, Buffer: %d minutes", 
            exchangeHours.size(), bufferMinutes);
    }

    /**
     * Trading hours configuration for an exchange
     */
    public static class TradingHours {
        private final LocalTime startTime;
        private final LocalTime endTime;
        private final String exchange;

        public TradingHours(LocalTime startTime, LocalTime endTime, String exchange) {
            this.startTime = startTime;
            this.endTime = endTime;
            this.exchange = exchange;
        }

        public boolean isWithinTradingHours(ZonedDateTime dateTime, int bufferMinutes) {
            LocalTime time = dateTime.toLocalTime();
            LocalTime startWithBuffer = startTime.minusMinutes(bufferMinutes);
            LocalTime endWithBuffer = endTime.plusMinutes(bufferMinutes);
            
            return !time.isBefore(startWithBuffer) && !time.isAfter(endWithBuffer);
        }

        public LocalTime getStartTime() { return startTime; }
        public LocalTime getEndTime() { return endTime; }
        public String getExchange() { return exchange; }

        @Override
        public String toString() {
            return String.format("%s: %s-%s", exchange, startTime, endTime);
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


controller/HealthController.java
File type: .java
package com.kotsin.consumer.controller;

import com.kotsin.consumer.monitoring.SystemMonitor;
import com.kotsin.consumer.processor.UnifiedMarketDataProcessor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.util.HashMap;
import java.util.Map;

/**
 * Health check and metrics endpoint
 * 
 * OBSERVABILITY: Expose health and metrics for monitoring
 * KUBERNETES READY: Provides liveness and readiness probes
 */
@RestController
@RequestMapping("/api/v1/health")
@RequiredArgsConstructor
@Slf4j
public class HealthController {

    private final SystemMonitor systemMonitor;
    private final UnifiedMarketDataProcessor processor;

    /**
     * Liveness probe - Is the application running?
     */
    @GetMapping("/live")
    public ResponseEntity<Map<String, Object>> liveness() {
        Map<String, Object> response = new HashMap<>();
        response.put("status", "UP");
        response.put("timestamp", System.currentTimeMillis());
        return ResponseEntity.ok(response);
    }

    /**
     * Readiness probe - Is the application ready to accept traffic?
     */
    @GetMapping("/ready")
    public ResponseEntity<Map<String, Object>> readiness() {
        Map<String, Object> response = new HashMap<>();
        
        boolean isReady = systemMonitor.isSystemHealthy() && processor.isHealthy();
        
        response.put("status", isReady ? "UP" : "DOWN");
        response.put("timestamp", System.currentTimeMillis());
        response.put("healthy", isReady);
        
        if (!isReady) {
            return ResponseEntity.status(503).body(response);
        }
        
        return ResponseEntity.ok(response);
    }

    /**
     * Detailed health check
     */
    @GetMapping
    public ResponseEntity<Map<String, Object>> health() {
        Map<String, Object> response = new HashMap<>();
        
        boolean isHealthy = systemMonitor.isSystemHealthy();
        
        response.put("status", isHealthy ? "HEALTHY" : "UNHEALTHY");
        response.put("timestamp", System.currentTimeMillis());
        response.put("systemMetrics", systemMonitor.getSystemMetrics());
        response.put("streamStates", processor.getStreamStates());
        response.put("streamStats", processor.getStreamStats());
        
        if (!isHealthy) {
            return ResponseEntity.status(503).body(response);
        }
        
        return ResponseEntity.ok(response);
    }

    /**
     * Metrics endpoint for Prometheus
     */
    @GetMapping("/metrics")
    public ResponseEntity<Map<String, Object>> metrics() {
        Map<String, Object> metrics = systemMonitor.getSystemMetrics();
        return ResponseEntity.ok(metrics);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


controller/CacheHealthController.java
File type: .java
package com.kotsin.consumer.controller;

import com.kotsin.consumer.model.InstrumentFamily;
import com.kotsin.consumer.service.MongoInstrumentFamilyService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.HashMap;
import java.util.Map;

/**
 * Health check and management endpoints for instrument family cache
 */
@RestController
@RequestMapping("/api/cache")
@RequiredArgsConstructor
@Slf4j
public class CacheHealthController {
    
    private final MongoInstrumentFamilyService cacheService;
    
    /**
     * Get cache health status
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> getCacheHealth() {
        try {
            Map<String, Object> health = new HashMap<>();
            Map<String, Object> stats = cacheService.getCacheStats();
            
            health.put("status", "UP");
            health.put("timestamp", System.currentTimeMillis());
            health.putAll(stats);
            
            return ResponseEntity.ok(health);
        } catch (Exception e) {
            log.error("‚ùå Health check failed", e);
            Map<String, Object> error = new HashMap<>();
            error.put("status", "DOWN");
            error.put("error", e.getMessage());
            error.put("timestamp", System.currentTimeMillis());
            return ResponseEntity.status(500).body(error);
        }
    }
    
    /**
     * Get specific instrument family
     */
    @GetMapping("/family/{scripCode}")
    public ResponseEntity<InstrumentFamily> getFamily(@PathVariable String scripCode) {
        try {
            InstrumentFamily family = cacheService.getFamily(scripCode);
            if (family != null) {
                return ResponseEntity.ok(family);
            } else {
                return ResponseEntity.notFound().build();
            }
        } catch (Exception e) {
            log.error("‚ùå Failed to get family for scripCode: {}", scripCode, e);
            return ResponseEntity.status(500).build();
        }
    }
    
    /**
     * Get cache statistics
     */
    @GetMapping("/stats")
    public ResponseEntity<Map<String, Object>> getCacheStats() {
        try {
            Map<String, Object> stats = cacheService.getCacheStats();
            return ResponseEntity.ok(stats);
        } catch (Exception e) {
            log.error("‚ùå Failed to get cache stats", e);
            return ResponseEntity.status(500).build();
        }
    }
    
    /**
     * Manual cache refresh
     */
    @PostMapping("/refresh")
    public ResponseEntity<Map<String, Object>> refreshCache() {
        try {
            log.info("üîÑ Manual cache refresh triggered");
            cacheService.refreshCache();
            
            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "Cache refresh completed successfully");
            response.put("timestamp", System.currentTimeMillis());
            response.put("cacheSize", cacheService.getCacheSize());
            
            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("‚ùå Manual cache refresh failed", e);
            
            Map<String, Object> error = new HashMap<>();
            error.put("success", false);
            error.put("message", "Cache refresh failed: " + e.getMessage());
            error.put("timestamp", System.currentTimeMillis());
            
            return ResponseEntity.status(500).body(error);
        }
    }
    
    /**
     * Clear cache
     */
    @PostMapping("/clear")
    public ResponseEntity<Map<String, Object>> clearCache() {
        try {
            log.info("üóëÔ∏è Manual cache clear triggered");
            cacheService.clearCache();
            
            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "Cache cleared successfully");
            response.put("timestamp", System.currentTimeMillis());
            
            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("‚ùå Manual cache clear failed", e);
            
            Map<String, Object> error = new HashMap<>();
            error.put("success", false);
            error.put("message", "Cache clear failed: " + e.getMessage());
            error.put("timestamp", System.currentTimeMillis());
            
            return ResponseEntity.status(500).body(error);
        }
    }
    
    /**
     * Get sample families (for testing)
     */
    @GetMapping("/sample")
    public ResponseEntity<Map<String, Object>> getSampleFamilies(@RequestParam(defaultValue = "5") int limit) {
        try {
            Map<String, InstrumentFamily> allFamilies = cacheService.getAllFamilies();
            
            Map<String, Object> response = new HashMap<>();
            response.put("totalFamilies", allFamilies.size());
            response.put("sampleSize", Math.min(limit, allFamilies.size()));
            response.put("families", allFamilies.entrySet().stream()
                .limit(limit)
                .collect(java.util.stream.Collectors.toMap(
                    Map.Entry::getKey,
                    Map.Entry::getValue
                )));
            
            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("‚ùå Failed to get sample families", e);
            return ResponseEntity.status(500).build();
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


audit/AuditLogger.java
File type: .java
package com.kotsin.consumer.audit;

import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.util.HashMap;
import java.util.Map;

/**
 * Audit logging service for tracking data processing events
 * 
 * COMPLIANCE: Track all critical data processing events
 * OBSERVABILITY: Structured audit trail for debugging and compliance
 */
@Component
@Slf4j
public class AuditLogger {

    @Value("${features.audit-logging.enabled:true}")
    private boolean auditLoggingEnabled;

    private static final DateTimeFormatter AUDIT_DATE_FORMAT = 
        DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss.SSSZ");

    /**
     * Log data processing event
     */
    public void logDataProcessing(String event, String scripCode, Map<String, Object> details) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> auditRecord = createAuditRecord(event, scripCode, details);
        log.info("AUDIT: {}", formatAuditRecord(auditRecord));
    }

    /**
     * Log configuration change
     */
    public void logConfigurationChange(String component, String parameter, Object oldValue, Object newValue) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> details = new HashMap<>();
        details.put("component", component);
        details.put("parameter", parameter);
        details.put("oldValue", oldValue);
        details.put("newValue", newValue);
        
        logDataProcessing("CONFIG_CHANGE", "SYSTEM", details);
    }

    /**
     * Log stream lifecycle event
     */
    public void logStreamLifecycle(String streamName, String action, String status) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> details = new HashMap<>();
        details.put("streamName", streamName);
        details.put("action", action);
        details.put("status", status);
        
        logDataProcessing("STREAM_LIFECYCLE", streamName, details);
    }

    /**
     * Log data quality issue
     */
    public void logDataQualityIssue(String issue, String scripCode, String description) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> details = new HashMap<>();
        details.put("issue", issue);
        details.put("description", description);
        details.put("severity", "WARNING");
        
        logDataProcessing("DATA_QUALITY_ISSUE", scripCode, details);
    }

    /**
     * Log processing error
     */
    public void logProcessingError(String operation, String scripCode, String errorMessage, String errorType) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> details = new HashMap<>();
        details.put("operation", operation);
        details.put("errorMessage", errorMessage);
        details.put("errorType", errorType);
        details.put("severity", "ERROR");
        
        logDataProcessing("PROCESSING_ERROR", scripCode, details);
    }

    /**
     * Log backpressure event
     */
    public void logBackpressureEvent(String action, long lag, long processedRecords) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> details = new HashMap<>();
        details.put("action", action);
        details.put("lag", lag);
        details.put("processedRecords", processedRecords);
        
        logDataProcessing("BACKPRESSURE_EVENT", "SYSTEM", details);
    }

    /**
     * Log candle emission
     */
    public void logCandleEmission(String scripCode, String timeframe, long timestamp, int tickCount) {
        if (!auditLoggingEnabled) {
            return;
        }

        Map<String, Object> details = new HashMap<>();
        details.put("timeframe", timeframe);
        details.put("timestamp", timestamp);
        details.put("tickCount", tickCount);
        
        logDataProcessing("CANDLE_EMISSION", scripCode, details);
    }

    /**
     * Create audit record
     */
    private Map<String, Object> createAuditRecord(String event, String scripCode, Map<String, Object> details) {
        Map<String, Object> auditRecord = new HashMap<>();
        auditRecord.put("timestamp", ZonedDateTime.now().format(AUDIT_DATE_FORMAT));
        auditRecord.put("event", event);
        auditRecord.put("scripCode", scripCode);
        auditRecord.put("details", details);
        return auditRecord;
    }

    /**
     * Format audit record for logging
     */
    private String formatAuditRecord(Map<String, Object> auditRecord) {
        StringBuilder sb = new StringBuilder();
        sb.append("event=").append(auditRecord.get("event"));
        sb.append(", timestamp=").append(auditRecord.get("timestamp"));
        sb.append(", scripCode=").append(auditRecord.get("scripCode"));
        
        @SuppressWarnings("unchecked")
        Map<String, Object> details = (Map<String, Object>) auditRecord.get("details");
        if (details != null && !details.isEmpty()) {
            sb.append(", details={");
            details.forEach((key, value) -> 
                sb.append(key).append("=").append(value).append(", ")
            );
            sb.delete(sb.length() - 2, sb.length()); // Remove trailing ", "
            sb.append("}");
        }
        
        return sb.toString();
    }
}


--------------------------------------------------
File End
--------------------------------------------------


transformers/CumToDeltaTransformer.java
File type: .java
package com.kotsin.consumer.transformers;

import com.kotsin.consumer.model.TickData;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.kstream.Transformer;
import org.apache.kafka.streams.processor.ProcessorContext;
import org.apache.kafka.streams.state.KeyValueStore;

public class CumToDeltaTransformer implements Transformer<String, TickData, KeyValue<String, TickData>> {
    private final String storeName;
    private KeyValueStore<String, Integer> store;

    public CumToDeltaTransformer(String storeName) { this.storeName = storeName; }

    @Override @SuppressWarnings("unchecked")
    public void init(ProcessorContext context) {
        this.store = (KeyValueStore<String, Integer>) context.getStateStore(storeName);
    }

    @Override
    public KeyValue<String, TickData> transform(String key, TickData tick) {
        if (tick == null) return null;

        // Ensure a stable per-instrument key for the delta store
        String stateKey = (key != null && !key.isEmpty()) ? key : tick.getScripCode();
        if (stateKey == null || stateKey.isEmpty()) {
            tick.setDeltaVolume(0);
            tick.setResetFlag(false);
            return KeyValue.pair(key, tick);
        }

        int curr = Math.max(0, tick.getTotalQuantity());   // cumulative day volume
        Integer prevMax = store.get(stateKey);

        int add;
        boolean isReset = false;
        
        if (prevMax == null) {
            // First observation in our store - this is a reset/startup
            // Use curr as delta BUT mark as reset for downstream filtering
            add = curr;
            isReset = true;
            store.put(stateKey, curr);
        } else if (curr < prevMax) {
            // CRITICAL: Day rollover or producer reset detected
            // DO NOT use curr as delta - this creates phantom volume
            // Instead, mark as reset and use 0 or null delta
            add = 0;  // No delta on reset
            isReset = true;
            store.put(stateKey, curr);
        } else {
            // Normal case: compute delta
            add = Math.max(0, curr - prevMax);
            isReset = false;
            store.put(stateKey, curr);
        }

        // Fallback: if cumulative delta is zero but we have a last trade size, use it
        // BUT only if this is NOT a reset
        if (add == 0 && !isReset && tick.getLastQuantity() > 0) {
            add = tick.getLastQuantity();
        }

        tick.setDeltaVolume(add);
        tick.setResetFlag(isReset);
        return KeyValue.pair(key, tick);
    }

    @Override public void close() {}
}


--------------------------------------------------
File End
--------------------------------------------------


model/InstrumentFamily.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

/**
 * Complete instrument family for an equity scrip
 * Contains equity + future + options in one structure
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class InstrumentFamily {
    
    private String equityScripCode;
    private String companyName;
    private InstrumentInfo equity;
    private InstrumentInfo future;
    private List<InstrumentInfo> options;
    private Long lastUpdated;
    private String dataSource; // "API", "CACHE", "ERROR"
    
    /**
     * Get total number of instruments in this family
     */
    public int getTotalInstruments() {
        int count = 1; // equity
        if (future != null) count++;
        if (options != null) count += options.size();
        return count;
    }
    
    /**
     * Check if family has options
     */
    public boolean hasOptions() {
        return options != null && !options.isEmpty();
    }
    
    /**
     * Check if family has future
     */
    public boolean hasFuture() {
        return future != null;
    }
    
    /**
     * Get options count
     */
    public int getOptionsCount() {
        return options != null ? options.size() : 0;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/ImbalanceBarState.java
File type: .java
package com.kotsin.consumer.model;

import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import java.io.Serializable;

/**
 * CRITICAL FIX: Completely rewritten State tracker for VIB/DIB
 * 
 * Based on "Advances in Financial Machine Learning" Chapter 2.3.2.2
 * 
 * ALL BUGS FIXED:
 * 1. ‚úÖ Serializable for Kafka state stores
 * 2. ‚úÖ All configuration externalized (no hardcoded 100, 20, 0.1, etc.)
 * 3. ‚úÖ Fixed division by zero (validate bid/ask, handle zero volumes)
 * 4. ‚úÖ Proper EWMA initialization with burn-in period
 * 5. ‚úÖ Improved tick classification per Chapter 19 tick rule
 * 6. ‚úÖ Input validation everywhere
 * 7. ‚úÖ Methods for processor (hasCompletedBar, emitAndReset)
 */
@Data
@NoArgsConstructor
@Slf4j
public class ImbalanceBarState implements Serializable {
    
    private static final long serialVersionUID = 1L;
    
    // Configuration (injected from application.properties via constructor)
    private double ewmaSpan = 100.0;
    private double minExpectedVolume = 100.0;
    private int warmupSamples = 20;
    private double thresholdMultiplier = 1.0;  // CRITICAL: Multiplier for multi-granularity (1x, 2x, 5x)
    
    // Current bar being constructed
    private InformationBar currentBar = new InformationBar();
    private boolean isComplete = false;
    
    // State for tick classification (Chapter 19: Tick Rule)
    private double previousPrice = 0.0;
    private int previousDirection = 1;  // Default to buy
    
    // EWMA state for expected imbalance calculation
    private double ewmaBuyVolume = 0.0;
    private double ewmaSellVolume = 0.0;
    private double ewmaBuyDollarVolume = 0.0;
    private double ewmaSellDollarVolume = 0.0;
    private double ewmaBuyProbability = 0.5;  // P[buy]
    private int sampleCount = 0;
    
    /**
     * Constructor with configuration
     * 
     * @param thresholdMultiplier - Like 1min vs 5min for time bars
     *                            - 1.0 = sensitive (high frequency)
     *                            - 2.0 = medium
     *                            - 5.0 = stable (low frequency, only major imbalances)
     */
    public ImbalanceBarState(double ewmaSpan, double minExpectedVolume, int warmupSamples, double thresholdMultiplier) {
        this.ewmaSpan = ewmaSpan;
        this.minExpectedVolume = minExpectedVolume;
        this.warmupSamples = warmupSamples;
        this.thresholdMultiplier = thresholdMultiplier;
        this.currentBar = new InformationBar();
        this.isComplete = false;
        this.previousDirection = 1;
    }
    
    /**
     * CRITICAL FIX: Classify tick using proper tick rule (Chapter 19)
     * 
     * Returns: 1 for buy-initiated, -1 for sell-initiated
     * 
     * Tick Rule:
     * - If Œîp > 0 ‚Üí buy (aggressor lifted offer)
     * - If Œîp < 0 ‚Üí sell (aggressor hit bid)
     * - If Œîp = 0 ‚Üí use previous direction (tick rule)
     */
    public int classifyTick(TickData tick) {
        // CRITICAL: Validate input
        if (tick == null) {
            log.warn("Null tick in classifyTick");
            return previousDirection;
        }
        
        double price = tick.getLastRate();
        
        // CRITICAL: Validate price
        if (price <= 0 || Double.isNaN(price) || Double.isInfinite(price)) {
            log.warn("Invalid price {} in classifyTick", price);
            return previousDirection;
        }
        
        // First tick: use bid/ask midpoint comparison
        if (previousPrice == 0) {
            double bid = tick.getBidRate();
            double ask = tick.getOfferRate();
            
            // CRITICAL: Handle zero/invalid bid-ask
            if (bid > 0 && ask > 0 && ask > bid) {
                double mid = (bid + ask) / 2.0;
                previousDirection = price >= mid ? 1 : -1;
            } else {
                // No valid bid/ask, assume buy
                previousDirection = 1;
            }
            previousPrice = price;
            return previousDirection;
        }
        
        // Tick rule: price change determines direction
        if (price > previousPrice) {
            previousDirection = 1;  // Buy (price went up)
        } else if (price < previousPrice) {
            previousDirection = -1;  // Sell (price went down)
        }
        // else: keep previousDirection (no price change)
        
        previousPrice = price;
        return previousDirection;
    }
    
    /**
     * Add tick to current bar
     * 
     * CRITICAL: Validates all inputs before processing
     */
    public void addTick(TickData tick, String barType) {
        try {
            // CRITICAL: Validate tick
            if (tick == null) {
                log.warn("[{}] Null tick, skipping", barType);
                return;
            }
            
            // CRITICAL: Validate deltaVolume
            Integer deltaVol = tick.getDeltaVolume();
            if (deltaVol == null || deltaVol <= 0) {
                // Quote update only, no trade
                return;
            }
            
            // Initialize bar on first tick
            if (currentBar.getScripCode() == null) {
                currentBar.setBarType(barType);
                currentBar.setScripCode(tick.getScripCode());
                currentBar.setToken(tick.getToken());
                currentBar.setExchange(tick.getExchange());
                currentBar.setCompanyName(tick.getCompanyName());
            }
            
            // CRITICAL FIX: Always set start timestamp if it's 0 (handles state restoration)
            if (currentBar.getWindowStartMillis() == 0) {
                long ts = tick.getTimestamp();
                currentBar.setWindowStartMillis(ts);
                log.debug("[{}] Set window start timestamp: {} for token {}", 
                    barType, ts, tick.getToken());
            }
            
            // Classify and add
            int direction = classifyTick(tick);
            currentBar.updateOHLC(tick);
            currentBar.addVolume(tick, direction);
            
            long endTs = tick.getTimestamp();
            currentBar.setWindowEndMillis(endTs);
            
            // DEBUG: Log every 10th tick
            if (currentBar.getTickCount() % 10 == 0) {
                log.debug("[{}] Tick #{}, start={}, end={}", 
                    barType, currentBar.getTickCount(), 
                    currentBar.getWindowStartMillis(), currentBar.getWindowEndMillis());
            }
            
            // Check if bar should be emitted
            if (barType.equals("VIB")) {
                if (shouldEmitVolumeImbalanceBar()) {
                    completeBar();
                }
            } else if (barType.equals("DIB")) {
                if (shouldEmitDollarImbalanceBar()) {
                    completeBar();
                }
            }
            
        } catch (Exception e) {
            log.error("[{}] Error adding tick: {}", barType, e.getMessage(), e);
        }
    }
    
    /**
     * CRITICAL FIX: Calculate expected volume imbalance
     * 
     * From Chapter 2, Section 2.3.2.2:
     * E[Œ∏_T] = E[T] * |2v+ - E[v]|
     * 
     * Where:
     * - v+ = P[buy] * E[v|buy]
     * - v- = P[sell] * E[v|sell]
     * - E[v] = v+ + v-
     */
    public double getExpectedVolumeImbalance() {
        // CRITICAL: During warmup, use conservative estimate
        if (sampleCount < warmupSamples) {
            double avgVolume = (ewmaBuyVolume + ewmaSellVolume);
            if (avgVolume == 0) {
                return minExpectedVolume;  // CRITICAL: Avoid zero
            }
            return Math.max(avgVolume * 0.1, minExpectedVolume);
        }
        
        // CRITICAL: Check for zero to avoid division issues
        double totalVolume = ewmaBuyVolume + ewmaSellVolume;
        if (totalVolume == 0) {
            return minExpectedVolume;
        }
        
        // Expected imbalance = |buy - sell|
        double expectedImbalance = Math.abs(ewmaBuyVolume - ewmaSellVolume);
        
        // CRITICAL: Floor at 5% of total volume
        return Math.max(expectedImbalance, totalVolume * 0.05);
    }
    
    /**
     * CRITICAL FIX: Calculate expected dollar imbalance
     */
    public double getExpectedDollarImbalance() {
        if (sampleCount < warmupSamples) {
            double avgDollar = (ewmaBuyDollarVolume + ewmaSellDollarVolume);
            if (avgDollar == 0) {
                return minExpectedVolume * 100.0;  // Assume ‚Çπ100 per unit
            }
            return Math.max(avgDollar * 0.1, minExpectedVolume * 100.0);
        }
        
        double totalDollar = ewmaBuyDollarVolume + ewmaSellDollarVolume;
        if (totalDollar == 0) {
            return minExpectedVolume * 100.0;
        }
        
        double expectedImbalance = Math.abs(ewmaBuyDollarVolume - ewmaSellDollarVolume);
        return Math.max(expectedImbalance, totalDollar * 0.05);
    }
    
    /**
     * Check if VIB bar should be emitted
     * 
     * CRITICAL: Uses threshold multiplier for multi-granularity
     * 1x = sensitive, 2x = medium, 5x = stable (like 1min vs 5min)
     * FIXED: Clear run fields (not applicable for imbalance bars)
     */
    public boolean shouldEmitVolumeImbalanceBar() {
        // CRITICAL: Min ticks check
        if (currentBar.getTickCount() < 10) {
            return false;
        }
        
        double buyVol = currentBar.getBuyVolume();
        double sellVol = currentBar.getSellVolume();
        double actualImbalance = Math.abs(buyVol - sellVol);
        double expectedImbalance = getExpectedVolumeImbalance() * thresholdMultiplier;  // CRITICAL: Apply multiplier!
        
        // Set imbalance metrics
        currentBar.setImbalance(actualImbalance);
        currentBar.setExpectedImbalance(expectedImbalance);
        
        // CRITICAL FIX: VIB bars are NOT runs bars - clear run fields
        currentBar.setRunLength(0);
        currentBar.setRunVolume(0.0);
        currentBar.setExpectedRunLength(0.0);
        currentBar.setRunDirection(0);
        
        log.debug("[VIB] actual={:.2f}, expected={:.2f}, threshold={:.1f}x, willEmit={}", 
            actualImbalance, expectedImbalance, thresholdMultiplier, actualImbalance >= expectedImbalance);
        
        return actualImbalance >= expectedImbalance;
    }
    
    /**
     * Check if DIB bar should be emitted
     * 
     * CRITICAL: Uses threshold multiplier for multi-granularity
     * FIXED: Clear run fields (not applicable for imbalance bars)
     */
    public boolean shouldEmitDollarImbalanceBar() {
        if (currentBar.getTickCount() < 10) {
            return false;
        }
        
        double buyDollar = currentBar.getBuyDollarVolume();
        double sellDollar = currentBar.getSellDollarVolume();
        double actualImbalance = Math.abs(buyDollar - sellDollar);
        double expectedImbalance = getExpectedDollarImbalance() * thresholdMultiplier;  // CRITICAL: Apply multiplier!
        
        // Set imbalance metrics
        currentBar.setImbalance(actualImbalance);
        currentBar.setExpectedImbalance(expectedImbalance);
        
        // CRITICAL FIX: DIB bars are NOT runs bars - clear run fields
        currentBar.setRunLength(0);
        currentBar.setRunVolume(0.0);
        currentBar.setExpectedRunLength(0.0);
        currentBar.setRunDirection(0);
        
        log.debug("[DIB] actual={:.2f}, expected={:.2f}, threshold={:.1f}x, willEmit={}", 
            actualImbalance, expectedImbalance, thresholdMultiplier, actualImbalance >= expectedImbalance);
        
        return actualImbalance >= expectedImbalance;
    }
    
    /**
     * Mark bar as complete and update EWMA
     */
    private void completeBar() {
        try {
            double alpha = 2.0 / (ewmaSpan + 1.0);
            
            // CRITICAL: Initialize EWMA on first bar
            if (sampleCount == 0) {
                ewmaBuyVolume = currentBar.getBuyVolume();
                ewmaSellVolume = currentBar.getSellVolume();
                ewmaBuyDollarVolume = currentBar.getBuyDollarVolume();
                ewmaSellDollarVolume = currentBar.getSellDollarVolume();
            } else {
                // Update EWMA
                ewmaBuyVolume = alpha * currentBar.getBuyVolume() + (1 - alpha) * ewmaBuyVolume;
                ewmaSellVolume = alpha * currentBar.getSellVolume() + (1 - alpha) * ewmaSellVolume;
                ewmaBuyDollarVolume = alpha * currentBar.getBuyDollarVolume() + (1 - alpha) * ewmaBuyDollarVolume;
                ewmaSellDollarVolume = alpha * currentBar.getSellDollarVolume() + (1 - alpha) * ewmaSellDollarVolume;
            }
            
            // Update buy probability
            double totalVol = currentBar.getVolume();
            if (totalVol > 0) {
                double buyProb = currentBar.getBuyVolume() / totalVol;
                ewmaBuyProbability = alpha * buyProb + (1 - alpha) * ewmaBuyProbability;
            }
            
            sampleCount++;
            currentBar.calculateImbalanceMetrics();
            isComplete = true;
            
            log.debug("Bar completed: sample #{}, imbalance={:.2f}, expected={:.2f}", 
                sampleCount, currentBar.getImbalance(), currentBar.getExpectedImbalance());
            
        } catch (Exception e) {
            log.error("Error completing bar: {}", e.getMessage(), e);
        }
    }
    
    /**
     * Check if bar is ready to emit
     */
    public boolean hasCompletedBar() {
        return isComplete;
    }
    
    /**
     * Emit completed bar and reset for next
     */
    public InformationBar emitAndReset() {
        InformationBar barToEmit = this.currentBar;
        
        // CRITICAL DEBUG: Log what we're emitting
        log.info("EMIT {} bar: token={}, start={}, end={}, timestamp={}, ticks={}",
            barToEmit.getBarType(),
            barToEmit.getToken(),
            barToEmit.getWindowStartMillis(),
            barToEmit.getWindowEndMillis(),
            barToEmit.getTimestamp(),
            barToEmit.getTickCount());
        
        // Reset for next bar
        this.currentBar = new InformationBar();
        this.isComplete = false;
        // CRITICAL: Keep EWMA state, previousPrice, previousDirection
        
        return barToEmit;
    }
    
    /**
     * Kafka Serde for ImbalanceBarState
     */
    public static org.apache.kafka.common.serialization.Serde<ImbalanceBarState> serde() {
        return new org.springframework.kafka.support.serializer.JsonSerde<>(ImbalanceBarState.class);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/FamilyStructuredAll.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.springframework.kafka.support.serializer.JsonSerde;

import java.util.HashMap;
import java.util.Map;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class FamilyStructuredAll {

    private String familyKey;
    private String familyName;

    // timeframe -> candle
    @Builder.Default
    private Map<String, InstrumentCandle> equity = new HashMap<>();

    @Builder.Default
    private Map<String, InstrumentCandle> future = new HashMap<>();

    // scripCode -> (timeframe -> candle)
    @Builder.Default
    private Map<String, Map<String, InstrumentCandle>> options = new HashMap<>();

    private Long processingTimestamp;

    public static JsonSerde<FamilyStructuredAll> serde() {
        return new JsonSerde<>(FamilyStructuredAll.class);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/MessageMetadata.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.Set;

/**
 * Message metadata and quality information
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class MessageMetadata {
    
    private String messageVersion;
    private Long producedAt;
    private String dataQuality;
    private Set<String> completeWindows;
    private Integer processingLatency;
    private String source;
    private Long sequenceNumber;
    
    /**
     * Check if message is high quality
     */
    public boolean isHighQuality() {
        return "HIGH".equals(dataQuality);
    }
    
    /**
     * Check if message is medium quality
     */
    public boolean isMediumQuality() {
        return "MEDIUM".equals(dataQuality);
    }
    
    /**
     * Check if message is low quality
     */
    public boolean isLowQuality() {
        return "LOW".equals(dataQuality);
    }
    
    /**
     * Get complete windows count
     */
    public int getCompleteWindowsCount() {
        return completeWindows != null ? completeWindows.size() : 0;
    }
    
    /**
     * Check if specific timeframe is complete
     */
    public boolean isTimeframeComplete(String timeframe) {
        return completeWindows != null && completeWindows.contains(timeframe);
    }
    
    /**
     * Get processing latency level
     */
    public String getLatencyLevel() {
        if (processingLatency == null) return "UNKNOWN";
        if (processingLatency < 10) return "VERY_FAST";
        if (processingLatency < 50) return "FAST";
        if (processingLatency < 100) return "NORMAL";
        if (processingLatency < 500) return "SLOW";
        return "VERY_SLOW";
    }
    
    /**
     * Get display string for logging
     */
    public String getDisplayString() {
        return String.format("Meta[v%s,%s,%dms,%d complete]",
            messageVersion, dataQuality, processingLatency, getCompleteWindowsCount());
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/OpenInterestData.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.Data;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.Serializer;

/**
 * Represents Open Interest data received from the market.
 * Example JSON:
 * {
 *   "companyName": "ZINC 31 OCT 2025",
 *   "receivedTimestamp": 1760196281770,
 *   "Exch": "M",
 *   "ExchType": "D",
 *   "Token": 458300,
 *   "OpenInterest": 3299
 * }
 */
@Data
@JsonIgnoreProperties(ignoreUnknown = true)
public class OpenInterestData {

    @JsonProperty("companyName")
    private String companyName;

    @JsonProperty("receivedTimestamp")
    private long receivedTimestamp;

    @JsonProperty("Exch")
    private String exchange;

    @JsonProperty("ExchType")
    private String exchangeType;

    @JsonProperty("Token")
    private int token;

    @JsonProperty("OpenInterest")
    private long openInterest;

    /**
     * Get timestamp for Kafka Streams processing.
     * Use receivedTimestamp as the event time.
     */
    public long getTimestamp() {
        return receivedTimestamp;
    }

    /**
     * Get scrip identifier (using token).
     */
    public String getScripKey() {
        return String.valueOf(token);
    }

    /**
     * Provides Kafka Serde for OpenInterestData.
     */
    public static Serde<OpenInterestData> serde() {
        return Serdes.serdeFrom(new OpenInterestDataSerializer(), new OpenInterestDataDeserializer());
    }

    // ---------------------------------------------------
    // Internal Serializer/Deserializer
    // ---------------------------------------------------
    public static class OpenInterestDataSerializer implements Serializer<OpenInterestData> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public byte[] serialize(String topic, OpenInterestData data) {
            if (data == null) return null;
            try {
                return objectMapper.writeValueAsBytes(data);
            } catch (Exception e) {
                throw new RuntimeException("Serialization failed for OpenInterestData", e);
            }
        }
    }

    public static class OpenInterestDataDeserializer implements Deserializer<OpenInterestData> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public OpenInterestData deserialize(String topic, byte[] bytes) {
            if (bytes == null) return null;
            try {
                return objectMapper.readValue(bytes, OpenInterestData.class);
            } catch (Exception e) {
                throw new RuntimeException("Deserialization failed for OpenInterestData", e);
            }
        }
    }
}



--------------------------------------------------
File End
--------------------------------------------------


model/OpenInterestAggregation.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.Serializer;

/**
 * Aggregated Open Interest data with microstructure/quant features.
 * Computed over time windows (1m, 2m, 3m, 5m, 15m, 30m).
 * 
 * Quant Features:
 * - OI Delta: Change in OI (positive = net long buildup, negative = net short buildup or unwinding)
 * - OI Momentum: Rate of change in OI
 * - OI Velocity: OI change per unit volume (divergence indicator)
 * - OI Concentration: Ratio of OI at window end vs start (buildup intensity)
 * - Cumulative OI Change: Total OI change across the window
 * 
 * Trading Signals:
 * - Rising OI + Rising Price = Bullish (long buildup)
 * - Rising OI + Falling Price = Bearish (short buildup)
 * - Falling OI + Rising Price = Short covering
 * - Falling OI + Falling Price = Long unwinding
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
@JsonIgnoreProperties(ignoreUnknown = true)
public class OpenInterestAggregation {

    @JsonProperty("token")
    private int token;

    @JsonProperty("companyName")
    private String companyName;

    @JsonProperty("exchange")
    private String exchange;

    @JsonProperty("exchangeType")
    private String exchangeType;

    @JsonProperty("windowStartTime")
    private long windowStartTime;

    @JsonProperty("windowEndTime")
    private long windowEndTime;

    @JsonProperty("windowSizeMinutes")
    private int windowSizeMinutes;

    // Core OI metrics
    @JsonProperty("openInterestStart")
    private long openInterestStart;

    @JsonProperty("openInterestEnd")
    private long openInterestEnd;

    @JsonProperty("openInterestHigh")
    private long openInterestHigh;

    @JsonProperty("openInterestLow")
    private long openInterestLow;

    @JsonProperty("oiChangeAbsolute")
    private long oiChangeAbsolute;  // OI Delta

    @JsonProperty("oiChangePercent")
    private double oiChangePercent;

    // Advanced quant features
    @JsonProperty("oiMomentum")
    private double oiMomentum;  // OI change per minute

    @JsonProperty("oiConcentration")
    private double oiConcentration;  // End/Start ratio (buildup intensity)

    @JsonProperty("cumulativeOiChange")
    private long cumulativeOiChange;  // Sum of all OI changes in window

    @JsonProperty("oiVolatility")
    private double oiVolatility;  // Standard deviation of OI changes

    @JsonProperty("updateCount")
    private int updateCount;  // Number of OI updates in window

    @JsonProperty("avgOiPerUpdate")
    private double avgOiPerUpdate;

    // For correlation with price/volume (to be enriched by downstream services)
    @JsonProperty("lastReceivedTimestamp")
    private long lastReceivedTimestamp;

    /**
     * Provides Kafka Serde for OpenInterestAggregation.
     */
    public static Serde<OpenInterestAggregation> serde() {
        return Serdes.serdeFrom(new OpenInterestAggregationSerializer(), new OpenInterestAggregationDeserializer());
    }

    // ---------------------------------------------------
    // Internal Serializer/Deserializer
    // ---------------------------------------------------
    public static class OpenInterestAggregationSerializer implements Serializer<OpenInterestAggregation> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public byte[] serialize(String topic, OpenInterestAggregation data) {
            if (data == null) return null;
            try {
                return objectMapper.writeValueAsBytes(data);
            } catch (Exception e) {
                throw new RuntimeException("Serialization failed for OpenInterestAggregation", e);
            }
        }
    }

    public static class OpenInterestAggregationDeserializer implements Deserializer<OpenInterestAggregation> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public OpenInterestAggregation deserialize(String topic, byte[] bytes) {
            if (bytes == null) return null;
            try {
                return objectMapper.readValue(bytes, OpenInterestAggregation.class);
            } catch (Exception e) {
                throw new RuntimeException("Deserialization failed for OpenInterestAggregation", e);
            }
        }
    }
}



--------------------------------------------------
File End
--------------------------------------------------


model/MicrostructureFeature.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.common.serialization.Serde;
import org.springframework.kafka.support.serializer.JsonSerde;

import java.io.Serializable;

/**
 * Market Microstructure Features
 * Based on "Advances in Financial Machine Learning" Chapter 19
 * 
 * Measures order flow dynamics, toxicity, and price impact
 */
@Data
@NoArgsConstructor
@AllArgsConstructor
public class MicrostructureFeature implements Serializable {
    
    private static final long serialVersionUID = 1L;
    
    // Identity
    @JsonProperty("token")
    private int token;
    
    @JsonProperty("scripCode")
    private String scripCode;
    
    @JsonProperty("companyName")
    private String companyName;
    
    @JsonProperty("exchange")
    private String exchange;
    
    @JsonProperty("exchangeType")
    private String exchangeType;
    
    @JsonProperty("timestamp")
    private long timestamp;
    
    // Order Flow Imbalance (OFI) - Chapter 19
    // Measures aggressive buying vs selling pressure
    @JsonProperty("ofi1")
    private double ofi1;  // Top-of-book (most sensitive)
    
    @JsonProperty("ofi5")
    private double ofi5;  // Top 5 levels
    
    @JsonProperty("ofi20")
    private double ofi20;  // Top 20 levels (full depth)
    
    // Depth Imbalance - Measures resting liquidity imbalance
    // Value between -1 (all asks) and +1 (all bids)
    @JsonProperty("depthImbalance1")
    private double depthImbalance1;  // Level 1
    
    @JsonProperty("depthImbalance5")
    private double depthImbalance5;  // Top 5 levels
    
    @JsonProperty("depthImbalance20")
    private double depthImbalance20;  // Top 20 levels
    
    // VPIN - Volume-Synchronized Probability of Informed Trading
    // Measures toxicity of order flow (0-1, higher = more toxic)
    @JsonProperty("vpin")
    private double vpin;
    
    // Kyle's Lambda - Price impact per unit volume
    // Measures market liquidity (higher = less liquid)
    @JsonProperty("kyleLambda")
    private double kyleLambda;
    
    // Microprice - Volume-weighted mid price
    // More accurate than simple mid for true value
    @JsonProperty("microprice")
    private double microprice;
    
    // Effective Spread - Actual cost of trading
    @JsonProperty("effectiveSpread")
    private double effectiveSpread;
    
    // Book State (for reference)
    @JsonProperty("bestBid")
    private double bestBid;
    
    @JsonProperty("bestAsk")
    private double bestAsk;
    
    @JsonProperty("midPrice")
    private double midPrice;
    
    @JsonProperty("spread")
    private double spread;
    
    @JsonProperty("spreadBps")
    private double spreadBps;  // Spread in basis points
    
    // Volume metrics
    @JsonProperty("bidVolume1")
    private int bidVolume1;  // Volume at best bid
    
    @JsonProperty("askVolume1")
    private int askVolume1;  // Volume at best ask
    
    @JsonProperty("totalBidVolume5")
    private int totalBidVolume5;  // Total bid volume (top 5)
    
    @JsonProperty("totalAskVolume5")
    private int totalAskVolume5;  // Total ask volume (top 5)
    
    @JsonProperty("totalBidVolume20")
    private int totalBidVolume20;  // Total bid volume (top 20)
    
    @JsonProperty("totalAskVolume20")
    private int totalAskVolume20;  // Total ask volume (top 20)
    
    // Number of orders (order book depth quality)
    @JsonProperty("bidOrders1")
    private int bidOrders1;
    
    @JsonProperty("askOrders1")
    private int askOrders1;
    
    /**
     * Kafka Serde for serialization
     */
    public static Serde<MicrostructureFeature> serde() {
        return new JsonSerde<>(MicrostructureFeature.class);
    }
    
    /**
     * Calculate spread in basis points
     */
    public void calculateSpreadBps() {
        if (midPrice > 0 && spread > 0) {
            this.spreadBps = (spread / midPrice) * 10000.0;
        } else {
            this.spreadBps = 0.0;
        }
    }
    
    /**
     * Validate feature values (for debugging)
     */
    public boolean isValid() {
        return token > 0 
            && timestamp > 0 
            && midPrice > 0
            && !Double.isNaN(ofi5)
            && !Double.isNaN(depthImbalance5)
            && !Double.isInfinite(ofi5)
            && !Double.isInfinite(depthImbalance5);
    }
    
    @Override
    public String toString() {
        return String.format(
            "MicrostructureFeature{token=%d, company=%s, timestamp=%d, " +
            "ofi5=%.2f, depthImb5=%.3f, vpin=%.4f, kyleLambda=%.6f, " +
            "mid=%.2f, spread=%.2f}",
            token, companyName, timestamp,
            ofi5, depthImbalance5, vpin, kyleLambda,
            midPrice, spread
        );
    }
}



--------------------------------------------------
File End
--------------------------------------------------


model/InstrumentInfo.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Individual instrument information
 * Used for equity, future, and options
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class InstrumentInfo {
    
    private String scripCode;
    private String token;        // Added: join key for OI/Orderbook
    private String name;
    private String fullName;
    private String exchange;
    private String exchangeType;
    private String series;
    private String expiry;
    private String scripType;
    private Double strikeRate;
    private Double tickSize;
    private Integer lotSize;
    private String isin;
    private String symbolRoot;
    private String bocoallowed;
    private String id;
    private String scriptTypeKotsin;
    private String insertionDate;
    private String multiplier;
    private String qtyLimit;
    private String scripData;
    
    /**
     * Check if this is an equity instrument
     */
    public boolean isEquity() {
        return "C".equals(exchangeType) && "XX".equals(scripType);
    }
    
    /**
     * Check if this is a future instrument
     */
    public boolean isFuture() {
        return "D".equals(exchangeType) && "XX".equals(scripType);
    }
    
    /**
     * Check if this is an option instrument
     */
    public boolean isOption() {
        return "D".equals(exchangeType) && ("CE".equals(scripType) || "PE".equals(scripType));
    }
    
    /**
     * Check if this is a call option
     */
    public boolean isCallOption() {
        return "CE".equals(scripType);
    }
    
    /**
     * Check if this is a put option
     */
    public boolean isPutOption() {
        return "PE".equals(scripType);
    }
    
    /**
     * Get display name for logging
     */
    public String getDisplayName() {
        if (isOption()) {
            return String.format("%s %s %.2f %s", name, expiry, strikeRate, scripType);
        }
        return name;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/OrderbookDepthData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;
import org.springframework.kafka.support.serializer.JsonSerde;

/**
 * Comprehensive orderbook depth analytics
 * Implements professional trading orderbook features
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class OrderbookDepthData {

    // 1. Depth Profile (levels 1-10)
    private List<DepthLevel> bidProfile;      // Bid quantities at each level
    private List<DepthLevel> askProfile;      // Ask quantities at each level

    // 2. Order book imbalance by level (weighted by distance from mid)
    private Double weightedDepthImbalance;     // Imbalance weighted by distance
    private Double level1Imbalance;            // Imbalance at best bid/ask
    private Double level2to5Imbalance;         // Imbalance at levels 2-5
    private Double level6to10Imbalance;        // Imbalance at levels 6-10

    // 3. Cumulative depth (sum of quantities at each level)
    private List<Double> cumulativeBidDepth;   // [level1, level1+2, level1+2+3, ...]
    private List<Double> cumulativeAskDepth;
    private Double totalBidDepth;              // Sum of all bid levels
    private Double totalAskDepth;              // Sum of all ask levels

    // 4. Depth pressure (volume-weighted average price of book)
    private Double bidVWAP;                    // VWAP of bid side (support level)
    private Double askVWAP;                    // VWAP of ask side (resistance level)
    private Double depthPressure;              // (bidVWAP - askVWAP) / midPrice

    // 5. Order book slope (rate of quantity decay by level)
    private Double bidSlope;                   // Negative = steep decay (weak support)
    private Double askSlope;                   // Negative = steep decay (weak resistance)
    private Double slopeRatio;                 // bidSlope / askSlope

    // 6. Iceberg detection (large orders split across levels)
    private Boolean icebergDetectedBid;        // Unusual uniformity on bid side
    private Boolean icebergDetectedAsk;        // Unusual uniformity on ask side
    private Double icebergProbabilityBid;      // 0-1 probability score
    private Double icebergProbabilityAsk;

    // 7. Spoofing detection (large orders that disappear quickly)
    private List<SpoofingEvent> spoofingEvents; // Recent spoofing events
    private Integer spoofingCountLast1Min;     // Number of events in last minute
    private Boolean activeSpoofingBid;         // Currently detecting spoof on bid
    private Boolean activeSpoofingAsk;         // Currently detecting spoof on ask

    // Metadata
    private Long timestamp;
    private Double midPrice;
    private Double spread;
    private Integer depthLevels;               // How many levels captured (typically 10)
    private Boolean isComplete;

    /**
     * Depth Level - represents aggregated data at a price level
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class DepthLevel {
        private Integer level;                 // 1 = best, 2 = second best, etc.
        private Double price;
        private Integer quantity;
        private Integer numberOfOrders;
        private Double distanceFromMid;        // Price distance from mid (bps)
        private Double percentOfTotalDepth;    // What % of total depth is at this level
    }

    /**
     * Spoofing Event - captures a large order that disappeared
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class SpoofingEvent {
        private Long timestamp;
        private String side;                   // "BID" or "ASK"
        private Double price;
        private Integer quantity;
        private Long durationMs;               // How long the order lasted
        private String classification;         // "POSSIBLE_SPOOF", "CONFIRMED_SPOOF"
    }

    /**
     * Check if depth data is valid
     */
    public boolean isValid() {
        return bidProfile != null && !bidProfile.isEmpty() &&
               askProfile != null && !askProfile.isEmpty() &&
               totalBidDepth != null && totalAskDepth != null;
    }

    /**
     * Get depth imbalance at best level (level 1)
     */
    public Double getLevel1DepthImbalance() {
        if (bidProfile == null || askProfile == null ||
            bidProfile.isEmpty() || askProfile.isEmpty()) {
            return 0.0;
        }

        double bidQty = bidProfile.get(0).getQuantity();
        double askQty = askProfile.get(0).getQuantity();

        if (bidQty + askQty == 0) {
            return 0.0;
        }

        return (bidQty - askQty) / (bidQty + askQty);
    }

    /**
     * Check if there's strong buying pressure (weighted imbalance > threshold)
     */
    public Boolean hasStrongBuyingPressure() {
        return weightedDepthImbalance != null && weightedDepthImbalance > 0.3;
    }

    /**
     * Check if there's strong selling pressure
     */
    public Boolean hasStrongSellingPressure() {
        return weightedDepthImbalance != null && weightedDepthImbalance < -0.3;
    }

    /**
     * Check if orderbook is balanced
     */
    public Boolean isBalanced() {
        return weightedDepthImbalance != null &&
               Math.abs(weightedDepthImbalance) < 0.1;
    }

    /**
     * Get support level (bid VWAP)
     */
    public Double getSupportLevel() {
        return bidVWAP;
    }

    /**
     * Get resistance level (ask VWAP)
     */
    public Double getResistanceLevel() {
        return askVWAP;
    }

    /**
     * Check if iceberg orders detected
     */
    public Boolean hasIcebergOrders() {
        return (icebergDetectedBid != null && icebergDetectedBid) ||
               (icebergDetectedAsk != null && icebergDetectedAsk);
    }

    /**
     * Check if spoofing activity detected recently
     */
    public Boolean hasSpoofingActivity() {
        return (spoofingCountLast1Min != null && spoofingCountLast1Min > 0) ||
               (activeSpoofingBid != null && activeSpoofingBid) ||
               (activeSpoofingAsk != null && activeSpoofingAsk);
    }

    /**
     * Get display string for logging
     */
    public String getDisplayString() {
        return String.format("Depth[Imb:%.2f,Slope:%.2f/%.2f,Ice:%s/%s,Spoof:%d] %s",
            weightedDepthImbalance != null ? weightedDepthImbalance : 0.0,
            bidSlope != null ? bidSlope : 0.0,
            askSlope != null ? askSlope : 0.0,
            icebergDetectedBid != null && icebergDetectedBid ? "Y" : "N",
            icebergDetectedAsk != null && icebergDetectedAsk ? "Y" : "N",
            spoofingCountLast1Min != null ? spoofingCountLast1Min : 0,
            isComplete != null && isComplete ? "COMPLETE" : "PARTIAL");
    }

    public static org.apache.kafka.common.serialization.Serde<OrderbookDepthData> serde() {
        return new JsonSerde<>(OrderbookDepthData.class);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/FamilyAggregatedMetrics.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Aggregated metrics across all instruments in an instrument family
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class FamilyAggregatedMetrics {

    // Volume metrics
    private Long totalVolume;                    // Sum of all instrument volumes
    private Long equityVolume;
    private Long futuresVolume;
    private Long optionsVolume;

    // Open Interest metrics
    private Long totalOpenInterest;
    private Long futuresOI;
    private Long callsOI;
    private Long putsOI;
    private Long futuresOIChange;
    private Long callsOIChange;
    private Long putsOIChange;

    // Options metrics
    private Double putCallRatio;                 // Puts OI / Calls OI
    private Double putCallVolumeRatio;           // Puts Volume / Calls Volume
    private Integer activeOptionsCount;          // Number of options with volume > 0

    // Price correlation
    private Double spotPrice;                    // Equity or index spot price
    private Double nearMonthFuturePrice;         // Nearest expiry future price
    private Double futuresBasis;                 // Future - Spot
    private Double futuresBasisPercent;          // (Future - Spot) / Spot * 100

    // Futures chain metrics
    private Integer activeFuturesCount;          // Number of futures contracts
    private String nearMonthExpiry;              // Nearest expiry date

    // Orderbook metrics
    private Double avgBidAskSpread;              // Average across all instruments
    private Long totalBidVolume;
    private Long totalAskVolume;
    private Double bidAskImbalance;              // (Bid - Ask) / (Bid + Ask)

    // Timestamp
    private Long calculatedAt;
}


--------------------------------------------------
File End
--------------------------------------------------


model/MicrostructureFeatureState.java
File type: .java
package com.kotsin.consumer.model;

import com.kotsin.consumer.service.MicrostructureMetricsCalculator;
import lombok.extern.slf4j.Slf4j;

import java.io.Serializable;
import java.util.ArrayDeque;
import java.util.Deque;

/**
 * State for calculating microstructure features
 * Based on "Advances in Financial Machine Learning" Chapter 19
 *
 * Maintains rolling window of order book snapshots to calculate:
 * - OFI (Order Flow Imbalance)
 * - VPIN (Volume-Synchronized Probability of Informed Trading)
 * - Kyle's Lambda (price impact)
 * - Depth Imbalance
 */
@Slf4j
public class MicrostructureFeatureState implements Serializable {

    private static final long serialVersionUID = 1L;

    // Calculator service (transient because it's not part of state)
    private transient MicrostructureMetricsCalculator calculator = new MicrostructureMetricsCalculator();

    // Configuration
    private final int windowSize;
    private final int minObservations;
    private final long emitIntervalMs;
    
    // History - using ArrayList instead of Deque for better serialization
    private final java.util.List<OrderBookSnapshot> orderBookHistory;
    private final java.util.List<Double> priceHistory;
    private final java.util.List<Double> signedVolumeHistory;
    
    // Current feature (to be emitted)
    private MicrostructureFeature currentFeature;
    private boolean isReady = false;
    
    // Time-based emission tracking
    private long lastEmitTimestamp = 0;
    
    /**
     * Constructor
     * 
     * @param windowSize Number of snapshots to keep for rolling calculations
     * @param minObservations Minimum observations before emitting features
     * @param emitIntervalMs Emit features at most once per this interval (milliseconds)
     */
    public MicrostructureFeatureState(int windowSize, int minObservations, long emitIntervalMs) {
        this.windowSize = windowSize;
        this.minObservations = minObservations;
        this.emitIntervalMs = emitIntervalMs;
        this.orderBookHistory = new java.util.ArrayList<>(windowSize);
        this.priceHistory = new java.util.ArrayList<>(windowSize);
        this.signedVolumeHistory = new java.util.ArrayList<>(windowSize);
        this.currentFeature = new MicrostructureFeature();
    }
    
    /**
     * Update state with new order book snapshot
     */
    public void update(OrderBookSnapshot snapshot) {
        try {
            // Validate
            if (snapshot == null || !snapshot.isValid()) {
                log.warn("Invalid order book snapshot for token {}", 
                    snapshot != null ? snapshot.getToken() : "null");
                return;
            }
            
            // Parse details if not already done
            if (snapshot.getAllBids() == null) {
                snapshot.parseDetails();
            }
            
            // Add to history (maintain window size)
            if (orderBookHistory.size() >= windowSize) {
                orderBookHistory.remove(0);
            }
            orderBookHistory.add(snapshot);
            
            // Add price to history
            if (priceHistory.size() >= windowSize) {
                priceHistory.remove(0);
            }
            priceHistory.add(snapshot.getMidPrice());
            
            // Calculate features if we have enough history
            if (orderBookHistory.size() >= minObservations) {
                calculateFeatures(snapshot);
                
                // Check if enough time has passed since last emit
                long currentTimestamp = snapshot.getTimestamp();
                if (currentTimestamp - lastEmitTimestamp >= emitIntervalMs) {
                    isReady = true;
                    lastEmitTimestamp = currentTimestamp;
                } else {
                    isReady = false;  // Throttle: not ready yet
                }
            }
            
        } catch (Exception e) {
            log.error("Error updating microstructure state for token {}: {}", 
                snapshot != null ? snapshot.getToken() : "null", 
                e.getMessage(), e);
        }
    }
    
    /**
     * Calculate all microstructure features
     */
    private void calculateFeatures(OrderBookSnapshot current) {
        currentFeature = new MicrostructureFeature();
        
        // Identity
        // Convert token from String to int
        try {
            currentFeature.setToken(Integer.parseInt(current.getToken()));
        } catch (NumberFormatException e) {
            log.warn("Invalid token format: {}", current.getToken());
            currentFeature.setToken(0);
        }
        currentFeature.setScripCode(current.getToken());
        currentFeature.setCompanyName(current.getCompanyName());
        currentFeature.setExchange(current.getExch());
        currentFeature.setExchangeType(current.getExchType());
        currentFeature.setTimestamp(current.getTimestamp());
        
        // Book state
        currentFeature.setBestBid(current.getBestBid());
        currentFeature.setBestAsk(current.getBestAsk());
        currentFeature.setMidPrice(current.getMidPrice());
        currentFeature.setSpread(current.getSpread());
        currentFeature.setMicroprice(current.getMicroprice());
        currentFeature.calculateSpreadBps();
        
        // Volume metrics
        if (!current.getAllBids().isEmpty()) {
            currentFeature.setBidVolume1(current.getAllBids().get(0).getQuantity());
            currentFeature.setBidOrders1(current.getAllBids().get(0).getNumberOfOrders());
        }
        if (!current.getAllAsks().isEmpty()) {
            currentFeature.setAskVolume1(current.getAllAsks().get(0).getQuantity());
            currentFeature.setAskOrders1(current.getAllAsks().get(0).getNumberOfOrders());
        }
        
        currentFeature.setTotalBidVolume5(getTotalVolume(current.getTopBids(5)));
        currentFeature.setTotalAskVolume5(getTotalVolume(current.getTopAsks(5)));
        currentFeature.setTotalBidVolume20(getTotalVolume(current.getTopBids(20)));
        currentFeature.setTotalAskVolume20(getTotalVolume(current.getTopAsks(20)));
        
        // Ensure calculator is initialized (important after deserialization)
        if (calculator == null) {
            calculator = new MicrostructureMetricsCalculator();
        }

        // OFI (need previous snapshot)
        if (orderBookHistory.size() >= 2) {
            OrderBookSnapshot previous = getPreviousSnapshot();
            if (previous != null) {
                currentFeature.setOfi1(calculator.calculateOFI(previous, current, 1));
                currentFeature.setOfi5(calculator.calculateOFI(previous, current, 5));
                currentFeature.setOfi20(calculator.calculateOFI(previous, current, 20));
            }
        }

        // Depth Imbalance
        currentFeature.setDepthImbalance1(calculator.calculateDepthImbalance(current, 1));
        currentFeature.setDepthImbalance5(calculator.calculateDepthImbalance(current, 5));
        currentFeature.setDepthImbalance20(calculator.calculateDepthImbalance(current, 20));

        // VPIN (requires signed volume history)
        currentFeature.setVpin(calculator.calculateVPIN(signedVolumeHistory, minObservations));

        // Kyle's Lambda (requires price change history)
        currentFeature.setKyleLambda(calculator.calculateKyleLambda(priceHistory, signedVolumeHistory, minObservations));
        
        // Effective Spread (simplified)
        currentFeature.setEffectiveSpread(current.getSpread());
        
        log.debug("Calculated features for {}: OFI5={:.2f}, DepthImb5={:.3f}, VPIN={:.4f}",
            current.getCompanyName(),
            currentFeature.getOfi5(),
            currentFeature.getDepthImbalance5(),
            currentFeature.getVpin());
    }
    
    /**
     * Get total volume from list of levels (delegates to calculator)
     */
    private int getTotalVolume(java.util.List<OrderBookSnapshot.OrderBookLevel> levels) {
        if (calculator == null) {
            calculator = new MicrostructureMetricsCalculator();
        }
        return calculator.getTotalVolume(levels);
    }
    
    /**
     * Get previous snapshot from history
     */
    private OrderBookSnapshot getPreviousSnapshot() {
        if (orderBookHistory.size() < 2) {
            return null;
        }
        // Get second-to-last element
        OrderBookSnapshot[] array = orderBookHistory.toArray(new OrderBookSnapshot[0]);
        return array[array.length - 2];
    }
    
    /**
     * Check if features are ready to emit
     */
    public boolean hasFeature() {
        return isReady && currentFeature != null && currentFeature.isValid();
    }
    
    /**
     * Get current feature and reset
     */
    public MicrostructureFeature emitAndReset() {
        MicrostructureFeature featureToEmit = currentFeature;
        isReady = false;
        return featureToEmit;
    }
    
    /**
     * Kafka Serde
     */
    public static org.apache.kafka.common.serialization.Serde<MicrostructureFeatureState> serde() {
        return new org.springframework.kafka.support.serializer.JsonSerde<>(MicrostructureFeatureState.class);
    }
}



--------------------------------------------------
File End
--------------------------------------------------


model/InformationBar.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.Serializer;

/**
 * Information-driven bar (VIB, DIB, TRB, VRB)
 * Based on "Advances in Financial Machine Learning" Chapter 2
 */
@Data
@NoArgsConstructor
@JsonIgnoreProperties(ignoreUnknown = true)
public class InformationBar {
    
    @JsonProperty("barType")
    private String barType;  // "VIB", "DIB", "TRB", "VRB"
    
    @JsonProperty("scripCode")
    private String scripCode;
    
    @JsonProperty("token")
    private int token;
    
    @JsonProperty("exchange")
    private String exchange;
    
    @JsonProperty("companyName")
    private String companyName;
    
    // OHLC
    @JsonProperty("open")
    private double open;
    
    @JsonProperty("high")
    private double high;
    
    @JsonProperty("low")
    private double low;
    
    @JsonProperty("close")
    private double close;
    
    // Volume metrics
    @JsonProperty("volume")
    private int volume;
    
    @JsonProperty("buyVolume")
    private int buyVolume;
    
    @JsonProperty("sellVolume")
    private int sellVolume;
    
    @JsonProperty("dollarVolume")
    private double dollarVolume;
    
    @JsonProperty("buyDollarVolume")
    private double buyDollarVolume;
    
    @JsonProperty("sellDollarVolume")
    private double sellDollarVolume;
    
    // Tick counts
    @JsonProperty("tickCount")
    private int tickCount;
    
    @JsonProperty("buyTicks")
    private int buyTicks;
    
    @JsonProperty("sellTicks")
    private int sellTicks;
    
    // Imbalance metrics
    @JsonProperty("imbalance")
    private double imbalance;  // Absolute imbalance that triggered bar
    
    @JsonProperty("expectedImbalance")
    private double expectedImbalance;  // Expected imbalance threshold
    
    @JsonProperty("imbalanceRatio")
    private double imbalanceRatio;  // actual / expected
    
    // Run metrics
    @JsonProperty("runLength")
    private int runLength;  // Length of run that triggered bar
    
    @JsonProperty("runVolume")
    private double runVolume;  // Volume in run that triggered bar
    
    @JsonProperty("expectedRunLength")
    private double expectedRunLength;  // Expected run threshold
    
    @JsonProperty("runDirection")
    private int runDirection;  // 1 = buy run, -1 = sell run
    
    // Timestamps
    @JsonProperty("windowStartMillis")
    private long windowStartMillis;
    
    @JsonProperty("windowEndMillis")
    private long windowEndMillis;
    
    @JsonProperty("timestamp")
    private long timestamp;
    
    /**
     * Update OHLC with new tick
     */
    public void updateOHLC(TickData tick) {
        double price = tick.getLastRate();
        
        if (open == 0) {
            open = price;
        }
        
        if (price > high || high == 0) {
            high = price;
        }
        
        if (price < low || low == 0) {
            low = price;
        }
        
        close = price;
        timestamp = tick.getTimestamp();
    }
    
    /**
     * Add volume from classified tick
     * Uses deltaVolume (computed by CumToDeltaTransformer)
     */
    public void addVolume(TickData tick, int direction) {
        // CRITICAL: Use deltaVolume, not lastQuantity
        Integer deltaVol = tick.getDeltaVolume();
        if (deltaVol == null || deltaVol == 0) {
            return;  // Skip quote updates (no actual trade)
        }
        
        int qty = deltaVol;
        double dollarValue = tick.getLastRate() * qty;
        
        volume += qty;
        dollarVolume += dollarValue;
        tickCount++;
        
        if (direction > 0) {
            buyVolume += qty;
            buyDollarVolume += dollarValue;
            buyTicks++;
        } else if (direction < 0) {
            sellVolume += qty;
            sellDollarVolume += dollarValue;
            sellTicks++;
        }
    }
    
    /**
     * Calculate imbalance metrics
     */
    public void calculateImbalanceMetrics() {
        if (barType != null && barType.endsWith("IB")) {
            if ("VIB".equals(barType)) {
                imbalance = Math.abs(buyVolume - sellVolume);
            } else if ("DIB".equals(barType)) {
                imbalance = Math.abs(buyDollarVolume - sellDollarVolume);
            }
            
            if (expectedImbalance > 0) {
                imbalanceRatio = imbalance / expectedImbalance;
            }
        }
    }
    
    /**
     * Kafka Serde
     */
    public static Serde<InformationBar> serde() {
        return Serdes.serdeFrom(new InformationBarSerializer(), new InformationBarDeserializer());
    }
    
    public static class InformationBarSerializer implements Serializer<InformationBar> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public byte[] serialize(String topic, InformationBar data) {
            if (data == null) return null;
            try {
                return objectMapper.writeValueAsBytes(data);
            } catch (Exception e) {
                throw new RuntimeException("Serialization failed for InformationBar", e);
            }
        }
    }
    
    public static class InformationBarDeserializer implements Deserializer<InformationBar> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public InformationBar deserialize(String topic, byte[] bytes) {
            if (bytes == null) return null;
            try {
                return objectMapper.readValue(bytes, InformationBar.class);
            } catch (Exception e) {
                throw new RuntimeException("Deserialization failed for InformationBar", e);
            }
        }
    }
}



--------------------------------------------------
File End
--------------------------------------------------


model/ImbalanceBarData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Imbalance bar data (VIB, DIB, TRB, VRB)
 * Real-time progress of information-driven bars
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ImbalanceBarData {
    
    private VolumeImbalanceData volumeImbalance;
    private DollarImbalanceData dollarImbalance;
    private TickRunsData tickRuns;
    private VolumeRunsData volumeRuns;
    
    /**
     * Check if any imbalance bar is complete
     */
    public boolean hasAnyCompleteBar() {
        return (volumeImbalance != null && volumeImbalance.getIsComplete()) ||
               (dollarImbalance != null && dollarImbalance.getIsComplete()) ||
               (tickRuns != null && tickRuns.getIsComplete()) ||
               (volumeRuns != null && volumeRuns.getIsComplete());
    }
    
    /**
     * Get total complete bars count
     */
    public int getCompleteBarsCount() {
        int count = 0;
        if (volumeImbalance != null && volumeImbalance.getIsComplete()) count++;
        if (dollarImbalance != null && dollarImbalance.getIsComplete()) count++;
        if (tickRuns != null && tickRuns.getIsComplete()) count++;
        if (volumeRuns != null && volumeRuns.getIsComplete()) count++;
        return count;
    }

    /**
     * Factory method to create ImbalanceBarData from raw values
     */
    public static ImbalanceBarData create(
        long volumeImb, long dollarImb, int tickRuns, long volumeRuns,
        String direction, double expVolImb, double expDollarImb,
        double expTickRuns, double expVolRuns
    ) {
        VolumeImbalanceData vib = VolumeImbalanceData.builder()
            .cumulative(volumeImb)
            .direction(direction)
            .threshold((long)expVolImb)
            .progress(Math.abs(volumeImb) / expVolImb)
            .isComplete(Math.abs(volumeImb) >= expVolImb)
            .build();

        DollarImbalanceData dib = DollarImbalanceData.builder()
            .cumulative(dollarImb)
            .direction(direction)
            .threshold((long)expDollarImb)
            .progress(Math.abs(dollarImb) / expDollarImb)
            .isComplete(Math.abs(dollarImb) >= expDollarImb)
            .build();

        TickRunsData trb = TickRunsData.builder()
            .currentRun(tickRuns)
            .direction(direction)
            .threshold((int)expTickRuns)
            .progress(Math.abs(tickRuns) / expTickRuns)
            .isComplete(Math.abs(tickRuns) >= expTickRuns)
            .build();

        VolumeRunsData vrb = VolumeRunsData.builder()
            .currentRun(volumeRuns)
            .direction(direction)
            .threshold((long)expVolRuns)
            .progress(Math.abs(volumeRuns) / expVolRuns)
            .isComplete(Math.abs(volumeRuns) >= expVolRuns)
            .build();

        return ImbalanceBarData.builder()
            .volumeImbalance(vib)
            .dollarImbalance(dib)
            .tickRuns(trb)
            .volumeRuns(vrb)
            .build();
    }
}

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
class VolumeImbalanceData {
    private Long cumulative;
    private String direction;
    private Long threshold;
    private Double progress;
    private Boolean isComplete;

    public Double getProgressPercent() {
        return progress != null ? progress * 100 : null;
    }
}

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
class DollarImbalanceData {
    private Long cumulative;
    private String direction;
    private Long threshold;
    private Double progress;
    private Boolean isComplete;

    public Double getProgressPercent() {
        return progress != null ? progress * 100 : null;
    }
}

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
class TickRunsData {
    private Integer currentRun;
    private String direction;
    private Integer threshold;
    private Double progress;
    private Boolean isComplete;

    public Double getProgressPercent() {
        return progress != null ? progress * 100 : null;
    }
}

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
class VolumeRunsData {
    private Long currentRun;
    private String direction;
    private Long threshold;
    private Double progress;
    private Boolean isComplete;

    public Double getProgressPercent() {
        return progress != null ? progress * 100 : null;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/OpenInterestTimeframeData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Open Interest data for a specific timeframe
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class OpenInterestTimeframeData {
    
    private Long oi;
    private Long oiChange;
    private Double oiChangePercent;
    private Double oiMomentum;
    private Double oiConcentration;          // HHI index of OI distribution
    private Boolean isComplete;
    private Long windowStart;
    private Long windowEnd;

    // Put/Call analytics (NEW - for options)
    private Long putOi;                      // Total Put OI
    private Long callOi;                     // Total Call OI
    private Double putCallRatio;             // putOi / callOi
    private Long putOiChange;                // Put OI delta in window
    private Long callOiChange;               // Call OI delta in window
    private Double putCallRatioChange;       // Change in put/call ratio

    // OI vs Volume correlation (NEW)
    private Long volumeInWindow;             // Total volume during this OI window
    private Double oiVolumeCorrelation;      // Correlation between OI change and volume
    
    /**
     * Calculate OI momentum (change per minute)
     */
    public Double getOiMomentumPerMinute() {
        if (oiMomentum == null || windowStart == null || windowEnd == null) return null;
        long windowMinutes = (windowEnd - windowStart) / (60 * 1000);
        return windowMinutes > 0 ? oiMomentum / windowMinutes : null;
    }
    
    /**
     * Check if OI is increasing
     */
    public Boolean isOiIncreasing() {
        return oiChange != null && oiChange > 0;
    }
    
    /**
     * Check if OI is decreasing
     */
    public Boolean isOiDecreasing() {
        return oiChange != null && oiChange < 0;
    }
    
    /**
     * Get OI change strength (absolute value)
     */
    public Long getOiChangeStrength() {
        return oiChange != null ? Math.abs(oiChange) : null;
    }
    
    /**
     * Check if put/call ratio indicates bearish sentiment (PCR > 1.2)
     */
    public Boolean isBearishSentiment() {
        return putCallRatio != null && putCallRatio > 1.2;
    }

    /**
     * Check if put/call ratio indicates bullish sentiment (PCR < 0.8)
     */
    public Boolean isBullishSentiment() {
        return putCallRatio != null && putCallRatio < 0.8;
    }

    /**
     * Check if OI and volume are correlated (both increasing/decreasing)
     */
    public Boolean isOiVolumeCorrelated() {
        return oiVolumeCorrelation != null && Math.abs(oiVolumeCorrelation) > 0.5;
    }

    /**
     * Check if OI is increasing but volume is low (weak signal)
     */
    public Boolean isOiIncreaseLowVolume() {
        if (oiChange == null || volumeInWindow == null) return null;
        return oiChange > 0 && volumeInWindow < 10000;  // Threshold adjustable
    }

    /**
     * Get display string for logging
     */
    public String getDisplayString() {
        if (putCallRatio != null) {
            return String.format("OI[%d,%+d,%.2f%%] PCR:%.2f Put:%d Call:%d %s",
                oi, oiChange, oiChangePercent, putCallRatio, putOi, callOi,
                isComplete != null && isComplete ? "COMPLETE" : "PARTIAL");
        }
        return String.format("OI[%d,%+d,%.2f%%] %s",
            oi, oiChange, oiChangePercent,
            isComplete != null && isComplete ? "COMPLETE" : "PARTIAL");
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/InstrumentCandle.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.springframework.kafka.support.serializer.JsonSerde;

/**
 * Per-Instrument Candle
 * Represents a single candle for ONE specific instrument (equity, future, or option)
 *
 * Key Design: Each scripCode gets its own separate candle
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class InstrumentCandle {

    // Instrument identification
    private String scripCode;                    // Unique instrument identifier
    private String instrumentType;               // EQUITY, FUTURE, OPTION, INDEX
    private String underlyingEquityScripCode;    // For derivatives: maps to underlying
    private String companyName;
    private String exchange;
    private String exchangeType;

    // Derivative-specific fields
    private String expiry;                       // For futures/options
    private Double strikePrice;                  // For options only
    private String optionType;                   // CE or PE (for options only)

    // OHLCV data
    private Double open;
    private Double high;
    private Double low;
    private Double close;
    private Long volume;

    // Buy/Sell volume breakdown
    private Long buyVolume;
    private Long sellVolume;
    private Double volumeDelta;
    private Double volumeDeltaPercent;

    // Advanced metrics
    private Double vwap;
    private Double hlc3;
    private Integer tickCount;

    // Window information
    private Long windowStartMillis;
    private Long windowEndMillis;
    private Boolean isComplete;

    // Timestamp info
    private String humanReadableStartTime;
    private String humanReadableEndTime;

    // Metadata
    private Long processingTimestamp;
    private String timeframe;                    // 1m, 2m, 3m, 5m, 15m, 30m

    // Optional: Per-instrument OI snapshot
    private Long openInterest;
    private Long oiChange;
    private Double oiChangePercent;

    // Optional: Per-instrument orderbook/microstructure snapshots
    private OrderbookDepthData orderbookDepth;
    private MicrostructureData microstructure;
    private ImbalanceBarData imbalanceBars;

    /**
     * Get Kafka Serde for serialization/deserialization
     */
    public static JsonSerde<InstrumentCandle> serde() {
        return new JsonSerde<>(InstrumentCandle.class);
    }

    /**
     * Validate if this candle has valid OHLCV data
     */
    public boolean isValid() {
        return open != null && high != null && low != null && close != null
            && volume != null && volume > 0;
    }

    /**
     * Check if this is a derivative instrument
     */
    public boolean isDerivative() {
        return "D".equalsIgnoreCase(exchangeType);
    }

    /**
     * Check if this is an option
     */
    public boolean isOption() {
        return "OPTION".equalsIgnoreCase(instrumentType);
    }

    /**
     * Check if this is a future
     */
    public boolean isFuture() {
        return "FUTURE".equalsIgnoreCase(instrumentType);
    }

    /**
     * Check if this is an equity
     */
    public boolean isEquity() {
        return "EQUITY".equalsIgnoreCase(instrumentType);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/EnrichedMarketData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.Map;

/**
 * Unified market data message containing all timeframes and features
 * Replaces 19 separate topics with 1 comprehensive message
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class EnrichedMarketData {
    
    // Basic identification
    private String scripCode;
    private String companyName;
    private String exchange;
    private String exchangeType;
    private Long timestamp;
    
    // Instrument family (from cache)
    private InstrumentFamily instrumentFamily;
    
    // Multi-timeframe candles (1m, 2m, 3m, 5m, 15m, 30m)
    private Map<String, CandleData> multiTimeframeCandles;
    
    // Open Interest (all timeframes)
    private Map<String, OpenInterestTimeframeData> openInterest;
    
    // Imbalance bars (real-time progress)
    private ImbalanceBarData imbalanceBars;
    
    // Microstructure features
    private MicrostructureData microstructure;

    // Orderbook depth analytics (NEW)
    private OrderbookDepthData orderbookDepth;

    // Metadata
    private MessageMetadata metadata;
    
    /**
     * Get complete timeframes only
     */
    public Map<String, CandleData> getCompleteCandles() {
        if (multiTimeframeCandles == null) return Map.of();
        
        return multiTimeframeCandles.entrySet().stream()
            .filter(entry -> entry.getValue().getIsComplete())
            .collect(java.util.stream.Collectors.toMap(
                Map.Entry::getKey,
                Map.Entry::getValue
            ));
    }
    
    /**
     * Get partial timeframes only
     */
    public Map<String, CandleData> getPartialCandles() {
        if (multiTimeframeCandles == null) return Map.of();
        
        return multiTimeframeCandles.entrySet().stream()
            .filter(entry -> !entry.getValue().getIsComplete())
            .collect(java.util.stream.Collectors.toMap(
                Map.Entry::getKey,
                Map.Entry::getValue
            ));
    }
    
    /**
     * Check if any timeframe is complete
     */
    public boolean hasAnyCompleteTimeframe() {
        return multiTimeframeCandles != null && 
               multiTimeframeCandles.values().stream()
                   .anyMatch(CandleData::getIsComplete);
    }
    
    /**
     * Get complete timeframes count
     */
    public int getCompleteTimeframesCount() {
        return (int) multiTimeframeCandles.values().stream()
            .mapToInt(candle -> candle.getIsComplete() ? 1 : 0)
            .sum();
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/MicrostructureData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Microstructure features data
 * Order Flow Imbalance, VPIN, Depth Imbalance, etc.
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class MicrostructureData {
    
    private Double ofi;                    // Order Flow Imbalance
    private Double vpin;                   // Volume-Synchronized Probability of Informed Trading
    private Double depthImbalance;         // Depth Imbalance
    private Double kyleLambda;             // Kyle's Lambda
    private Double effectiveSpread;        // Effective Spread
    private Double microprice;             // Microprice
    private Double midPrice;               // Mid-price (bid+ask)/2
    private Double bidAskSpread;           // Bid-Ask spread
    private Boolean isComplete;
    private Long windowStart;
    private Long windowEnd;
    
    /**
     * Check if microstructure data is valid
     */
    public boolean isValid() {
        return ofi != null || vpin != null || depthImbalance != null || 
               kyleLambda != null || effectiveSpread != null || microprice != null;
    }
    
    /**
     * Get OFI strength (absolute value)
     */
    public Double getOfiStrength() {
        return ofi != null ? Math.abs(ofi) : null;
    }
    
    /**
     * Check if OFI is positive (buying pressure)
     */
    public Boolean isOfiPositive() {
        return ofi != null && ofi > 0;
    }
    
    /**
     * Check if OFI is negative (selling pressure)
     */
    public Boolean isOfiNegative() {
        return ofi != null && ofi < 0;
    }
    
    /**
     * Get VPIN level (0-1 scale)
     */
    public String getVpinLevel() {
        if (vpin == null) return "UNKNOWN";
        if (vpin < 0.3) return "LOW";
        if (vpin < 0.6) return "MEDIUM";
        if (vpin < 0.8) return "HIGH";
        return "VERY_HIGH";
    }
    
    /**
     * Check if depth is imbalanced (buy side)
     */
    public Boolean isDepthBuyImbalanced() {
        return depthImbalance != null && depthImbalance > 0.1;
    }
    
    /**
     * Check if depth is imbalanced (sell side)
     */
    public Boolean isDepthSellImbalanced() {
        return depthImbalance != null && depthImbalance < -0.1;
    }
    
    /**
     * Get spread level
     */
    public String getSpreadLevel() {
        if (effectiveSpread == null) return "UNKNOWN";
        if (effectiveSpread < 0.01) return "TIGHT";
        if (effectiveSpread < 0.05) return "NORMAL";
        if (effectiveSpread < 0.1) return "WIDE";
        return "VERY_WIDE";
    }
    
    /**
     * Get display string for logging
     */
    public String getDisplayString() {
        return String.format("Micro[OFI:%.2f,VPIN:%.2f,Depth:%.2f,Spread:%.3f] %s",
            ofi != null ? ofi : 0.0,
            vpin != null ? vpin : 0.0,
            depthImbalance != null ? depthImbalance : 0.0,
            effectiveSpread != null ? effectiveSpread : 0.0,
            isComplete != null && isComplete ? "COMPLETE" : "PARTIAL");
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/CandleData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Candle data for a specific timeframe
 * Contains OHLCV + completion status
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class CandleData {
    
    private Double open;
    private Double high;
    private Double low;
    private Double close;
    private Long volume;
    private Long windowStart;
    private Long windowEnd;
    private Boolean isComplete;
    private String exchange;
    private String exchangeType;

    // Buy/Sell volume separation (NEW)
    private Long buyVolume;              // Volume from buy-side trades
    private Long sellVolume;             // Volume from sell-side trades
    private Double volumeDelta;          // buyVolume - sellVolume
    private Double volumeDeltaPercent;   // (buyVolume - sellVolume) / totalVolume * 100

    // Volume profile (NEW)
    private Double vwap;                 // Volume-weighted average price
    private Integer tickCount;           // Number of ticks in this candle
    
    /**
     * Calculate price change
     */
    public Double getPriceChange() {
        if (open == null || close == null) return null;
        return close - open;
    }
    
    /**
     * Calculate price change percentage
     */
    public Double getPriceChangePercent() {
        if (open == null || close == null || open == 0) return null;
        return ((close - open) / open) * 100;
    }
    
    /**
     * Calculate true range
     */
    public Double getTrueRange() {
        if (high == null || low == null) return null;
        return high - low;
    }
    
    /**
     * Calculate body size
     */
    public Double getBodySize() {
        if (open == null || close == null) return null;
        return Math.abs(close - open);
    }
    
    /**
     * Calculate upper shadow
     */
    public Double getUpperShadow() {
        if (high == null || open == null || close == null) return null;
        return high - Math.max(open, close);
    }
    
    /**
     * Calculate lower shadow
     */
    public Double getLowerShadow() {
        if (low == null || open == null || close == null) return null;
        return Math.min(open, close) - low;
    }
    
    /**
     * Check if candle is bullish
     */
    public Boolean isBullish() {
        if (open == null || close == null) return null;
        return close > open;
    }
    
    /**
     * Check if candle is bearish
     */
    public Boolean isBearish() {
        if (open == null || close == null) return null;
        return close < open;
    }
    
    /**
     * Check if candle is doji (small body)
     */
    public Boolean isDoji() {
        if (open == null || close == null) return null;
        return Math.abs(close - open) < (getTrueRange() * 0.1); // Body < 10% of range
    }
    
    /**
     * Get window duration in minutes
     */
    public Long getWindowDurationMinutes() {
        if (windowStart == null || windowEnd == null) return null;
        return (windowEnd - windowStart) / (60 * 1000);
    }
    
    /**
     * Check if there's buying pressure (buy volume > sell volume)
     */
    public Boolean hasBuyingPressure() {
        if (buyVolume == null || sellVolume == null) return null;
        return buyVolume > sellVolume;
    }

    /**
     * Check if there's selling pressure (sell volume > buy volume)
     */
    public Boolean hasSellingPressure() {
        if (buyVolume == null || sellVolume == null) return null;
        return sellVolume > buyVolume;
    }

    /**
     * Check for volume delta divergence (price up but volume delta down)
     */
    public Boolean hasVolumeDeltaDivergence() {
        if (isBullish() == null || volumeDelta == null) return null;
        return isBullish() && volumeDelta < 0;  // Price up but sells dominate
    }

    /**
     * Get buy/sell volume ratio
     */
    public Double getBuySellRatio() {
        if (buyVolume == null || sellVolume == null || sellVolume == 0) return null;
        return (double) buyVolume / sellVolume;
    }

    /**
     * Get display string for logging
     */
    public String getDisplayString() {
        if (buyVolume != null && sellVolume != null) {
            return String.format("OHLCV[%.2f,%.2f,%.2f,%.2f,%d] BuyVol:%d SellVol:%d Delta:%.1f%% %s",
                open, high, low, close, volume,
                buyVolume, sellVolume, volumeDeltaPercent,
                isComplete != null && isComplete ? "COMPLETE" : "PARTIAL");
        }
        return String.format("OHLCV[%.2f,%.2f,%.2f,%.2f,%d] %s",
            open, high, low, close, volume,
            isComplete != null && isComplete ? "COMPLETE" : "PARTIAL");
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/OpenInterest.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonAlias;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Open Interest data model
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
@JsonIgnoreProperties(ignoreUnknown = true)
public class OpenInterest {
    
    @JsonProperty("Exch")
    @JsonAlias({"exchange"})
    private String exchange;

    @JsonProperty("ExchType")
    @JsonAlias({"exchangeType"})
    private String exchangeType;

    @JsonProperty("Token")
    @JsonAlias({"token"})
    private int token;

    @JsonProperty("OpenInterest")
    @JsonAlias({"openInterest"})
    private Long openInterest;

    // Optional fields (may not be present in producer payload)
    @JsonAlias({"oiChange"})
    private Long oiChange;
    @JsonAlias({"oiChangePercent"})
    private Double oiChangePercent;
    @JsonAlias({"lastRate"})
    private Double lastRate;
    @JsonAlias({"volume"})
    private Long volume;

    @JsonProperty("companyName")
    private String companyName;

    @JsonProperty("receivedTimestamp")
    private Long receivedTimestamp;
    
    /**
     * Create serde for Kafka Streams
     */
    public static org.apache.kafka.common.serialization.Serde<OpenInterest> serde() {
        return new org.springframework.kafka.support.serializer.JsonSerde<>(OpenInterest.class);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/Candlestick.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.Data;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.Serializer;

import java.text.SimpleDateFormat;
import java.time.Instant;
import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Date;

/**
 * Represents a market data candlestick (OHLC + volume) used for technical analysis.
 * <p>
 * This class stores:
 * - Price data (open, high, low, close)
 * - Volume data
 * - Metadata (exchange, symbol, etc.)
 * - Window timing information (for debugging and data analysis)
 * <p>
 * The candlestick can be built either:
 * - Directly from raw TickData (for 1-minute candles)
 * - By aggregating smaller timeframe candles (for multi-minute candles)
 */
@Data
public class Candlestick {

    // Price data
    private double open;
    private double high;
    private double low;
    private double close;
    private int volume;

    // Metadata
    private String exchange;
    private String exchangeType;
    private String companyName;
    private String scripCode;

    // Window timing information (in epoch millis)
    private long windowStartMillis;
    private long windowEndMillis;

    // Human-readable window timestamps
    private String humanReadableStartTime;
    private String humanReadableEndTime;

    // Validation fields (added to match JSON structure being deserialized)
    private Boolean validCandle;
    private String validationIssues;

    // Completeness flag (CRITICAL for finalized candles)
    private Boolean isComplete = false;

    // Optional extra fields (enabled via feature flag)
    private Double vwap;          // Volume-weighted average price
    private Double hlc3;          // (High + Low + Close) / 3
    private Double logReturnFromPrevBar;  // Log return from previous candle
    private Integer ticksInWindow;  // Number of ticks in this window
    private Long windowLatencyMs;   // Processing latency

    // Transient field for processing
    @JsonIgnore
    private transient long alignedWindowStart;

    @JsonIgnore
    private long firstTs = Long.MAX_VALUE;
    @JsonIgnore
    private long lastTs = Long.MIN_VALUE;

    /**
     * Creates a new empty candlestick with default values.
     */
    public Candlestick() {
        this.open = 0;
        this.high = Double.MIN_VALUE;
        this.low = Double.MAX_VALUE;
        this.close = 0;
        this.volume = 0;

        // Initialize validation fields
        this.validCandle = null;
        this.validationIssues = null;
    }


    /**
     * Update using event-time and delta volume (for raw TickData -> 1m).
     * Deterministic: open = price at min(event_ts), close = price at max(event_ts).
     */
    public void updateWithDelta(TickData tick) {
        long ts = tick.getTimestamp();
        double px = tick.getLastRate();

        // Open/Close by event time (not arrival order)
        if (ts < firstTs) {
            firstTs = ts;
            open = px;
        }
        if (ts >= lastTs) {
            lastTs = ts;
            close = px;
        }

        // High/Low with proper initialization
        high = (high == Double.MIN_VALUE) ? px : Math.max(high, px);
        low = (low == Double.MAX_VALUE) ? px : Math.min(low, px);

        // Sum delta volume (0 if null)
        Integer dv = tick.getDeltaVolume();
        volume += (dv == null ? 0 : dv);

        // Metadata (idempotent)
        if (companyName == null) companyName = tick.getCompanyName();
        if (scripCode == null) scripCode = tick.getScripCode();
        exchange = tick.getExchange();
        if (exchangeType == null) exchangeType = tick.getExchangeType();
    }

    /**
     * Merges another Candlestick into this one.
     * Used when building multi-minute candles from smaller timeframe candles.
     *
     * @param other The candle to merge into this one
     */

    public void updateCandle(Candlestick other) {
        // Reset validation since data is changing
        this.validCandle = null;
        this.validationIssues = null;

        // Set open price only for the first candle in the window
        if (this.open == 0) {
            this.open = other.open;
        }

        // Take highest high and lowest low
        this.high = Math.max(this.high, other.high);
        this.low = Math.min(this.low, other.low);

        // Always update close to the latest candle's close
        this.close = other.close;

        // Accumulate volume
        this.volume += other.volume;

        // Update metadata
        this.exchange = other.exchange;

        // Handle exchangeType - ensure it's not null
        if (other.exchangeType != null) {
            this.exchangeType = other.exchangeType;
        } else if (this.exchangeType == null) {
            // If both are null, set a default based on exchange
            if ("N".equals(other.exchange)) {
                this.exchangeType = "EQUITY"; // Default for NSE
            } else if ("M".equals(other.exchange)) {
                this.exchangeType = "COMMODITY"; // Default for MCX
            } else {
                this.exchangeType = "UNKNOWN";
            }
        }

        this.companyName = other.companyName;
        this.scripCode = other.scripCode;
    }

    /**
     * Updates the human-readable timestamps based on windowStartMillis and windowEndMillis
     * with improved formatting and alignment to ensure exact minute boundaries
     */
    public void updateHumanReadableTimestamps() {
        if (windowStartMillis > 0) {
            ZonedDateTime startTime = ZonedDateTime.ofInstant(
                    Instant.ofEpochMilli(windowStartMillis),
                    ZoneId.of("Asia/Kolkata")
            );

            // Ensure alignment to exact minute boundaries
            startTime = startTime.withSecond(0).withNano(0);

            this.humanReadableStartTime = startTime.format(
                    DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")
            );
        }

        if (windowEndMillis > 0) {
            ZonedDateTime endTime = ZonedDateTime.ofInstant(
                    Instant.ofEpochMilli(windowEndMillis),
                    ZoneId.of("Asia/Kolkata")
            );

            // Ensure alignment to exact minute boundaries
            endTime = endTime.withSecond(0).withNano(0);

            this.humanReadableEndTime = endTime.format(
                    DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")
            );
        }
    }

    /**
     * Sets the window start time in milliseconds and updates the human-readable representation.
     *
     * @param windowStartMillis The window start time in epoch milliseconds
     */
    public void setWindowStartMillis(long windowStartMillis) {
        this.windowStartMillis = windowStartMillis;
        updateHumanReadableTimestamps();
    }

    /**
     * Sets the window end time in milliseconds and updates the human-readable representation.
     *
     * @param windowEndMillis The window end time in epoch milliseconds
     */
    public void setWindowEndMillis(long windowEndMillis) {
        this.windowEndMillis = windowEndMillis;
        updateHumanReadableTimestamps();
    }


    /**
     * Provides a Kafka Serde for Candlestick.
     */
    public static Serde<Candlestick> serde() {
        return Serdes.serdeFrom(new CandlestickSerializer(), new CandlestickDeserializer());
    }

    // ---------------------------------------------------
    // Internal Serializer/Deserializer
    // ---------------------------------------------------
    public static class CandlestickSerializer implements Serializer<Candlestick> {
        private final ObjectMapper objectMapper = new ObjectMapper();

        @Override
        public byte[] serialize(String topic, Candlestick data) {
            if (data == null) return null;
            try {
                return objectMapper.writeValueAsBytes(data);
            } catch (Exception e) {
                throw new RuntimeException("Serialization failed for Candlestick", e);
            }
        }
    }

    public static class CandlestickDeserializer implements Deserializer<Candlestick> {
        private final ObjectMapper objectMapper = new ObjectMapper();

        @Override
        public Candlestick deserialize(String topic, byte[] bytes) {
            if (bytes == null) return null;
            try {
                return objectMapper.readValue(bytes, Candlestick.class);
            } catch (Exception e) {
                throw new RuntimeException("Deserialization failed for Candlestick", e);
            }
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/OrderBookSnapshot.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonAlias;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.ArrayList;
import java.util.List;
import java.util.stream.Collectors;

/**
 * Order Book Snapshot model
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
@JsonIgnoreProperties(ignoreUnknown = true)
public class OrderBookSnapshot {

    @JsonProperty("Exch")
    @JsonAlias({"exchange"})
    private String exchange;

    @JsonProperty("ExchType")
    @JsonAlias({"exchangeType"})
    private String exchangeType;

    @JsonProperty("Token")
    @JsonAlias({"token"})
    private String token;

    // Backward-compat arrays (may be absent)
    @JsonAlias({"bidRate"})
    private List<Double> bidRate;
    @JsonAlias({"bidQty"})
    private List<Long> bidQty;
    @JsonAlias({"offRate","askRate"})
    private List<Double> offRate;
    @JsonAlias({"offQty","askQty"})
    private List<Long> offQty;

    @JsonProperty("TBidQ")
    @JsonAlias({"totalBidQty"})
    private Long totalBidQty;

    @JsonProperty("TOffQ")
    @JsonAlias({"totalOffQty"})
    private Long totalOffQty;

    @JsonProperty("companyName")
    private String companyName;

    @JsonProperty("receivedTimestamp")
    private Long receivedTimestamp;

    // NEW: Object array format (bids/asks from new producer)
    @JsonProperty("bids")
    private List<OrderBookLevel> bids;
    
    @JsonProperty("asks")
    private List<OrderBookLevel> asks;

    // Parsed details (unified view - combines both formats)
    private List<OrderBookLevel> allBids;
    private List<OrderBookLevel> allAsks;

    /**
     * Order Book Level - represents a single price level in the order book
     */
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class OrderBookLevel {
        @JsonProperty("Price")
        private double price;
        @JsonProperty("Quantity")
        private int quantity;
        @JsonProperty("NumberOfOrders")
        private int numberOfOrders;
        // Flags in producer (ignored for now): bid/ask booleans
    }

    /**
     * Parse raw bid/ask data into OrderBookLevel objects
     * Handles BOTH formats:
     * 1. NEW: bids/asks arrays of objects (with Price, Quantity, NumberOfOrders)
     * 2. OLD: bidRate/bidQty/offRate/offQty separate arrays
     */
    public void parseDetails() {
        // Priority 1: Use new format (bids/asks objects) if available
        if (bids != null && !bids.isEmpty()) {
            allBids = new ArrayList<>(bids);
        } 
        // Fallback: Use old format (bidRate/bidQty arrays)
        else if (allBids == null && bidRate != null && bidQty != null) {
            allBids = new ArrayList<>();
            int minSize = Math.min(bidRate.size(), bidQty.size());
            for (int i = 0; i < minSize; i++) {
                allBids.add(OrderBookLevel.builder()
                    .price(bidRate.get(i))
                    .quantity(bidQty.get(i).intValue())
                    .numberOfOrders(1) // Default to 1 as we don't have order count in raw data
                    .build());
            }
        }
        
        // Priority 1: Use new format (asks objects) if available
        if (asks != null && !asks.isEmpty()) {
            allAsks = new ArrayList<>(asks);
        }
        // Fallback: Use old format (offRate/offQty arrays)
        else if (allAsks == null && offRate != null && offQty != null) {
            allAsks = new ArrayList<>();
            int minSize = Math.min(offRate.size(), offQty.size());
            for (int i = 0; i < minSize; i++) {
                allAsks.add(OrderBookLevel.builder()
                    .price(offRate.get(i))
                    .quantity(offQty.get(i).intValue())
                    .numberOfOrders(1) // Default to 1 as we don't have order count in raw data
                    .build());
            }
        }
    }

    /**
     * Get top N bid levels
     */
    public List<OrderBookLevel> getTopBids(int n) {
        if (allBids == null || allBids.isEmpty()) {
            return new ArrayList<>();
        }
        return allBids.stream()
            .limit(n)
            .collect(Collectors.toList());
    }

    /**
     * Get top N ask levels
     */
    public List<OrderBookLevel> getTopAsks(int n) {
        if (allAsks == null || allAsks.isEmpty()) {
            return new ArrayList<>();
        }
        return allAsks.stream()
            .limit(n)
            .collect(Collectors.toList());
    }

    /**
     * Get best bid price
     */
    public double getBestBid() {
        if (allBids != null && !allBids.isEmpty()) {
            return allBids.get(0).getPrice();
        }
        return bidRate != null && !bidRate.isEmpty() ? bidRate.get(0) : 0.0;
    }

    /**
     * Get best ask price
     */
    public double getBestAsk() {
        if (allAsks != null && !allAsks.isEmpty()) {
            return allAsks.get(0).getPrice();
        }
        return offRate != null && !offRate.isEmpty() ? offRate.get(0) : 0.0;
    }

    /**
     * Get spread (ask - bid)
     */
    public double getSpread() {
        return getBestAsk() - getBestBid();
    }

    /**
     * Get microprice (weighted mid price)
     */
    public double getMicroprice() {
        double bestBid = getBestBid();
        double bestAsk = getBestAsk();

        if (bestBid == 0 || bestAsk == 0) {
            return 0.0;
        }

        int bidQty = allBids != null && !allBids.isEmpty() ? allBids.get(0).getQuantity() : 0;
        int askQty = allAsks != null && !allAsks.isEmpty() ? allAsks.get(0).getQuantity() : 0;

        if (bidQty + askQty == 0) {
            return (bestBid + bestAsk) / 2.0;
        }

        return (bestBid * askQty + bestAsk * bidQty) / (bidQty + askQty);
    }

    /**
     * Get mid price (simple average of best bid and ask)
     */
    public double getMidPrice() {
        double bestBid = getBestBid();
        double bestAsk = getBestAsk();
        
        if (bestBid == 0 && bestAsk == 0) {
            return 0.0;
        }
        
        return (bestBid + bestAsk) / 2.0;
    }

    /**
     * Get timestamp (using receivedTimestamp)
     */
    public long getTimestamp() {
        return receivedTimestamp != null ? receivedTimestamp : 0L;
    }

    /**
     * Get exchange (alias for getExchange for compatibility)
     */
    public String getExch() {
        return exchange;
    }

    /**
     * Get exchange type (alias for getExchangeType for compatibility)
     */
    public String getExchType() {
        return exchangeType;
    }

    /**
     * Check if this snapshot is valid
     * Supports BOTH old and new formats
     */
    public boolean isValid() {
        if (token == null || token.isEmpty()) {
            return false;
        }
        
        // Check new format (bids/asks objects)
        boolean hasNewBids = bids != null && !bids.isEmpty();
        boolean hasNewAsks = asks != null && !asks.isEmpty();
        
        // Check old format (bidRate/bidQty arrays)
        boolean hasOldBids = bidRate != null && !bidRate.isEmpty() && bidQty != null && !bidQty.isEmpty();
        boolean hasOldAsks = offRate != null && !offRate.isEmpty() && offQty != null && !offQty.isEmpty();
        
        boolean hasLevels = hasNewBids || hasNewAsks || hasOldBids || hasOldAsks;
        boolean hasTotals = (totalBidQty != null && totalBidQty > 0) || (totalOffQty != null && totalOffQty > 0);
        
        if (!hasLevels && !hasTotals) { return false; }
        if (receivedTimestamp == null || receivedTimestamp <= 0) { return false; }
        return true;
    }

    /**
     * True if we have any bid/ask price levels present
     * Supports BOTH old and new formats
     */
    public boolean hasBookLevels() {
        // Check new format (bids/asks objects)
        boolean hasNewBids = bids != null && !bids.isEmpty();
        boolean hasNewAsks = asks != null && !asks.isEmpty();
        
        // Check old format (bidRate/bidQty arrays)
        boolean hasOldBids = bidRate != null && !bidRate.isEmpty() && bidQty != null && !bidQty.isEmpty();
        boolean hasOldAsks = offRate != null && !offRate.isEmpty() && offQty != null && !offQty.isEmpty();
        
        return hasNewBids || hasNewAsks || hasOldBids || hasOldAsks;
    }

    /**
     * Create serde for Kafka Streams
     */
    public static org.apache.kafka.common.serialization.Serde<OrderBookSnapshot> serde() {
        return new org.springframework.kafka.support.serializer.JsonSerde<>(OrderBookSnapshot.class);
    }
}

--------------------------------------------------
File End
--------------------------------------------------


model/FamilyEnrichedData.java
File type: .java
package com.kotsin.consumer.model;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.springframework.kafka.support.serializer.JsonSerde;

import java.util.ArrayList;
import java.util.List;

/**
 * Family-Level Enriched Data
 * Contains ALL instruments (equity + futures + options) in a single family
 *
 * Key Design: All instruments grouped by underlying equity scripCode
 * Used by strategies to analyze entire instrument family together
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class FamilyEnrichedData {

    // Family identification
    private String familyKey;                    // Underlying equity scripCode (or index for NIFTY/BANKNIFTY)
    private String familyName;                   // Company name or index name
    private String instrumentType;               // EQUITY_FAMILY, INDEX_FAMILY

    // Window information
    private Long windowStartMillis;
    private Long windowEndMillis;
    private String timeframe;                    // 1m, 2m, 3m, 5m, 15m, 30m

    // Equity data (null for index families)
    private InstrumentCandle equity;

    // Futures data (list of all active futures contracts)
    @Builder.Default
    private List<InstrumentCandle> futures = new ArrayList<>();

    // Options data (list of all active options)
    @Builder.Default
    private List<InstrumentCandle> options = new ArrayList<>();

    // Aggregated metrics across all instruments
    private FamilyAggregatedMetrics aggregatedMetrics;

    // Microstructure and orderbook data (aggregated across family)
    private MicrostructureData microstructure;
    private OrderbookDepthData orderbookDepth;
    private ImbalanceBarData imbalanceBars;

    // Metadata
    private Long processingTimestamp;
    private Integer totalInstrumentsCount;
    private String dataQuality;                  // HIGH, MEDIUM, LOW

    /**
     * Get Kafka Serde for serialization/deserialization
     */
    public static JsonSerde<FamilyEnrichedData> serde() {
        return new JsonSerde<>(FamilyEnrichedData.class);
    }

    /**
     * Check if this family has equity data
     */
    public boolean hasEquity() {
        return equity != null && equity.isValid();
    }

    /**
     * Check if this family has any futures
     */
    public boolean hasFutures() {
        return futures != null && !futures.isEmpty();
    }

    /**
     * Check if this family has any options
     */
    public boolean hasOptions() {
        return options != null && !options.isEmpty();
    }

    /**
     * Get near-month future (nearest expiry)
     */
    public InstrumentCandle getNearMonthFuture() {
        if (futures == null || futures.isEmpty()) {
            return null;
        }

        // Return future with earliest expiry
        return futures.stream()
            .filter(f -> f.getExpiry() != null)
            .min((f1, f2) -> f1.getExpiry().compareTo(f2.getExpiry()))
            .orElse(futures.get(0));
    }

    /**
     * Get ATM options (around spot price)
     */
    public List<InstrumentCandle> getAtmOptions(double spotPrice, int strikeRange) {
        if (options == null || options.isEmpty()) {
            return new ArrayList<>();
        }

        List<InstrumentCandle> atmOptions = new ArrayList<>();
        for (InstrumentCandle option : options) {
            if (option.getStrikePrice() != null) {
                double diff = Math.abs(option.getStrikePrice() - spotPrice);
                if (diff <= strikeRange * 50) {  // Assuming 50 point strike intervals
                    atmOptions.add(option);
                }
            }
        }
        return atmOptions;
    }

    /**
     * Count instruments by type
     */
    public int getFuturesCount() {
        return futures != null ? futures.size() : 0;
    }

    public int getOptionsCount() {
        return options != null ? options.size() : 0;
    }

    /**
     * Calculate total family count
     */
    public int calculateTotalCount() {
        int count = 0;
        if (hasEquity()) count++;
        count += getFuturesCount();
        count += getOptionsCount();
        return count;
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/TickData.java
File type: .java
package com.kotsin.consumer.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.Data;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.Serializer;

/**
 * Represents tick data received from the market.
 */
@Data
@JsonIgnoreProperties(ignoreUnknown = true)
public class TickData {

    @JsonProperty("Exch")
    private String exchange;

    @JsonProperty("ExchType")
    private String exchangeType;

    @JsonProperty("Token")
    private int token;

    @JsonProperty("ScripCode")
    private String scripCode;

    @JsonProperty("LastRate")
    private double lastRate;

    @JsonProperty("LastQty")
    private int lastQuantity;

    @JsonProperty("TotalQty")
    private int totalQuantity;

    @JsonProperty("High")
    private double high;

    @JsonProperty("Low")
    private double low;

    @JsonProperty("OpenRate")
    private double openRate;

    @JsonProperty("PClose")
    private double previousClose;

    @JsonProperty("AvgRate")
    private double averageRate;

    @JsonProperty("Time")
    private long time;

    @JsonProperty("BidQty")
    private int bidQuantity;

    @JsonProperty("BidRate")
    private double bidRate;

    @JsonProperty("OffQty")
    private int offerQuantity;

    @JsonProperty("OffRate")
    private double offerRate;

    @JsonProperty("TBidQ")
    private int totalBidQuantity;

    @JsonProperty("TOffQ")
    private int totalOfferQuantity;

    @JsonProperty("TickDt")
    private String tickDt;

    @JsonProperty("ChgPcnt")
    private double changePercent;

    @JsonProperty("companyName")
    private String companyName;

    private long timestamp;

    @JsonProperty("DeltaQty")        // internal field used inside streams topology
    private Integer deltaVolume;     // null until we compute the delta
    
    @JsonProperty("ResetFlag")       // internal flag to indicate cumulative volume reset
    private Boolean resetFlag;       // true when volume reset detected (day rollover or feed restart)
    
    // Additional fields for unified processing
    private Long openInterest;
    private Long oiChange;

    // Transient field for full orderbook (not serialized, used during processing)
    private transient OrderBookSnapshot fullOrderbook;

    /**
     * Parses timestamp from TickDt field.
     * CRITICAL: Never uses System.currentTimeMillis() to handle lag correctly.
     */
    public void parseTimestamp() {
        if (tickDt != null && tickDt.startsWith("/Date(")) {
            try {
                // Extract the milliseconds value from the "/Date(1234567890000)/" format
                this.timestamp = Long.parseLong(tickDt.replaceAll("[^0-9]", ""));
            } catch (NumberFormatException e) {
                // CRITICAL: Set to 0 (not current time) and let TimestampExtractor handle it
                this.timestamp = 0;
                System.err.println("Failed to parse TickDt: " + tickDt + ". Timestamp set to 0.");
            }
        }
        // REMOVED: else clause that was setting System.currentTimeMillis()
    }

    /**
     * Get scrip code, using token as fallback if not available.
     * This ensures backward compatibility with existing code.
     */
    public String getScripCode() {
        if (scripCode != null && !scripCode.isEmpty()) {
            return scripCode;
        }
        // Fallback to string representation of token
        return String.valueOf(token);
    }

    /**
     * Provides Kafka Serde for TickData.
     */
    public static Serde<TickData> serde() {
        return Serdes.serdeFrom(new TickDataSerializer(), new TickDataDeserializer());
    }

    // ---------------------------------------------------
    // Internal Serializer/Deserializer
    // ---------------------------------------------------
    public static class TickDataSerializer implements Serializer<TickData> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public byte[] serialize(String topic, TickData data) {
            if (data == null) return null;
            try {
                return objectMapper.writeValueAsBytes(data);
            } catch (Exception e) {
                throw new RuntimeException("Serialization failed for TickData", e);
            }
        }
    }

    public static class TickDataDeserializer implements Deserializer<TickData> {
        private final ObjectMapper objectMapper = new ObjectMapper();
        @Override
        public TickData deserialize(String topic, byte[] bytes) {
            if (bytes == null) return null;
            try {
                TickData data = objectMapper.readValue(bytes, TickData.class);
                data.parseTimestamp();
                return data;
            } catch (Exception e) {
                throw new RuntimeException("Deserialization failed for TickData", e);
            }
        }
    }
}


--------------------------------------------------
File End
--------------------------------------------------


model/RunsBarState.java
File type: .java
package com.kotsin.consumer.model;

import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import java.io.Serializable;

/**
 * CRITICAL FIX: Completely rewritten State tracker for TRB/VRB
 * 
 * Based on "Advances in Financial Machine Learning" Chapter 2.3.2.3-2.3.2.4
 * 
 * ALL BUGS FIXED:
 * 1. ‚úÖ Serializable for Kafka state stores
 * 2. ‚úÖ All configuration externalized
 * 3. ‚úÖ Fixed division by zero
 * 4. ‚úÖ Proper EWMA initialization
 * 5. ‚úÖ Improved tick classification
 * 6. ‚úÖ Input validation everywhere
 * 7. ‚úÖ Methods for processor
 */
@Data
@NoArgsConstructor
@Slf4j
public class RunsBarState implements Serializable {
    
    private static final long serialVersionUID = 1L;
    
    // Configuration (injected from application.properties)
    private double ewmaSpan = 100.0;
    private double expectedBarSize = 50.0;
    private int warmupSamples = 20;
    private double thresholdMultiplier = 1.0;  // CRITICAL: Multiplier for multi-granularity (1x, 2x, 5x)
    
    // Current bar being constructed
    private InformationBar currentBar = new InformationBar();
    private boolean isComplete = false;
    
    // State for tick classification
    private double previousPrice = 0.0;
    private int previousDirection = 1;  // 1=buy, -1=sell
    
    // EWMA state for expected run length
    private double ewmaBuyProbability = 0.5;
    private double ewmaBuyVolume = 0.0;
    private double ewmaSellVolume = 0.0;
    
    // Current run tracking
    private int currentBuyRun = 0;
    private int currentSellRun = 0;
    private double currentBuyVolumeRun = 0.0;
    private double currentSellVolumeRun = 0.0;
    private int currentRunDirection = 0;  // Direction of current run
    
    private int sampleCount = 0;
    
    /**
     * Constructor with configuration
     * 
     * @param thresholdMultiplier - Like 1min vs 5min for time bars
     *                            - 1.0 = sensitive (high frequency)
     *                            - 2.0 = medium
     *                            - 5.0 = stable (low frequency, only major runs)
     */
    public RunsBarState(double ewmaSpan, double expectedBarSize, int warmupSamples, double thresholdMultiplier) {
        this.ewmaSpan = ewmaSpan;
        this.expectedBarSize = expectedBarSize;
        this.warmupSamples = warmupSamples;
        this.thresholdMultiplier = thresholdMultiplier;
        this.currentBar = new InformationBar();
        this.isComplete = false;
        this.previousDirection = 1;
    }
    
    /**
     * CRITICAL FIX: Classify tick using proper tick rule (Chapter 19)
     */
    public int classifyTick(TickData tick) {
        if (tick == null) {
            log.warn("Null tick in classifyTick");
            return previousDirection;
        }
        
        double price = tick.getLastRate();
        
        if (price <= 0 || Double.isNaN(price) || Double.isInfinite(price)) {
            log.warn("Invalid price {} in classifyTick", price);
            return previousDirection;
        }
        
        // First tick: use bid/ask midpoint
        if (previousPrice == 0) {
            double bid = tick.getBidRate();
            double ask = tick.getOfferRate();
            
            if (bid > 0 && ask > 0 && ask > bid) {
                double mid = (bid + ask) / 2.0;
                previousDirection = price >= mid ? 1 : -1;
            } else {
                previousDirection = 1;
            }
            previousPrice = price;
            return previousDirection;
        }
        
        // Tick rule
        if (price > previousPrice) {
            previousDirection = 1;
        } else if (price < previousPrice) {
            previousDirection = -1;
        }
        
        previousPrice = price;
        return previousDirection;
    }
    
    /**
     * Add tick to current bar
     */
    public void addTick(TickData tick, String barType) {
        try {
            if (tick == null) {
                log.warn("[{}] Null tick, skipping", barType);
                return;
            }
            
            Integer deltaVol = tick.getDeltaVolume();
            if (deltaVol == null || deltaVol <= 0) {
                return;
            }
            
            // Initialize bar on first tick
            if (currentBar.getScripCode() == null) {
                currentBar.setBarType(barType);
                currentBar.setScripCode(tick.getScripCode());
                currentBar.setToken(tick.getToken());
                currentBar.setExchange(tick.getExchange());
                currentBar.setCompanyName(tick.getCompanyName());
            }
            
            // CRITICAL FIX: Always set start timestamp if it's 0 (handles state restoration)
            if (currentBar.getWindowStartMillis() == 0) {
                long ts = tick.getTimestamp();
                currentBar.setWindowStartMillis(ts);
                log.debug("[{}] Set window start timestamp: {} for token {}", 
                    barType, ts, tick.getToken());
            }
            
            int direction = classifyTick(tick);
            currentBar.updateOHLC(tick);
            currentBar.addVolume(tick, direction);
            currentBar.setWindowEndMillis(tick.getTimestamp());
            
            // Update run tracking
            updateRunTracking(direction, deltaVol);
            
            // Check if bar should be emitted
            if (barType.equals("TRB")) {
                if (shouldEmitTickRunsBar()) {
                    completeBar();
                }
            } else if (barType.equals("VRB")) {
                if (shouldEmitVolumeRunsBar()) {
                    completeBar();
                }
            }
            
        } catch (Exception e) {
            log.error("[{}] Error adding tick: {}", barType, e.getMessage(), e);
        }
    }
    
    /**
     * CRITICAL: Update run tracking
     * 
     * From Chapter 2.3.2.3:
     * Œ∏_T = max{Œ£(b_t where b_t=1), -Œ£(b_t where b_t=-1)}
     * 
     * Count ticks in longest run (no offset)
     */
    private void updateRunTracking(int direction, int volume) {
        if (currentRunDirection == 0) {
            // Start first run
            currentRunDirection = direction;
        }
        
        if (direction == 1) {
            // Buy tick
            currentBuyRun++;
            currentBuyVolumeRun += volume;
        } else {
            // Sell tick
            currentSellRun++;
            currentSellVolumeRun += volume;
        }
        
        // Store in bar for later
        currentBar.setRunLength(Math.max(currentBuyRun, currentSellRun));
        currentBar.setRunVolume(Math.max(currentBuyVolumeRun, currentSellVolumeRun));
    }
    
    /**
     * CRITICAL FIX: Calculate expected tick run length
     * 
     * From Chapter 2.3.2.3:
     * E[Œ∏_T] = E[T] * max{P[buy], P[sell]}
     */
    public double getExpectedTickRunLength() {
        if (sampleCount < warmupSamples) {
            return Math.max(expectedBarSize * 0.5, 10.0);  // Min 10 ticks
        }
        
        // Expected run = bar size * probability of dominant side
        double maxProb = Math.max(ewmaBuyProbability, 1 - ewmaBuyProbability);
        return expectedBarSize * maxProb;
    }
    
    /**
     * CRITICAL FIX: Calculate expected volume run length
     */
    public double getExpectedVolumeRunLength() {
        if (sampleCount < warmupSamples) {
            double avgVol = (ewmaBuyVolume + ewmaSellVolume);
            if (avgVol == 0) {
                return expectedBarSize * 10.0;  // Assume 10 volume per tick
            }
            return Math.max(avgVol * 0.5, 100.0);
        }
        
        // Expected volume run = volume * probability of dominant side
        double maxVol = Math.max(ewmaBuyVolume, ewmaSellVolume);
        return Math.max(maxVol, expectedBarSize * 10.0);
    }
    
    /**
     * Check if TRB bar should be emitted
     * 
     * CRITICAL: Uses threshold multiplier for multi-granularity
     * FIXED: Always set expectedRunLength BEFORE checking threshold
     */
    public boolean shouldEmitTickRunsBar() {
        if (currentBar.getTickCount() < 10) {
            return false;
        }
        
        int actualRunLength = currentBar.getRunLength();
        double expectedRunLength = getExpectedTickRunLength() * thresholdMultiplier;  // CRITICAL: Apply multiplier!
        
        // CRITICAL FIX: Always set before checking (so it's in emitted bar)
        currentBar.setExpectedRunLength(expectedRunLength);
        currentBar.setRunDirection(currentRunDirection);  // Also set run direction
        
        log.debug("[TRB] actual={}, expected={:.2f}, threshold={:.1f}x, willEmit={}", 
            actualRunLength, expectedRunLength, thresholdMultiplier, actualRunLength >= expectedRunLength);
        
        return actualRunLength >= expectedRunLength;
    }
    
    /**
     * Check if VRB bar should be emitted
     * 
     * CRITICAL: Uses threshold multiplier for multi-granularity
     * FIXED: Set expectedRunLength (not expectedRunVolume - we reuse the field)
     */
    public boolean shouldEmitVolumeRunsBar() {
        if (currentBar.getTickCount() < 10) {
            return false;
        }
        
        double actualRunVolume = currentBar.getRunVolume();
        double expectedRunVolume = getExpectedVolumeRunLength() * thresholdMultiplier;  // CRITICAL: Apply multiplier!
        
        // CRITICAL FIX: Always set before checking (so it's in emitted bar)
        // Note: We reuse expectedRunLength field for volume runs (stores expected volume)
        currentBar.setExpectedRunLength(expectedRunVolume);
        currentBar.setRunDirection(currentRunDirection);  // Also set run direction
        
        log.debug("[VRB] actual={:.2f}, expected={:.2f}, threshold={:.1f}x, willEmit={}", 
            actualRunVolume, expectedRunVolume, thresholdMultiplier, actualRunVolume >= expectedRunVolume);
        
        return actualRunVolume >= expectedRunVolume;
    }
    
    /**
     * Mark bar as complete and update EWMA
     */
    private void completeBar() {
        try {
            double alpha = 2.0 / (ewmaSpan + 1.0);
            
            // Initialize EWMA on first bar
            if (sampleCount == 0) {
                ewmaBuyVolume = currentBar.getBuyVolume();
                ewmaSellVolume = currentBar.getSellVolume();
                
                double totalVol = currentBar.getVolume();
                if (totalVol > 0) {
                    ewmaBuyProbability = currentBar.getBuyVolume() / totalVol;
                }
            } else {
                // Update EWMA
                ewmaBuyVolume = alpha * currentBar.getBuyVolume() + (1 - alpha) * ewmaBuyVolume;
                ewmaSellVolume = alpha * currentBar.getSellVolume() + (1 - alpha) * ewmaSellVolume;
                
                double totalVol = currentBar.getVolume();
                if (totalVol > 0) {
                    double buyProb = currentBar.getBuyVolume() / totalVol;
                    ewmaBuyProbability = alpha * buyProb + (1 - alpha) * ewmaBuyProbability;
                }
            }
            
            sampleCount++;
            currentBar.calculateImbalanceMetrics();
            isComplete = true;
            
            log.debug("Runs bar completed: sample #{}, run_length={}", 
                sampleCount, currentBar.getRunLength());
            
        } catch (Exception e) {
            log.error("Error completing runs bar: {}", e.getMessage(), e);
        }
    }
    
    /**
     * Check if bar is ready to emit
     */
    public boolean hasCompletedBar() {
        return isComplete;
    }
    
    /**
     * Emit completed bar and reset for next
     */
    public InformationBar emitAndReset() {
        InformationBar barToEmit = this.currentBar;
        
        // CRITICAL DEBUG: Log what we're emitting
        log.info("EMIT {} bar: token={}, start={}, end={}, timestamp={}, ticks={}, run={}",
            barToEmit.getBarType() != null ? barToEmit.getBarType() : "RUN",
            barToEmit.getToken(),
            barToEmit.getWindowStartMillis(),
            barToEmit.getWindowEndMillis(),
            barToEmit.getTimestamp(),
            barToEmit.getTickCount(),
            barToEmit.getRunLength());
        
        // Reset for next bar
        this.currentBar = new InformationBar();
        this.isComplete = false;
        this.currentBuyRun = 0;
        this.currentSellRun = 0;
        this.currentBuyVolumeRun = 0.0;
        this.currentSellVolumeRun = 0.0;
        this.currentRunDirection = 0;
        // CRITICAL: Keep EWMA state, previousPrice, previousDirection
        
        return barToEmit;
    }
    
    /**
     * Kafka Serde for RunsBarState
     */
    public static org.apache.kafka.common.serialization.Serde<RunsBarState> serde() {
        return new org.springframework.kafka.support.serializer.JsonSerde<>(RunsBarState.class);
    }
}


--------------------------------------------------
File End
--------------------------------------------------


service/MicrostructureMetricsCalculator.java
File type: .java
package com.kotsin.consumer.service;

import com.kotsin.consumer.model.OrderBookSnapshot;
import lombok.extern.slf4j.Slf4j;

import java.util.List;

/**
 * Service for calculating microstructure metrics
 * Extracted from MicrostructureFeatureState to follow Single Responsibility Principle
 */
@Slf4j
public class MicrostructureMetricsCalculator {

    /**
     * Calculate Order Flow Imbalance (OFI)
     *
     * Formula: OFI = Œ£[ŒîQ_bid * I{P_bid >= P_bid_prev}] - Œ£[ŒîQ_ask * I{P_ask <= P_ask_prev}]
     */
    public double calculateOFI(OrderBookSnapshot prev, OrderBookSnapshot curr, int levels) {
        double ofi = 0.0;

        try {
            // Bid side OFI
            for (int i = 0; i < Math.min(levels, curr.getAllBids().size()); i++) {
                if (i >= prev.getAllBids().size()) break;

                OrderBookSnapshot.OrderBookLevel bidPrev = prev.getAllBids().get(i);
                OrderBookSnapshot.OrderBookLevel bidCurr = curr.getAllBids().get(i);

                double deltaQty;
                if (bidCurr.getPrice() == bidPrev.getPrice()) {
                    // Price unchanged: use quantity difference
                    deltaQty = bidCurr.getQuantity() - bidPrev.getQuantity();
                } else if (bidCurr.getPrice() > bidPrev.getPrice()) {
                    // Price increased: aggressive buy
                    deltaQty = bidCurr.getQuantity();
                } else {
                    // Price decreased: demand removed
                    deltaQty = -bidPrev.getQuantity();
                }

                ofi += deltaQty * bidCurr.getPrice();
            }

            // Ask side OFI
            for (int i = 0; i < Math.min(levels, curr.getAllAsks().size()); i++) {
                if (i >= prev.getAllAsks().size()) break;

                OrderBookSnapshot.OrderBookLevel askPrev = prev.getAllAsks().get(i);
                OrderBookSnapshot.OrderBookLevel askCurr = curr.getAllAsks().get(i);

                double deltaQty;
                if (askCurr.getPrice() == askPrev.getPrice()) {
                    // Price unchanged: use quantity difference
                    deltaQty = askCurr.getQuantity() - askPrev.getQuantity();
                } else if (askCurr.getPrice() < askPrev.getPrice()) {
                    // Price decreased: aggressive sell
                    deltaQty = askCurr.getQuantity();
                } else {
                    // Price increased: supply removed
                    deltaQty = -askPrev.getQuantity();
                }

                ofi -= deltaQty * askCurr.getPrice();
            }

        } catch (Exception e) {
            log.error("Error calculating OFI: {}", e.getMessage());
        }

        return ofi;
    }

    /**
     * Calculate Depth Imbalance
     *
     * Formula: DI = (bid_qty - ask_qty) / (bid_qty + ask_qty)
     * Returns value between -1 (all asks) and +1 (all bids)
     */
    public double calculateDepthImbalance(OrderBookSnapshot snapshot, int levels) {
        try {
            int bidQty = getTotalVolume(snapshot.getTopBids(levels));
            int askQty = getTotalVolume(snapshot.getTopAsks(levels));
            int totalQty = bidQty + askQty;

            if (totalQty == 0) {
                return 0.0;
            }

            return (double) (bidQty - askQty) / totalQty;

        } catch (Exception e) {
            log.error("Error calculating depth imbalance: {}", e.getMessage());
            return 0.0;
        }
    }

    /**
     * Calculate VPIN (Volume-Synchronized Probability of Informed Trading)
     *
     * Formula: VPIN = |V_buy - V_sell| / V_total
     * Measures toxicity of order flow
     */
    public double calculateVPIN(List<Double> signedVolumeHistory, int minObservations) {
        if (signedVolumeHistory.size() < minObservations) {
            return 0.0;
        }

        try {
            double buyVolume = 0.0;
            double sellVolume = 0.0;

            for (Double signedVol : signedVolumeHistory) {
                if (signedVol > 0) {
                    buyVolume += signedVol;
                } else {
                    sellVolume += Math.abs(signedVol);
                }
            }

            double totalVolume = buyVolume + sellVolume;
            if (totalVolume == 0) {
                return 0.0;
            }

            return Math.abs(buyVolume - sellVolume) / totalVolume;

        } catch (Exception e) {
            log.error("Error calculating VPIN: {}", e.getMessage());
            return 0.0;
        }
    }

    /**
     * Calculate Kyle's Lambda (price impact per unit volume)
     *
     * Formula: Œª = Cov(Œîp, V_signed) / Var(V_signed)
     * Measures market liquidity
     */
    public double calculateKyleLambda(List<Double> priceHistory, List<Double> signedVolumeHistory, int minObservations) {
        if (priceHistory.size() < minObservations || signedVolumeHistory.size() < minObservations) {
            return 0.0;
        }

        try {
            // Calculate price changes
            Double[] prices = priceHistory.toArray(new Double[0]);
            double[] priceChanges = new double[prices.length - 1];
            for (int i = 1; i < prices.length; i++) {
                priceChanges[i - 1] = prices[i] - prices[i - 1];
            }

            // Get signed volumes
            Double[] signedVols = signedVolumeHistory.toArray(new Double[0]);
            int minLen = Math.min(priceChanges.length, signedVols.length);

            if (minLen < minObservations) {
                return 0.0;
            }

            // Calculate means
            double meanPriceChange = 0.0;
            double meanSignedVol = 0.0;
            for (int i = 0; i < minLen; i++) {
                meanPriceChange += priceChanges[i];
                meanSignedVol += signedVols[i];
            }
            meanPriceChange /= minLen;
            meanSignedVol /= minLen;

            // Calculate covariance and variance
            double covariance = 0.0;
            double variance = 0.0;
            for (int i = 0; i < minLen; i++) {
                double priceDev = priceChanges[i] - meanPriceChange;
                double volDev = signedVols[i] - meanSignedVol;
                covariance += priceDev * volDev;
                variance += volDev * volDev;
            }
            covariance /= minLen;
            variance /= minLen;

            if (variance == 0 || Double.isNaN(variance)) {
                return 0.0;
            }

            double kyleLambda = covariance / variance;

            // Sanity check
            if (kyleLambda < 0 || kyleLambda > 0.1) {
                log.debug("Unusual Kyle's Lambda: {}", kyleLambda);
            }

            return kyleLambda;

        } catch (Exception e) {
            log.error("Error calculating Kyle's Lambda: {}", e.getMessage());
            return 0.0;
        }
    }

    /**
     * Get total volume from list of levels
     */
    public int getTotalVolume(List<OrderBookSnapshot.OrderBookLevel> levels) {
        return levels.stream()
            .mapToInt(OrderBookSnapshot.OrderBookLevel::getQuantity)
            .sum();
    }
}


--------------------------------------------------
File End
--------------------------------------------------


service/MongoInstrumentFamilyService.java
File type: .java
package com.kotsin.consumer.service;

import com.kotsin.consumer.entity.Scrip;
import com.kotsin.consumer.entity.ScripGroup;
import com.kotsin.consumer.model.InstrumentFamily;
import com.kotsin.consumer.model.InstrumentInfo;
import com.kotsin.consumer.repository.ScripGroupRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import jakarta.annotation.PostConstruct;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;

/**
 * MongoDB-based service to cache instrument families (equity + future + options)
 * Fetches from MongoDB ScripGroup collection and caches in in-memory LocalHashMap
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class MongoInstrumentFamilyService {
    
    private final ScripGroupRepository scripGroupRepository;
    
    // OPTIMIZED: Two-level cache for O(1) lookups
    private final Map<String, InstrumentFamily> localCache = new ConcurrentHashMap<>();
    
    // NEW: Scrip-to-FamilyKey lookup map (scripCode ‚Üí equityScripCode)
    // This eliminates ALL blocking DB calls during real-time processing
    private final Map<String, String> scripToFamilyMap = new ConcurrentHashMap<>();
    
    @PostConstruct
    public void initializeCache() {
        log.info("üöÄ Initializing MongoDB-based instrument family cache...");
        try {
            refreshCache();
            log.info("‚úÖ MongoDB instrument family cache initialized successfully");
        } catch (Exception e) {
            log.error("‚ùå Failed to initialize MongoDB instrument family cache", e);
        }
    }
    
    /**
     * Daily refresh at 3 AM IST
     */
    @Scheduled(cron = "0 0 3 * * ?")
    public void dailyRefresh() {
        log.info("üîÑ Daily MongoDB cache refresh started...");
        refreshCache();
    }
    
    /**
     * Manual cache refresh
     */
    public void refreshCache() {
        try {
            long startTime = System.currentTimeMillis();
            
            // 1. Get all ScripGroups from MongoDB
            List<ScripGroup> scripGroups = scripGroupRepository.findByTradingType("EQUITY");
            log.info("üìä Found {} ScripGroups from MongoDB for tradingType=EQUITY", scripGroups.size());
            
            // Debug: Try to get all ScripGroups to see what's in the database
            List<ScripGroup> allScripGroups = scripGroupRepository.findAll();
            log.info("üìä Total ScripGroups in database: {}", allScripGroups.size());
            if (!allScripGroups.isEmpty()) {
                ScripGroup first = allScripGroups.get(0);
                log.info("üìä Sample ScripGroup - tradingType: '{}', equityScripCode: '{}', companyName: '{}'", 
                    first.getTradingType(), first.getEquityScripCode(), first.getCompanyName());
            } else {
                log.warn("‚ö†Ô∏è No ScripGroups found in database at all! Check database name and collection name.");
                log.warn("‚ö†Ô∏è Current MongoDB URI: mongodb://localhost:27017/tradeIngestion");
                log.warn("‚ö†Ô∏è Expected collection: ScripGroup");
                log.warn("‚ö†Ô∏è Expected tradingType values: EQUITY, FUTURE, OPTION");
            }
            
            if (scripGroups.isEmpty()) {
                log.warn("‚ö†Ô∏è No ScripGroups found in MongoDB, skipping cache refresh");
                return;
            }
            
            // 2. Convert ScripGroups to InstrumentFamilies
            Map<String, InstrumentFamily> families = convertScripGroupsToFamilies(scripGroups);
            
            // 3. Update local cache (in-memory only)
            localCache.clear();
            localCache.putAll(families);
            
            // 4. Build scrip-to-family lookup map for ALL derivatives
            scripToFamilyMap.clear();
            buildScripToFamilyMap(scripGroups);
            
            long duration = System.currentTimeMillis() - startTime;
            log.info("‚úÖ MongoDB cache refresh complete. Loaded {} families in {}ms", 
                families.size(), duration);
            
        } catch (Exception e) {
            log.error("‚ùå MongoDB cache refresh failed", e);
        }
    }
    
    private Map<String, InstrumentFamily> convertScripGroupsToFamilies(List<ScripGroup> scripGroups) {
        Map<String, InstrumentFamily> families = new ConcurrentHashMap<>();
        
        for (ScripGroup scripGroup : scripGroups) {
            try {
                InstrumentFamily family = convertScripGroupToFamily(scripGroup);
                if (family != null) {
                    families.put(scripGroup.getEquityScripCode(), family);
                }
            } catch (Exception e) {
                log.error("‚ùå Failed to convert ScripGroup to InstrumentFamily: {}", 
                    scripGroup.getEquityScripCode(), e);
            }
        }
        
        return families;
    }
    
    /**
     * Build scrip-to-family lookup map for O(1) derivative resolution
     * Maps: derivativeScripCode ‚Üí underlyingEquityScripCode
     * 
     * Example for PNB family:
     * - "10666" (equity) ‚Üí "10666" (itself)
     * - "49067" (future) ‚Üí "10666" (PNB equity)
     * - "112921" (option) ‚Üí "10666" (PNB equity)
     */
    private void buildScripToFamilyMap(List<ScripGroup> scripGroups) {
        int equityCount = 0;
        int futureCount = 0;
        int optionCount = 0;
        
        for (ScripGroup group : scripGroups) {
            String equityScripCode = group.getEquityScripCode();
            if (equityScripCode == null || equityScripCode.isBlank()) {
                continue;
            }
            
            // Map equity to itself
            if (group.getEquity() != null && group.getEquity().getScripCode() != null) {
                scripToFamilyMap.put(group.getEquity().getScripCode(), equityScripCode);
                equityCount++;
            }
            
            // Map all futures to equity
            if (group.getFutures() != null) {
                for (Scrip future : group.getFutures()) {
                    if (future.getScripCode() != null) {
                        scripToFamilyMap.put(future.getScripCode(), equityScripCode);
                        futureCount++;
                    }
                }
            }
            
            // Map all options to equity
            if (group.getOptions() != null) {
                for (Scrip option : group.getOptions()) {
                    if (option.getScripCode() != null) {
                        scripToFamilyMap.put(option.getScripCode(), equityScripCode);
                        optionCount++;
                    }
                }
            }
        }
        
        log.info("üìä Built scrip-to-family map: {} equities, {} futures, {} options = {} total mappings",
            equityCount, futureCount, optionCount, scripToFamilyMap.size());
    }
    
    private InstrumentFamily convertScripGroupToFamily(ScripGroup scripGroup) {
        try {
            // Convert equity Scrip to InstrumentInfo
            InstrumentInfo equityInfo = null;
            if (scripGroup.getEquity() != null) {
                equityInfo = convertScripToInstrumentInfo(scripGroup.getEquity());
            }
            
            // Convert futures
            InstrumentInfo futureInfo = null;
            if (scripGroup.getFutures() != null && !scripGroup.getFutures().isEmpty()) {
                futureInfo = convertScripToInstrumentInfo(scripGroup.getFutures().get(0));
            }
            
            // Convert options
            List<InstrumentInfo> optionsInfo = new ArrayList<>();
            if (scripGroup.getOptions() != null) {
                for (Scrip option : scripGroup.getOptions()) {
                    optionsInfo.add(convertScripToInstrumentInfo(option));
                }
            }
            
            return InstrumentFamily.builder()
                .equityScripCode(scripGroup.getEquityScripCode())
                .companyName(scripGroup.getCompanyName())
                .equity(equityInfo)
                .future(futureInfo)
                .options(optionsInfo)
                .lastUpdated(System.currentTimeMillis())
                .dataSource("MONGODB")
                .build();
                
        } catch (Exception e) {
            log.error("‚ùå Failed to convert ScripGroup to InstrumentFamily: {}", 
                scripGroup.getEquityScripCode(), e);
            return null;
        }
    }
    
    private InstrumentInfo convertScripToInstrumentInfo(Scrip scrip) {
        // Attempt to extract token from scripData JSON if present (non-fatal if absent)
        String token = null;
        try {
            if (scrip.getScripData() != null && !scrip.getScripData().isBlank()) {
                String data = scrip.getScripData();
                // naive extract: "Token": 12345 or "token":"12345"
                java.util.regex.Matcher m = java.util.regex.Pattern.compile("\"[Tt]oken\"\s*:\s*\"?(\\d+)\"?").matcher(data);
                if (m.find()) {
                    token = m.group(1);
                }
            }
        } catch (Exception ignore) {}

        return InstrumentInfo.builder()
            .scripCode(scrip.getScripCode())
            .token(token)
            .name(scrip.getName())
            .fullName(scrip.getFullName())
            .exchange(scrip.getExch())
            .exchangeType(scrip.getExchType())
            .series(scrip.getSeries())
            .expiry(scrip.getExpiry())
            .scripType(scrip.getScripType())
            .strikeRate(scrip.getStrikeRate() != null ? Double.parseDouble(scrip.getStrikeRate()) : null)
            .tickSize(scrip.getTickSize() != null ? Double.parseDouble(scrip.getTickSize()) : null)
            .lotSize(scrip.getLotSize() != null ? Integer.parseInt(scrip.getLotSize()) : null)
            .isin(scrip.getISIN())
            .symbolRoot(scrip.getSymbolRoot())
            .bocoallowed(scrip.getBOCOAllowed())
            .id(scrip.getId())
            .scriptTypeKotsin(scrip.getScriptTypeKotsin())
            .insertionDate(scrip.getInsertionDate() != null ? scrip.getInsertionDate().toString() : null)
            .multiplier(scrip.getMultiplier())
            .qtyLimit(scrip.getQtyLimit())
            .scripData(scrip.getScripData())
            .build();
    }
    
    // Redis removed; in-memory only
    
    /**
     * Get instrument family by scripCode (equity lookup)
     */
    public InstrumentFamily getFamily(String scripCode) {
        if (scripCode == null || scripCode.trim().isEmpty()) {
            return null;
        }
        
        // Try local cache first
        InstrumentFamily family = localCache.get(scripCode);
        if (family != null) {
            return family;
        }
        
        return null;
    }
    
    /**
     * OPTIMIZED: Resolve instrument family using in-memory lookup ONLY
     * NO BLOCKING DB CALLS - Uses pre-built scripToFamilyMap
     * 
     * Performance: O(1) lookup vs O(n) DB query
     */
    public InstrumentFamily resolveFamily(String scripCode, String exchangeType, String companyName) {
        if (scripCode == null || scripCode.isBlank()) {
            return null;
        }
        
        // Step 1: Try scrip-to-family map (O(1) lookup)
        String familyKey = scripToFamilyMap.get(scripCode);
        if (familyKey != null) {
            InstrumentFamily family = localCache.get(familyKey);
            if (family != null) {
                log.debug("‚úÖ Resolved derivative {} ‚Üí family {} (cache hit)", scripCode, familyKey);
                return family;
            }
        }
        
        // Step 2: Fallback for equities or non-mapped scrips
        InstrumentFamily family = localCache.get(scripCode);
        if (family != null) {
            log.debug("‚úÖ Found equity family for {} (direct lookup)", scripCode);
            return family;
        }
        
        // Step 3: Last resort - extract from company name (NO DB CALL)
        if (companyName != null && !companyName.isBlank()) {
            String symbolRoot = extractSymbolRoot(companyName);
            if (symbolRoot != null) {
                family = localCache.get(symbolRoot);
                if (family != null) {
                    log.debug("‚úÖ Resolved {} ‚Üí family {} (name extraction)", scripCode, symbolRoot);
                    return family;
                }
            }
        }
        
        log.debug("‚ùå No family found for scripCode: {} (not in cache)", scripCode);
        return null;
    }
    
    private String extractSymbolRoot(String companyName) {
        if (companyName == null || companyName.isBlank()) {
            return null;
        }
        
        // Extract first word as symbol root (e.g., "LT 28 OCT 2025 CE 3850.00" -> "LT")
        String[] parts = companyName.split("\\s+");
        if (parts.length > 0) {
            return parts[0].replaceAll("[^A-Za-z0-9&]", "");
        }
        
        return null;
    }
    
    /**
     * Get all cached families
     */
    public Map<String, InstrumentFamily> getAllFamilies() {
        return new HashMap<>(localCache);
    }
    
    /**
     * Get cache size
     */
    public int getCacheSize() {
        return localCache.size();
    }
    
    /**
     * Get cache statistics
     */
    public Map<String, Object> getCacheStats() {
        Map<String, Object> stats = new HashMap<>();
        stats.put("cacheSize", localCache.size());
        stats.put("familiesWithFutures", localCache.values().stream()
            .mapToInt(f -> f.hasFuture() ? 1 : 0)
            .sum());
        stats.put("familiesWithOptions", localCache.values().stream()
            .mapToInt(f -> f.hasOptions() ? 1 : 0)
            .sum());
        stats.put("totalOptions", localCache.values().stream()
            .mapToInt(InstrumentFamily::getOptionsCount)
            .sum());
        stats.put("mongodbFamilies", localCache.values().stream()
            .mapToInt(f -> "MONGODB".equals(f.getDataSource()) ? 1 : 0)
            .sum());
        return stats;
    }
    
    /**
     * Clear cache
     */
    public void clearCache() {
        localCache.clear();
        log.info("üóëÔ∏è Local cache cleared");
    }
}


--------------------------------------------------
File End
--------------------------------------------------


monitoring/SystemMonitor.java
File type: .java
package com.kotsin.consumer.monitoring;

import com.kotsin.consumer.config.ProcessingConstants;
import com.kotsin.consumer.service.BackpressureHandler;
import com.kotsin.consumer.service.StreamMetrics;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;

import java.lang.management.ManagementFactory;
import java.lang.management.MemoryMXBean;
import java.lang.management.MemoryUsage;
import java.util.HashMap;
import java.util.Map;

/**
 * System monitoring and alerting service
 * 
 * OBSERVABILITY: Track system health and performance metrics
 * ALERTING: Detect and alert on critical conditions
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class SystemMonitor {

    private final StreamMetrics streamMetrics;
    private final BackpressureHandler backpressureHandler;
    
    private final MemoryMXBean memoryBean = ManagementFactory.getMemoryMXBean();
    
    private long lastReportTime = System.currentTimeMillis();
    private long lastAlertTime = 0;
    private static final long ALERT_COOLDOWN_MS = 60000; // 1 minute cooldown

    /**
     * Periodic health check and metrics reporting
     */
    @Scheduled(fixedRate = 60000) // Every 60 seconds
    public void reportMetrics() {
        log.info("üìä === SYSTEM METRICS REPORT ===");
        
        // Stream metrics
        log.info("üìà Stream Metrics: {}", streamMetrics.getMetrics());
        
        // Memory metrics
        MemoryUsage heapUsage = memoryBean.getHeapMemoryUsage();
        double heapUsedPercent = (double) heapUsage.getUsed() / heapUsage.getMax() * 100;
        log.info("üíæ Memory: Used={}MB, Max={}MB, Usage={:.2f}%", 
            heapUsage.getUsed() / 1024 / 1024,
            heapUsage.getMax() / 1024 / 1024,
            heapUsedPercent);
        
        // Backpressure metrics
        log.info("üö¶ Backpressure: {}", backpressureHandler.getBackpressureStats());
        
        // System health
        boolean isHealthy = isSystemHealthy();
        log.info("üè• System Health: {}", isHealthy ? "HEALTHY" : "UNHEALTHY");
        
        // Check for alert conditions
        checkAlertConditions(heapUsedPercent);
        
        log.info("============================");
        
        lastReportTime = System.currentTimeMillis();
    }

    /**
     * Check if system is healthy
     */
    public boolean isSystemHealthy() {
        // Check memory usage
        MemoryUsage heapUsage = memoryBean.getHeapMemoryUsage();
        double heapUsedPercent = (double) heapUsage.getUsed() / heapUsage.getMax() * 100;
        if (heapUsedPercent > 90) {
            return false;
        }
        
        // Check stream metrics
        if (!streamMetrics.isHealthy()) {
            return false;
        }
        
        // Check backpressure
        if (!backpressureHandler.isHealthy()) {
            return false;
        }
        
        return true;
    }

    /**
     * Check for alert conditions
     */
    private void checkAlertConditions(double heapUsedPercent) {
        long now = System.currentTimeMillis();
        
        // Cooldown to avoid alert spam
        if (now - lastAlertTime < ALERT_COOLDOWN_MS) {
            return;
        }

        // High memory usage alert
        if (heapUsedPercent > 90) {
            sendAlert(AlertLevel.CRITICAL, "High memory usage", 
                String.format("Heap memory usage: %.2f%%", heapUsedPercent));
            lastAlertTime = now;
        } else if (heapUsedPercent > 80) {
            sendAlert(AlertLevel.WARNING, "Elevated memory usage", 
                String.format("Heap memory usage: %.2f%%", heapUsedPercent));
            lastAlertTime = now;
        }

        // Backpressure alert
        if (!backpressureHandler.isHealthy()) {
            sendAlert(AlertLevel.WARNING, "Backpressure detected", 
                backpressureHandler.getBackpressureStats());
            lastAlertTime = now;
        }

        // Stream health alert
        if (!streamMetrics.isHealthy()) {
            sendAlert(AlertLevel.CRITICAL, "Stream processing issues", 
                streamMetrics.getMetrics());
            lastAlertTime = now;
        }
    }

    /**
     * Send alert (can be extended to send to external alerting systems)
     */
    private void sendAlert(AlertLevel level, String title, String message) {
        String emoji = level == AlertLevel.CRITICAL ? "üö®" : "‚ö†Ô∏è";
        log.error("{} ALERT [{}]: {} - {}", emoji, level, title, message);
        
        // TODO: Integrate with external alerting systems
        // - PagerDuty
        // - Slack
        // - Email
        // - SMS
    }

    /**
     * Get system metrics as map
     */
    public Map<String, Object> getSystemMetrics() {
        Map<String, Object> metrics = new HashMap<>();
        
        // Memory metrics
        MemoryUsage heapUsage = memoryBean.getHeapMemoryUsage();
        metrics.put("memory.heap.used", heapUsage.getUsed());
        metrics.put("memory.heap.max", heapUsage.getMax());
        metrics.put("memory.heap.usage.percent", 
            (double) heapUsage.getUsed() / heapUsage.getMax() * 100);
        
        // Stream metrics
        metrics.put("stream.metrics", streamMetrics.getMetrics());
        
        // Backpressure metrics
        metrics.put("backpressure.stats", backpressureHandler.getBackpressureStats());
        
        // Health status
        metrics.put("system.healthy", isSystemHealthy());
        
        // Uptime
        long uptime = System.currentTimeMillis() - lastReportTime;
        metrics.put("system.uptime.seconds", uptime / 1000);
        
        return metrics;
    }

    /**
     * Alert level enum
     */
    public enum AlertLevel {
        INFO,
        WARNING,
        CRITICAL
    }
}


--------------------------------------------------
File End
--------------------------------------------------


timeExtractor/TickTimestampExtractor.java
File type: .java
package com.kotsin.consumer.timeExtractor;

import com.kotsin.consumer.model.TickData;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.processor.TimestampExtractor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;

/**
 * A simple, correct TimestampExtractor that extracts the event time from
 * the TickData payload. This is used for the initial 1-minute candle aggregation.
 */
public class TickTimestampExtractor implements TimestampExtractor {

    private static final Logger LOGGER = LoggerFactory.getLogger(TickTimestampExtractor.class);
    private static final DateTimeFormatter DT_FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss").withZone(ZoneId.of("Asia/Kolkata"));

    @Override
    public long extract(ConsumerRecord<Object, Object> record, long previousTimestamp) {
        Object value = record.value();
        if (!(value instanceof TickData)) {
            // Not a TickData record, fall back to record timestamp
            return record.timestamp();
        }

        TickData tick = (TickData) value;
        String tickDt = tick.getTickDt();

        if (tickDt == null || tickDt.isBlank()) {
            LOGGER.warn("Tick has null or empty timestamp, using Kafka record timestamp for token {}", tick.getToken());
            return record.timestamp();
        }

        try {
            // Handle the "/Date(1746430676000)/" format
            if (tickDt.startsWith("/Date(") && tickDt.endsWith(")/")) {
                String millisStr = tickDt.substring(6, tickDt.length() - 2);
                long ts = Long.parseLong(millisStr);

                // CRITICAL: Reject negative timestamps (before Unix epoch)
                if (ts < 0) {
                    LOGGER.warn("Negative timestamp {} for token {}. Using Kafka record timestamp instead.", ts, tick.getToken());
                    return record.timestamp();
                }

                // CRITICAL: Validate using business logic (trading hours), NOT wall-clock time
                // This allows replay of historical data without "too old" rejections
                
                // Validate timestamp is within reasonable range (not too far from record timestamp)
                // Use record timestamp as reference (works for both live and replay)
                long recordTs = record.timestamp();
                if (recordTs > 0) {
                    long deviation = Math.abs(ts - recordTs);
                    // Allow 7 days deviation (handles clock skew, but catches corrupt data)
                    if (deviation > 7L * 24 * 3600 * 1000) {
                        LOGGER.warn("Timestamp {} deviates {} ms from record timestamp {} for token {}. Using record timestamp.", 
                            ts, deviation, recordTs, tick.getToken());
                        return recordTs;
                    }
                }
                
                // Additional validation: Check if timestamp is within trading hours
                // This catches obviously wrong timestamps (e.g., midnight, weekends)
                try {
                    ZonedDateTime zdt = ZonedDateTime.ofInstant(
                        java.time.Instant.ofEpochMilli(ts), 
                        ZoneId.of("Asia/Kolkata")
                    );
                    int hour = zdt.getHour();
                    int dayOfWeek = zdt.getDayOfWeek().getValue();
                    
                    // Weekend check
                    if (dayOfWeek == 6 || dayOfWeek == 7) {
                        LOGGER.debug("Timestamp {} is on weekend for token {}. Using record timestamp.", ts, tick.getToken());
                        return recordTs > 0 ? recordTs : ts; // Use record if available
                    }
                    
                    // Trading hours check (9 AM to 4 PM IST with buffer)
                    if (hour < 8 || hour > 17) {
                        LOGGER.debug("Timestamp {} is outside trading hours for token {}. Using record timestamp.", ts, tick.getToken());
                        return recordTs > 0 ? recordTs : ts; // Use record if available
                    }
                } catch (Exception e) {
                    LOGGER.warn("Failed to validate timestamp {} for token {}: {}", ts, tick.getToken(), e.getMessage());
                }

                return ts;
            }

            // Handle standard format
            ZonedDateTime zdt = ZonedDateTime.parse(tickDt, DT_FORMATTER);
            return zdt.toInstant().toEpochMilli();

        } catch (DateTimeParseException | NumberFormatException e) {
            LOGGER.error("Could not parse timestamp '{}' for token {}. Using Kafka record timestamp.", tickDt, tick.getToken(), e);
            // CRITICAL: Use record.timestamp(), NEVER System.currentTimeMillis()
            return record.timestamp();
        }
    }
}

--------------------------------------------------
File End
--------------------------------------------------


timeExtractor/MultiMinuteOffsetTimestampExtractor.java
File type: .java
package com.kotsin.consumer.timeExtractor;

import com.kotsin.consumer.model.Candlestick;
import com.kotsin.consumer.util.MarketTimeAligner;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.processor.TimestampExtractor;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Timestamp extractor for multi-minute rollups built from 1m candles.
 * Uses the 1m candle END time and applies an exchange/window-size offset
 * so windows align to exchange rules but stream-time still advances.
 *
 * CRITICAL: Never falls back to System.currentTimeMillis() to handle lag correctly.
 */
public final class MultiMinuteOffsetTimestampExtractor implements TimestampExtractor {

    private static final Logger LOGGER = LoggerFactory.getLogger(MultiMinuteOffsetTimestampExtractor.class);
    private final int windowSizeMinutes;

    public MultiMinuteOffsetTimestampExtractor(int windowSizeMinutes) {
        this.windowSizeMinutes = windowSizeMinutes;
    }

    @Override
    public long extract(ConsumerRecord<Object, Object> record, long partitionTime) {
        long baseTs = 0L;

        Object v = record.value();
        if (v instanceof Candlestick c) {
            // Prefer END time for faster window closure; fall back to START, then record/partition time
            if (c.getWindowEndMillis() > 0L) {
                baseTs = c.getWindowEndMillis();
            } else if (c.getWindowStartMillis() > 0L) {
                baseTs = c.getWindowStartMillis();
            } else if (record.timestamp() > 0L) {
                baseTs = record.timestamp();
            } else {
                baseTs = partitionTime;
            }

            // CRITICAL: If still invalid, log error and use partition time
            // NEVER use System.currentTimeMillis() - it breaks lag processing
            if (baseTs <= 0L) {
                LOGGER.error("Invalid timestamp for candle {} (company: {}). Using partition time: {}",
                        c.getScripCode(), c.getCompanyName(), partitionTime);
                baseTs = Math.max(partitionTime, 0L);
            }

            // SHIFT (do not collapse) by per-exchange offset to align boundaries
            String exch = c.getExchange();
            int offMin = MarketTimeAligner.getWindowOffsetMinutes(exch, windowSizeMinutes);
            return baseTs + offMin * 60_000L;
        }

        // Non-candle records (unlikely here)
        if (record.timestamp() > 0L) return record.timestamp();
        return Math.max(partitionTime, 0L);
    }
}

--------------------------------------------------
File End
--------------------------------------------------


timeExtractor/OpenInterestTimestampExtractor.java
File type: .java
package com.kotsin.consumer.timeExtractor;

import com.kotsin.consumer.model.OpenInterestData;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.processor.TimestampExtractor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * TimestampExtractor for OpenInterest data.
 * Extracts receivedTimestamp from OpenInterestData payload.
 */
public class OpenInterestTimestampExtractor implements TimestampExtractor {

    private static final Logger LOGGER = LoggerFactory.getLogger(OpenInterestTimestampExtractor.class);

    @Override
    public long extract(ConsumerRecord<Object, Object> record, long previousTimestamp) {
        Object value = record.value();
        if (!(value instanceof OpenInterestData)) {
            // Not an OpenInterestData record, fall back to record timestamp
            return record.timestamp();
        }

        OpenInterestData oiData = (OpenInterestData) value;
        long timestamp = oiData.getTimestamp();

        if (timestamp <= 0) {
            LOGGER.warn("OpenInterest has invalid timestamp ({}) for token {}, using Kafka record timestamp", 
                    timestamp, oiData.getToken());
            return record.timestamp();
        }

        // Validate timestamp is reasonable (not too far in future/past)
        long now = System.currentTimeMillis();
        if (timestamp > now + 60000L) { // More than 1 min in future
            LOGGER.warn("OpenInterest timestamp {} is in the future for token {}. Using record timestamp.", 
                    timestamp, oiData.getToken());
            return record.timestamp();
        }
        if (timestamp < now - 7L * 24 * 3600 * 1000) { // More than 7 days old
            LOGGER.warn("OpenInterest timestamp {} is more than 7 days old for token {}. Using record timestamp.", 
                    timestamp, oiData.getToken());
            return record.timestamp();
        }

        return timestamp;
    }
}



--------------------------------------------------
File End
--------------------------------------------------


timeExtractor/InstrumentCandleTimestampExtractor.java
File type: .java
package com.kotsin.consumer.timeExtractor;

import com.kotsin.consumer.model.InstrumentCandle;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.processor.TimestampExtractor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Timestamp extractor for InstrumentCandle records.
 * Prefers windowEndMillis, falls back to windowStartMillis, then record timestamp.
 */
public class InstrumentCandleTimestampExtractor implements TimestampExtractor {
    private static final Logger LOGGER = LoggerFactory.getLogger(InstrumentCandleTimestampExtractor.class);

    @Override
    public long extract(ConsumerRecord<Object, Object> record, long previousTimestamp) {
        Object value = record.value();
        if (!(value instanceof InstrumentCandle)) {
            return record.timestamp();
        }

        InstrumentCandle candle = (InstrumentCandle) value;
        Long ts = candle.getWindowEndMillis();
        if (ts == null) {
            ts = candle.getWindowStartMillis();
        }
        if (ts == null) {
            LOGGER.warn("InstrumentCandle missing window times for scripCode {}. Using record timestamp.", candle.getScripCode());
            return record.timestamp();
        }
        return ts;
    }
}


--------------------------------------------------
File End
--------------------------------------------------
