# Production Environment Configuration
# Active profile: prod

# Kafka Configuration (Production)
spring.kafka.bootstrap-servers=${KAFKA_BOOTSTRAP_SERVERS}
spring.kafka.streams.application-id=unified-market-processor-prod
spring.kafka.streams.state-dir=/var/lib/kafka-streams/streamingcandle

# Processing Configuration (Production - Optimized)
spring.kafka.streams.properties.processing.guarantee=exactly_once_v2
spring.kafka.streams.properties.commit.interval.ms=1000
spring.kafka.streams.properties.num.stream.threads=4

# Input topics (Production)
unified.input.topic.ticks=${INPUT_TOPIC_TICKS:forwardtesting-data}
unified.input.topic.oi=${INPUT_TOPIC_OI:OpenInterest}
unified.input.topic.orderbook=${INPUT_TOPIC_ORDERBOOK:Orderbook}

# Output topics (Production)
stream.outputs.candles.enabled=true
stream.outputs.candles.1m=candle-complete-1m
stream.outputs.candles.2m=candle-complete-2m
stream.outputs.candles.5m=candle-complete-5m
stream.outputs.candles.15m=candle-complete-15m
stream.outputs.candles.30m=candle-complete-30m

# Family aggregation (Production)
stream.outputs.familyStructured.enabled=true
stream.outputs.familyStructured.1m=family-structured-1m
stream.outputs.familyStructured.2m=family-structured-2m
stream.outputs.familyStructured.5m=family-structured-5m
stream.outputs.familyStructured.15m=family-structured-15m
stream.outputs.familyStructured.30m=family-structured-30m
stream.outputs.familyStructured.all=family-structured-all

# MongoDB Configuration (Production)
spring.data.mongodb.uri=${MONGODB_URI}

# Logging Configuration (Production - Info level)
logging.level.com.kotsin.consumer=INFO
logging.level.org.apache.kafka.streams=WARN
logging.level.org.springframework.kafka=WARN
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n
logging.file.name=/var/log/streamingcandle/application.log
logging.file.max-size=100MB
logging.file.max-history=30

# Trading Hours (Production)
trading.hours.nse.start=09:15
trading.hours.nse.end=15:30
trading.hours.mcx.start=09:00
trading.hours.mcx.end=23:30
trading.hours.buffer.minutes=15

# Backpressure (Production - Strict)
kafka.streams.backpressure.enabled=true
kafka.streams.backpressure.max.poll.records=100
kafka.streams.backpressure.lag.threshold=1000
kafka.streams.backpressure.throttle.factor=0.5

# Monitoring (Production)
management.endpoints.web.exposure.include=health,metrics,info,prometheus
management.endpoint.health.show-details=when-authorized
management.metrics.export.prometheus.enabled=true

# Feature Flags (Production)
features.circuit-breaker.enabled=true
features.audit-logging.enabled=true
features.performance-profiling.enabled=false

# Security (Production)
server.port=8080
server.ssl.enabled=false
management.server.port=8081

# Resilience (Production)
spring.kafka.streams.properties.max.poll.records=100
spring.kafka.streams.properties.max.poll.interval.ms=300000
spring.kafka.streams.properties.session.timeout.ms=30000
spring.kafka.streams.properties.heartbeat.interval.ms=3000

# Performance (Production)
spring.kafka.streams.properties.buffered.records.per.partition=1000
spring.kafka.streams.properties.cache.max.bytes.buffering=10485760
spring.kafka.streams.properties.rocksdb.config.setter=org.apache.kafka.streams.state.RocksDBConfigSetter

# State Store Resilience (CRITICAL FIX for OffsetOutOfRangeException)
# Enable standby replicas for automatic failover
spring.kafka.streams.properties.num.standby.replicas=1
# Acceptable recovery lag for standby replicas
spring.kafka.streams.properties.acceptable.recovery.lag=10000
# State store cache configuration
spring.kafka.streams.properties.statestore.cache.max.bytes=104857600

# Changelog Topic Configuration (matches state store retention)
# NOTE: Changelog topics should be created with these settings:
# - retention.ms=604800000 (7 days)
# - cleanup.policy=compact,delete
# - min.compaction.lag.ms=86400000 (1 day)