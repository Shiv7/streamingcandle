# ============================================================================
# PRODUCTION PROFILE - streamingcandle
# ============================================================================

spring.application.name=consumer
server.port=8081

# ============================================================================
# MONGODB CONFIGURATION - PRODUCTION
# ============================================================================
# TODO: Update with production MongoDB credentials
spring.data.mongodb.uri=mongodb://localhost:27017/tradeIngestion
spring.data.mongodb.auto-index-creation=true

# ============================================================================
# KAFKA CONFIGURATION - PRODUCTION
# ============================================================================
# CRITICAL: Update with actual production Kafka broker addresses
spring.kafka.bootstrap-servers=13.203.60.173:9094

# Consumer configuration
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.group-id=streamingcandle-production

# Deserializers
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=com.kotsin.consumer.model
spring.kafka.consumer.properties.spring.json.value.default.type=com.kotsin.consumer.model.TickData

# Producer configuration
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

# ============================================================================
# KAFKA STREAMS CONFIGURATION - PRODUCTION
# ============================================================================
spring.kafka.streams.application-id=production-streamingcandle
spring.kafka.streams.bootstrap-servers=13.203.60.173:9094
spring.kafka.streams.client-id=streamingcandle-prod
spring.kafka.streams.properties.default.key.serde=org.apache.kafka.common.serialization.Serdes$StringSerde
spring.kafka.streams.properties.default.value.serde=org.springframework.kafka.support.serializer.JsonSerde
spring.kafka.streams.properties.spring.json.trusted.packages=com.kotsin.consumer.model
spring.kafka.streams.properties.auto.offset.reset=earliest

# CRITICAL: Production state directory (must be persistent across restarts)
# Ensure this directory exists and has proper permissions
spring.kafka.streams.state-dir=/var/lib/kafka-streams/streamingcandle
spring.kafka.streams.properties.processing.guarantee=exactly_once_v2

# Production reliability configurations
spring.kafka.streams.properties.commit.interval.ms=100
spring.kafka.streams.properties.statestore.cache.max.bytes=10485760
spring.kafka.streams.properties.session.timeout.ms=30000
spring.kafka.streams.properties.heartbeat.interval.ms=3000
spring.kafka.streams.properties.request.timeout.ms=40000
spring.kafka.streams.properties.retry.backoff.ms=100
spring.kafka.streams.properties.reconnect.backoff.ms=50

# REAL-TIME OPTIMIZATION
spring.kafka.streams.properties.linger.ms=5
spring.kafka.streams.properties.batch.size=1024
spring.kafka.streams.properties.buffer.memory=33554432

spring.kafka.streams.properties.spring.json.value.default.type=com.kotsin.consumer.model.TickData

# ============================================================================
# INFORMATION-DRIVEN BARS CONFIGURATION
# Based on "Advances in Financial Machine Learning" Chapter 2
# ============================================================================

# Enable all information bars
information.bars.enabled=true

# Multi-threshold approach (1x, 2x, 5x multipliers)
# Creates 12 topics: VIB/DIB/TRB/VRB Ã— 3 thresholds each
information.bars.thresholds=1.0,2.0,5.0

# VIB - Volume Imbalance Bars
information.bars.vib.enabled=true
information.bars.vib.min.ticks=10
information.bars.vib.warmup.samples=20

# DIB - Dollar Imbalance Bars  
information.bars.dib.enabled=true
information.bars.dib.min.ticks=10
information.bars.dib.warmup.samples=20

# TRB - Tick Runs Bars
information.bars.trb.enabled=true
information.bars.trb.min.ticks=10
information.bars.trb.warmup.samples=20

# VRB - Volume Runs Bars
information.bars.vrb.enabled=true
information.bars.vrb.min.ticks=10
information.bars.vrb.warmup.samples=20

# EWMA parameters for adaptive thresholding
information.bars.ewma.span=100.0

# Minimum expected volume (prevents division by zero during warmup)
information.bars.min.expected.volume=100.0

# Expected bar size for runs bars (TRB/VRB)
information.bars.expected.bar.size=50.0

# ============================================================================
# KAFKA TOPICS - PRODUCTION
# ============================================================================
information.bars.input.topic=forwardtesting-data
information.bars.vib.output.topic=volume-imbalance-bars
information.bars.dib.output.topic=dollar-imbalance-bars
information.bars.trb.output.topic=tick-runs-bars
information.bars.vrb.output.topic=volume-runs-bars

# ============================================================================
# MICROSTRUCTURE FEATURES CONFIGURATION
# Based on "Advances in Financial Machine Learning" Chapter 19
# ============================================================================

# Enable microstructure feature calculation
microstructure.enabled=true

# Kafka topics
microstructure.input.topic=Orderbook
microstructure.output.topic=microstructure-features

# Rolling window configuration
microstructure.window.size=50
microstructure.min.observations=20

# Emission strategy - emit features every 1 second
microstructure.emit.interval.ms=1000

# ============================================================================
# LOGGING - PRODUCTION
# ============================================================================
logging.level.com.kotsin=INFO
logging.level.org.apache.kafka.streams=WARN
logging.level.org.springframework.kafka=WARN

# Log to file in production
logging.file.name=/var/log/streamingcandle/application.log
logging.file.max-size=100MB
logging.file.max-history=30
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} - %msg%n
logging.pattern.file=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n

# ============================================================================
# PRODUCTION OPTIMIZATION
# ============================================================================

# Thread pool for processors
spring.task.execution.pool.core-size=4
spring.task.execution.pool.max-size=8
spring.task.execution.pool.queue-capacity=100

# JVM optimizations (set in startup script or systemd service)
# -Xms2g -Xmx4g -XX:+UseG1GC -XX:MaxGCPauseMillis=20
# -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled

